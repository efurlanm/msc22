{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stencil Fortran Parallel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting stencil_mpi.f90\n"
     ]
    }
   ],
   "source": [
    "%%writefile stencil_mpi.f90\n",
    "! === stencil_mpi.f90 - stencil versão paralela ===\n",
    "! Revisão 2020-08-11\n",
    "! Programa originalmente escrito por C. R. Souza\n",
    "! Adaptado por E. Furlan M.\n",
    "! Parâmetros de compilação para debugging e para produção:\n",
    "!   $ mpiifort -g -check all -fpe0 -warn -traceback -debug extended  \\\n",
    "!              -o stencil_mpi  stencil_mpi.f90\n",
    "!   $ mpiifort  -O3  -o stencil_mpi  stencil_mpi.f90\n",
    "! Para rodar em 4 processos:\n",
    "!   $ mpiexec -n 4 ./stencil_mpi 4800 1 500\n",
    "! HISTÓRICO:\n",
    "! - \"aold(sources(i,1)+1, sources(i,2)+1)\": adicionado \"+1\" porque os índices\n",
    "!   em fortran iniciam em 1. Sem essa correção acontecem erros de runtime por\n",
    "!   violação dos limites da matriz.\n",
    "! - \"sources(3,:) = (/ n*4/5, n*8/9 /)\": alterado o \"n*8/9\" para ficar simi-\n",
    "!   lar à versão \"C\" do programa, escrita por Balaji.\n",
    "\n",
    "program stencil\n",
    "    use MPI\n",
    "    implicit none\n",
    "    integer :: n=0          ! nxn grid\n",
    "    integer :: energy=0     ! energy to be injected per iteration\n",
    "    integer :: niters=0     ! number of iterations\n",
    "    integer :: iters, i, j, px, py, rx, ry\n",
    "    integer :: north, south, west, east, bx, by, offx, offy\n",
    "    integer :: nargs=0, iargc\n",
    "    integer :: mpirank, mpisize, mpitag=1, mpierror\n",
    "    integer, dimension(3) :: args\n",
    "    integer, dimension(2) :: pdims=0\n",
    "    integer, dimension(4) :: sendrequest, recvrequest\n",
    "    double precision :: mpiwtime=0.0, heat=0.0, rheat=0.0\n",
    "    double precision, dimension(:), allocatable   :: sendnorthgz, sendsouthgz\n",
    "    double precision, dimension(:), allocatable   :: recvnorthgz, recvsouthgz\n",
    "    double precision, dimension(:,:), allocatable :: aold, anew\n",
    "    character(len=50)                             :: argv\n",
    "\n",
    "    integer, parameter  :: nsources=3        ! three heat sources\n",
    "    ! locnsources = number of sources in my area\n",
    "    integer             :: locnsources=0, locx, locy\n",
    "    ! locsources = sources local to my rank\n",
    "    integer, dimension(nsources, 2) :: locsources=0, sources\n",
    "\n",
    "    call MPI_Init(mpierror)\n",
    "    call MPI_Comm_rank(MPI_COMM_WORLD, mpirank, mpierror)\n",
    "    call MPI_Comm_size(MPI_COMM_WORLD, mpisize, mpierror)\n",
    "\n",
    "    if (mpirank == 0) then                          ! rank 0 argument checking\n",
    "        mpiwtime = -MPI_Wtime()     ! inicializa contador de tempo\n",
    "        nargs = iargc()\n",
    "        call getarg(1, argv); read(argv, *) n       ! nxn grid\n",
    "        call getarg(2, argv); read(argv, *) energy  ! energy to be injected\n",
    "        call getarg(3, argv); read(argv, *) niters  ! number of iterations\n",
    "        args = [ n, energy, niters ]                ! distribute arguments\n",
    "        call MPI_Bcast(args, 3, MPI_INTEGER, 0, MPI_COMM_WORLD, mpierror)\n",
    "    else\n",
    "        call MPI_Bcast(args, 3, MPI_INTEGER, 0, MPI_COMM_WORLD, mpierror)\n",
    "        n = args(1); energy = args(2); niters  = args(3)\n",
    "    endif\n",
    "    \n",
    "    ! Creates a division of processors in a Cartesian grid\n",
    "    ! MPI_DIMS_CREATE(NNODES, NDIMS, DIMS, IERROR)\n",
    "    !   NNODES - number of nodes in a grid\n",
    "    !   NDIMS - number of Cartesian dimensions \n",
    "    !   DIMS - array specifying the number of nodes in each dimension\n",
    "    ! Examples:\n",
    "    !   MPI_Dims_create(6, 2, dims)  ->  (3,2)\n",
    "    !   MPI_Dims_create(7, 2, dims)  ->  (7,1)\n",
    "    call MPI_Dims_create(mpisize, 2, pdims, mpierror)\n",
    "\n",
    "    ! determine my coordinates (x,y)\n",
    "    px = pdims(1)\n",
    "    py = pdims(2)\n",
    "    rx = mod(mpirank, px)\n",
    "    ry = mpirank / px\n",
    "\n",
    "    ! determine my four neighbors\n",
    "    north = (ry - 1) * px + rx; if( (ry - 1) < 0  ) north = MPI_PROC_NULL\n",
    "    south = (ry + 1) * px + rx; if( (ry + 1) >= py) south = MPI_PROC_NULL\n",
    "    west = ry * px + rx - 1;    if( (rx - 1) < 0  ) west  = MPI_PROC_NULL\n",
    "    east = ry * px + rx + 1;    if( (rx + 1) >= px) east  = MPI_PROC_NULL\n",
    "\n",
    "    ! decompose the domain   \n",
    "    bx = n / px             ! block size in x\n",
    "    by = n / py             ! block size in y\n",
    "    offx = (rx * bx) + 1    ! offset in x\n",
    "    offy = (ry * by) + 1    ! offset in y\n",
    "\n",
    "    ! initialize heat sources\n",
    "    sources = reshape( [ n/2,   n/2,        &\n",
    "                         n/3,   n/3,        &\n",
    "                         n*4/5, n*8/9 ],    &\n",
    "              shape(sources), order=[2, 1])\n",
    "\n",
    "    do i = 1, nsources      ! determine which sources are in my patch\n",
    "        locx = sources(i, 1) - offx\n",
    "        locy = sources(i, 2) - offy    \n",
    "        if(locx >= 0 .and. locx <= bx .and. locy >= 0 .and. locy <= by) then\n",
    "            locnsources = locnsources + 1\n",
    "            locsources(locnsources, 1) = locx + 2\n",
    "            locsources(locnsources, 2) = locy + 2\n",
    "        endif\n",
    "    enddo\n",
    "\n",
    "    ! allocate communication buffers\n",
    "    allocate(sendnorthgz(bx))   ! send buffers\n",
    "    allocate(sendsouthgz(bx))\n",
    "    allocate(recvnorthgz(bx))   ! receive buffers\n",
    "    allocate(recvsouthgz(bx))\n",
    "    ! allocate two work arrays\n",
    "    allocate(aold(bx+2, by+2)); aold = 0.0   ! 1-wide halo zones!\n",
    "    allocate(anew(bx+2, by+2)); anew = 0.0   ! 1-wide halo zones!\n",
    "\n",
    "    ! laco principal das iteracoes\n",
    "    do iters = 1, niters, 2\n",
    "\n",
    "        ! --- anew <- stencil(aold) ---\n",
    "        if(north /= MPI_PROC_NULL) then \n",
    "            sendnorthgz = aold(2, 2:bx+1)\n",
    "            recvnorthgz = 0.0\n",
    "            call MPI_IRecv(recvnorthgz, bx, MPI_DOUBLE_PRECISION, north,  &\n",
    "                            mpitag, MPI_COMM_WORLD, recvrequest(1), mpierror)\n",
    "            call MPI_ISend(sendnorthgz, bx, MPI_DOUBLE_PRECISION, north,  &\n",
    "                            mpitag, MPI_COMM_WORLD, sendrequest(1), mpierror)\n",
    "        endif   \n",
    "        if(south /= MPI_PROC_NULL) then \n",
    "            sendsouthgz = aold(bx+1, 2:bx+1)\n",
    "            recvsouthgz(:) = 0.0\n",
    "            call MPI_IRecv(recvsouthgz, bx, MPI_DOUBLE_PRECISION, south,  &\n",
    "                            mpitag, MPI_COMM_WORLD, recvrequest(2), mpierror)\n",
    "            call MPI_ISend(sendsouthgz, bx, MPI_DOUBLE_PRECISION, south,  &\n",
    "                            mpitag, MPI_COMM_WORLD, sendrequest(2), mpierror)\n",
    "        endif    \n",
    "        if(east /= MPI_PROC_NULL) then \n",
    "            call MPI_IRecv(aold(2:bx+1, bx+2), bx, MPI_DOUBLE_PRECISION, east, &\n",
    "                            mpitag, MPI_COMM_WORLD, recvrequest(3), mpierror)\n",
    "            call MPI_ISend(aold(2:bx+1, bx+1), bx, MPI_DOUBLE_PRECISION, east, &\n",
    "                            mpitag, MPI_COMM_WORLD, sendrequest(3), mpierror)\n",
    "        endif    \n",
    "        if(west /= MPI_PROC_NULL) then \n",
    "            call MPI_IRecv(aold(2:bx+1, 1), bx, MPI_DOUBLE_PRECISION, west, &\n",
    "                           mpitag, MPI_COMM_WORLD, recvrequest(4), mpierror)\n",
    "            call MPI_ISend(aold(2:bx+1, 2), bx, MPI_DOUBLE_PRECISION, west, &\n",
    "                           mpitag, MPI_COMM_WORLD, sendrequest(4), mpierror)\n",
    "            endif\n",
    "        if(north /= MPI_PROC_NULL) then \n",
    "            call MPI_Wait(recvrequest(1), MPI_STATUS_IGNORE, mpierror)\n",
    "            call MPI_Wait(sendrequest(1), MPI_STATUS_IGNORE, mpierror)\n",
    "            aold(1, 2:bx+1)=recvnorthgz\n",
    "        endif\n",
    "        if(south /= MPI_PROC_NULL) then \n",
    "            call MPI_Wait(recvrequest(2), MPI_STATUS_IGNORE, mpierror)\n",
    "            call MPI_Wait(sendrequest(2), MPI_STATUS_IGNORE, mpierror)\n",
    "            aold(bx+2, 2:bx+1)=recvsouthgz\n",
    "        endif\n",
    "        if(east /= MPI_PROC_NULL) then \n",
    "            call MPI_Wait(recvrequest(3), MPI_STATUS_IGNORE, mpierror)\n",
    "            call MPI_Wait(sendrequest(3), MPI_STATUS_IGNORE, mpierror)\n",
    "        endif\n",
    "        if(west /= MPI_PROC_NULL) then \n",
    "            call MPI_Wait(recvrequest(4), MPI_STATUS_IGNORE, mpierror)\n",
    "            call MPI_Wait(sendrequest(4), MPI_STATUS_IGNORE, mpierror)\n",
    "        endif  \n",
    "\n",
    "        ! update grid points\n",
    "        do j = 2, by+1 \n",
    "            do i = 2, bx+1\n",
    "                anew(i, j) = aold(i, j)/2.0 + (aold(i-1, j) + aold(i+1, j) +  &\n",
    "                             aold(i, j-1) + aold(i, j+1)) / 4.0 / 2.0\n",
    "            enddo\n",
    "        enddo\n",
    "\n",
    "        ! adiciona calor a malha\n",
    "        do i = 1, locnsources\n",
    "            anew(locsources(i, 1), locsources(i, 2)) =   &\n",
    "                anew(locsources(i, 1), locsources(i, 2)) + energy\n",
    "        enddo\n",
    "\n",
    "        ! --- aold <- stencil(anew) ---\n",
    "        if(north /= MPI_PROC_NULL) then \n",
    "            sendnorthgz=anew(2, 2:bx+1)\n",
    "            call MPI_IRecv(recvnorthgz, bx, MPI_DOUBLE_PRECISION, north, mpitag,  &\n",
    "                            MPI_COMM_WORLD, recvrequest(1), mpierror)\n",
    "            call MPI_ISend(sendnorthgz, bx, MPI_DOUBLE_PRECISION, north, mpitag,  &\n",
    "                            MPI_COMM_WORLD, sendrequest(1), mpierror)\n",
    "        endif\n",
    "        if(south /= MPI_PROC_NULL) then \n",
    "            sendsouthgz=anew(bx+1, 2:bx+1)\n",
    "            call MPI_IRecv(recvsouthgz, bx, MPI_DOUBLE_PRECISION, south, mpitag,  &\n",
    "                            MPI_COMM_WORLD, recvrequest(2), mpierror)   \n",
    "            call MPI_ISend(sendsouthgz, bx, MPI_DOUBLE_PRECISION, south, mpitag,  &\n",
    "                            MPI_COMM_WORLD, sendrequest(2), mpierror)\n",
    "        endif\n",
    "        if(east /= MPI_PROC_NULL) then \n",
    "            call MPI_IRecv(anew(2:bx+1, bx+2), bx, MPI_DOUBLE_PRECISION, east,  &\n",
    "                            mpitag, MPI_COMM_WORLD, recvrequest(3), mpierror)\n",
    "            call MPI_ISend(anew(2:bx+1, bx+1), bx, MPI_DOUBLE_PRECISION, east,  &\n",
    "                            mpitag, MPI_COMM_WORLD, sendrequest(3), mpierror)\n",
    "        endif\n",
    "        if(west /= MPI_PROC_NULL) then \n",
    "            call MPI_IRecv(anew(2:bx+1, 1), bx, MPI_DOUBLE_PRECISION, west, mpitag,  &\n",
    "                            MPI_COMM_WORLD, recvrequest(4), mpierror)\n",
    "            call MPI_ISend(anew(2:bx+1, 2), bx, MPI_DOUBLE_PRECISION, west, mpitag,  &\n",
    "                            MPI_COMM_WORLD, sendrequest(4), mpierror)\n",
    "        endif\n",
    "        if(north /= MPI_PROC_NULL) then \n",
    "            call MPI_Wait(recvrequest(1), MPI_STATUS_IGNORE, mpierror)\n",
    "            call MPI_Wait(sendrequest(1), MPI_STATUS_IGNORE, mpierror)\n",
    "            anew(1, 2:bx+1)=recvnorthgz\n",
    "        endif\n",
    "        if(south /= MPI_PROC_NULL) then \n",
    "            call MPI_Wait(recvrequest(2), MPI_STATUS_IGNORE, mpierror)\n",
    "            call MPI_Wait(sendrequest(2), MPI_STATUS_IGNORE, mpierror)\n",
    "            anew(bx+2, 2:bx+1)=recvsouthgz\n",
    "        endif\n",
    "        if(east /= MPI_PROC_NULL) then \n",
    "            call MPI_Wait(recvrequest(3), MPI_STATUS_IGNORE, mpierror)\n",
    "            call MPI_Wait(sendrequest(3), MPI_STATUS_IGNORE, mpierror)\n",
    "        endif\n",
    "        if(west /= MPI_PROC_NULL) then \n",
    "            call MPI_Wait(recvrequest(4), MPI_STATUS_IGNORE, mpierror)\n",
    "            call MPI_Wait(sendrequest(4), MPI_STATUS_IGNORE, mpierror)\n",
    "        endif\n",
    "\n",
    "        ! update grid points\n",
    "        do j = 2, by+1 \n",
    "            do i = 2, bx+1\n",
    "                aold(i, j) = anew(i, j)/2.0 + (anew(i-1, j) + anew(i+1, j) +  &\n",
    "                             anew(i, j-1) + anew(i, j+1)) / 4.0 / 2.0\n",
    "            enddo\n",
    "        enddo\n",
    "\n",
    "        ! adiciona calor a malha:\n",
    "        do i = 1, locnsources\n",
    "            aold(locsources(i, 1), locsources(i, 2)) =  &\n",
    "                aold(locsources(i, 1), locsources(i, 2)) + energy\n",
    "        enddo\n",
    "\n",
    "    enddo\n",
    "   \n",
    "    ! ALL REDUCE:\n",
    "    heat = 0.0\n",
    "    do j = 2, by+1 \n",
    "        do i = 2, bx+1\n",
    "            heat = heat + aold(i, j)\n",
    "        enddo\n",
    "    enddo\n",
    "    call MPI_Allreduce(heat, rheat, 1, MPI_DOUBLE_PRECISION, MPI_SUM,  &\n",
    "                       MPI_COMM_WORLD, mpierror)\n",
    "\n",
    "    if(mpirank == 0) then\n",
    "        mpiwtime = mpiwtime + MPI_Wtime()\n",
    "        write(*, \"('Heat='     f0.2' | ')\", advance=\"no\") rheat\n",
    "        write(*, \"('Tempo='    f0.4' | ')\", advance=\"no\") mpiwtime\n",
    "        write(*, \"('MPI_Size=' i0  ' | ')\", advance=\"no\") mpisize\n",
    "        write(*, \"('MPI_Dims=('i0','i0') | ')\", advance=\"no\") pdims\n",
    "        write(*, \"('bx,by=('i0','i0')')\") bx,by\n",
    "    endif\n",
    "\n",
    "    call MPI_Finalize(mpierror)\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Carrega as variáveis de ambiente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intel(R) Parallel Studio XE 2019 Update 3 for Linux*\n",
      "Copyright (C) 2009-2019 Intel Corporation. All rights reserved.\n",
      "-rwxr-xr-x 1 xxxx.xxxx ampemi 817K Nov  8 20:44 stencil_mpi\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "source /scratch/app/modulos/intel-psxe-2019.sh\n",
    "mpiifort  -O3  -o stencil_mpi  stencil_mpi.f90\n",
    "ls -lh stencil_mpi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testa o funcionamento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intel(R) Parallel Studio XE 2019 Update 3 for Linux*\n",
      "Copyright (C) 2009-2019 Intel Corporation. All rights reserved.\n",
      "Heat=1500.00 | Tempo=19.4014 | MPI_Size=1 | MPI_Dims=(1,1) | bx,by=(4800,4800)\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "source /scratch/app/modulos/intel-psxe-2019.sh\n",
    "mpiexec -n 1 ./stencil_mpi 4800 1 500"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Executa no nó de execução"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copia o executável para /scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 769K\n",
      "-rwxr-xr-x 1 xxxx.xxxx ampemi 817K Nov  8 21:12 stencil_mpi\n",
      "-rwxr-xr-x 1 xxxx.xxxx ampemi 765K Nov  8 18:59 stencil_seq\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "a='stencil_mpi'\n",
    "b='/stnc/Fortran'\n",
    "s='/prj/ampemi/xxxx.xxxx'$b\n",
    "d='/scratch/ampemi/xxxx.xxxx'$b\n",
    "#mkdir -p $d\n",
    "cp $s/$a $d\n",
    "ls -lh $d/$a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Arquivo de lote de submissão"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting stc_for_par_25.srm\n"
     ]
    }
   ],
   "source": [
    "%%writefile stc_for_par_25.srm\n",
    "#!/bin/bash\n",
    "# limites das filas (1,0 UA):\n",
    "#   cpu_dev  : 20 min.,  1-4  nós, 1/1   tarefas em exec/fila máximo\n",
    "#   cpu_small: 72 horas, 1-20 nós, 16/96 tarefas em exec/fila máximo\n",
    "# 1x1=1, 2x2=4, 3x3=9, 4x4=16, 6x6=36, 7x7=49, 8x8=64, 9x9=81\n",
    "\n",
    "#SBATCH --ntasks=25            #Total de tarefas\n",
    "#SBATCH -J stcforpa            #Nome do job, 8 caracteres\n",
    "#SBATCH -p cpu_small           #Fila (partition) a ser utilizada\n",
    "#SBATCH --time=00:02:00        #Tempo max. de execução 2 minutos\n",
    "#SBATCH --nodes=5              #Qtd de nós\n",
    "#SBATCH --ntasks-per-node=5    #Qtd de tarefas por nó ($SLURM_NTASKS_PER_NODE)\n",
    "# #SBATCH --exclusive            #Utilização exclusiva dos nós\n",
    "\n",
    "echo '========================================'\n",
    "echo '- Stencil Fortran Paralelo MPI'\n",
    "echo '- Job ID:' $SLURM_JOB_ID\n",
    "echo '- Tarefas por no:' $SLURM_NTASKS_PER_NODE\n",
    "echo '- Qtd. de nos:' $SLURM_JOB_NUM_NODES\n",
    "echo '- Tot. de tarefas:' $SLURM_NTASKS\n",
    "echo '- Nos alocados:' $SLURM_JOB_NODELIST\n",
    "echo '- diretorio onde sbatch foi chamado ($SLURM_SUBMIT_DIR):'\n",
    "echo $SLURM_SUBMIT_DIR\n",
    "cd $SLURM_SUBMIT_DIR\n",
    "\n",
    "#Configura o ambiente\n",
    "echo '-- modulos ----------------------------'\n",
    "echo '$ source /scratch/app/modulos/intel-psxe-2019.sh'\n",
    "source /scratch/app/modulos/intel-psxe-2019.sh\n",
    "\n",
    "#Configura o executavel\n",
    "a='stencil_mpi'\n",
    "b='/stnc/Fortran'\n",
    "d='/scratch/ampemi/xxxx.xxxx'$b\n",
    "EXEC=$d/$a\n",
    "PARA='4800 1 500'\n",
    "\n",
    "#Dispara a execucao\n",
    "echo '-- srun -------------------------------'\n",
    "echo '$ srun -n ' $SLURM_NTASKS $EXEC $PARA\n",
    "srun -n $SLURM_NTASKS $EXEC $PARA\n",
    "echo '-- FIM --------------------------------'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Submete o job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submitted batch job 769871\n",
      "Submitted batch job 769872\n",
      "Submitted batch job 769873\n",
      "Submitted batch job 769874\n",
      "Submitted batch job 769875\n",
      "Submitted batch job 769876\n",
      "Submitted batch job 769877\n",
      "Submitted batch job 769878\n",
      "Submitted batch job 769879\n",
      "             JOBID PARTITION     NAME     USER ST       TIME  NODES NODELIST(REASON)\n",
      "            769864 cpu_small stc_for_ xxxx. PD       0:00      2 (Resources)\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "# 1x1=1, 2x2=4, 3x3=9, 4x4=16, 5x5=25, 6x6=36, 7x7=49, 8x8=64, 9x9=81\n",
    "# sbatch stencil_mpi.srm\n",
    "sbatch stc_for_par_01.srm\n",
    "sbatch stc_for_par_04.srm\n",
    "sbatch stc_for_par_09.srm\n",
    "sbatch stc_for_par_16.srm\n",
    "sbatch stc_for_par_25.srm\n",
    "sbatch stc_for_par_36.srm\n",
    "sbatch stc_for_par_49.srm\n",
    "sbatch stc_for_par_64.srm\n",
    "sbatch stc_for_par_81.srm\n",
    "squeue -n stc_for_par\n",
    "squeue -n stcforpa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             JOBID PARTITION     NAME     USER ST       TIME  NODES NODELIST(REASON)\n",
      "             JOBID PARTITION     NAME     USER ST       TIME  NODES NODELIST(REASON)\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "# verifica se já terminou\n",
    "squeue -n stc_for_par\n",
    "squeue -n stcforpa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submitted batch job 770304\n",
      "             JOBID PARTITION     NAME     USER ST       TIME  NODES NODELIST(REASON)\n",
      "            770304 cpu_small stcforpa xxxx. PD       0:00      5 (Priority)\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "# roda de novo o 25 com 5 tarefas por nó, em 4 nós\n",
    "# 1x1=1, 2x2=4, 3x3=9, 4x4=16, 5x5=25, 6x6=36, 7x7=49, 8x8=64, 9x9=81\n",
    "sbatch stc_for_par_25.srm\n",
    "squeue -n stcforpa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             JOBID PARTITION     NAME     USER ST       TIME  NODES NODELIST(REASON)\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "# verifica se já terminou\n",
    "squeue -n stcforpa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mostra os arquivos de saída intel-psxe-2019"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================================\n",
      "- Stencil Fortran Paralelo MPI\n",
      "- Job ID: 769871\n",
      "- Tarefas por no:\n",
      "- Qtd. de nos: 1\n",
      "- Tot. de tarefas: 1\n",
      "- Nos alocados: sdumont1358\n",
      "- diretorio onde sbatch foi chamado ($SLURM_SUBMIT_DIR):\n",
      "/prj/ampemi/xxxx.xxxx/stnc/Fortran\n",
      "-- modulos ----------------------------\n",
      "$ source /scratch/app/modulos/intel-psxe-2019.sh\n",
      "Intel(R) Parallel Studio XE 2019 Update 3 for Linux*\n",
      "Copyright (C) 2009-2019 Intel Corporation. All rights reserved.\n",
      "-- srun -------------------------------\n",
      "$ srun -n  1 /scratch/ampemi/xxxx.xxxx/stnc/Fortran/stencil_mpi 4800 1 500\n",
      "Heat=1500.00 | Tempo=21.9490 | MPI_Size=1 | MPI_Dims=(1,1) | bx,by=(4800,4800)\n",
      "-- FIM --------------------------------\n",
      "========================================\n",
      "- Stencil Fortran Paralelo MPI\n",
      "- Job ID: 769872\n",
      "- Tarefas por no:\n",
      "- Qtd. de nos: 1\n",
      "- Tot. de tarefas: 4\n",
      "- Nos alocados: sdumont1358\n",
      "- diretorio onde sbatch foi chamado ($SLURM_SUBMIT_DIR):\n",
      "/prj/ampemi/xxxx.xxxx/stnc/Fortran\n",
      "-- modulos ----------------------------\n",
      "$ source /scratch/app/modulos/intel-psxe-2019.sh\n",
      "Intel(R) Parallel Studio XE 2019 Update 3 for Linux*\n",
      "Copyright (C) 2009-2019 Intel Corporation. All rights reserved.\n",
      "-- srun -------------------------------\n",
      "$ srun -n  4 /scratch/ampemi/xxxx.xxxx/stnc/Fortran/stencil_mpi 4800 1 500\n",
      "Heat=1500.00 | Tempo=7.4350 | MPI_Size=4 | MPI_Dims=(2,2) | bx,by=(2400,2400)\n",
      "-- FIM --------------------------------\n",
      "========================================\n",
      "- Stencil Fortran Paralelo MPI\n",
      "- Job ID: 769873\n",
      "- Tarefas por no:\n",
      "- Qtd. de nos: 1\n",
      "- Tot. de tarefas: 9\n",
      "- Nos alocados: sdumont1358\n",
      "- diretorio onde sbatch foi chamado ($SLURM_SUBMIT_DIR):\n",
      "/prj/ampemi/xxxx.xxxx/stnc/Fortran\n",
      "-- modulos ----------------------------\n",
      "$ source /scratch/app/modulos/intel-psxe-2019.sh\n",
      "Intel(R) Parallel Studio XE 2019 Update 3 for Linux*\n",
      "Copyright (C) 2009-2019 Intel Corporation. All rights reserved.\n",
      "-- srun -------------------------------\n",
      "$ srun -n  9 /scratch/ampemi/xxxx.xxxx/stnc/Fortran/stencil_mpi 4800 1 500\n",
      "Heat=1500.00 | Tempo=6.2084 | MPI_Size=9 | MPI_Dims=(3,3) | bx,by=(1600,1600)\n",
      "-- FIM --------------------------------\n",
      "========================================\n",
      "- Stencil Fortran Paralelo MPI\n",
      "- Job ID: 769874\n",
      "- Tarefas por no:\n",
      "- Qtd. de nos: 1\n",
      "- Tot. de tarefas: 16\n",
      "- Nos alocados: sdumont1254\n",
      "- diretorio onde sbatch foi chamado ($SLURM_SUBMIT_DIR):\n",
      "/prj/ampemi/xxxx.xxxx/stnc/Fortran\n",
      "-- modulos ----------------------------\n",
      "$ source /scratch/app/modulos/intel-psxe-2019.sh\n",
      "Intel(R) Parallel Studio XE 2019 Update 3 for Linux*\n",
      "Copyright (C) 2009-2019 Intel Corporation. All rights reserved.\n",
      "-- srun -------------------------------\n",
      "$ srun -n  16 /scratch/ampemi/xxxx.xxxx/stnc/Fortran/stencil_mpi 4800 1 500\n",
      "Heat=1500.00 | Tempo=4.6041 | MPI_Size=16 | MPI_Dims=(4,4) | bx,by=(1200,1200)\n",
      "-- FIM --------------------------------\n",
      "========================================\n",
      "- Stencil Fortran Paralelo M0PI\n",
      "- Job ID: 769875\n",
      "- Tarefas por no:\n",
      "- Qtd. de nos: 2\n",
      "- Tot. de tarefas: 25\n",
      "- Nos alocados: sdumont[1209-1210]\n",
      "- diretorio onde sbatch foi chamado ($SLURM_SUBMIT_DIR):\n",
      "/prj/ampemi/xxxx.xxxx/stnc/Fortran\n",
      "-- modulos ----------------------------\n",
      "$ source /scratch/app/modulos/intel-psxe-2019.sh\n",
      "Intel(R) Parallel Studio XE 2019 Update 3 for Linux*\n",
      "Copyright (C) 2009-2019 Intel Corporation. All rights reserved.\n",
      "-- srun -------------------------------\n",
      "$ srun -n  25 /scratch/ampemi/xxxx.xxxx/stnc/Fortran/stencil_mpi 4800 1 500\n",
      "Heat=1067.45 | Tempo=2.9438 | MPI_Size=25 | MPI_Dims=(5,5) | bx,by=(960,960)\n",
      "-- FIM --------------------------------\n",
      "========================================\n",
      "- Stencil Fortran Paralelo MPI\n",
      "- Job ID: 769876\n",
      "- Tarefas por no:\n",
      "- Qtd. de nos: 2\n",
      "- Tot. de tarefas: 36\n",
      "- Nos alocados: sdumont[1209-1210]\n",
      "- diretorio onde sbatch foi chamado ($SLURM_SUBMIT_DIR):\n",
      "/prj/ampemi/xxxx.xxxx/stnc/Fortran\n",
      "-- modulos ----------------------------\n",
      "$ source /scratch/app/modulos/intel-psxe-2019.sh\n",
      "Intel(R) Parallel Studio XE 2019 Update 3 for Linux*\n",
      "Copyright (C) 2009-2019 Intel Corporation. All rights reserved.\n",
      "-- srun -------------------------------\n",
      "$ srun -n  36 /scratch/ampemi/xxxx.xxxx/stnc/Fortran/stencil_mpi 4800 1 500\n",
      "Heat=1500.00 | Tempo=2.2293 | MPI_Size=36 | MPI_Dims=(6,6) | bx,by=(800,800)\n",
      "-- FIM --------------------------------\n",
      "========================================\n",
      "- Stencil Fortran Paralelo MPI\n",
      "- Job ID: 769877\n",
      "- Tarefas por no:\n",
      "- Qtd. de nos: 3\n",
      "- Tot. de tarefas: 49\n",
      "- Nos alocados: sdumont[1209-1210,1357]\n",
      "- diretorio onde sbatch foi chamado ($SLURM_SUBMIT_DIR):\n",
      "/prj/ampemi/xxxx.xxxx/stnc/Fortran\n",
      "-- modulos ----------------------------\n",
      "$ source /scratch/app/modulos/intel-psxe-2019.sh\n",
      "Intel(R) Parallel Studio XE 2019 Update 3 for Linux*\n",
      "Copyright (C) 2009-2019 Intel Corporation. All rights reserved.\n",
      "- srun -------------------------------\n",
      "$ srun -n  49 /scratch/ampemi/xxxx.xxxx/stnc/Fortran/stencil_mpi 4800 1 500\n",
      "Heat=1500.00 | Tempo=1.5379 | MPI_Size=49 | MPI_Dims=(7,7) | bx,by=(685,685)\n",
      "-- FIM --------------------------------\n",
      "========================================\n",
      "- Stencil Fortran Paralelo MPI\n",
      "- Job ID: 769878\n",
      "- Tarefas por no:\n",
      "- Qtd. de nos: 3\n",
      "- Tot. de tarefas: 64\n",
      "- Nos alocados: sdumont[1209-1210,1357]\n",
      "- diretorio onde sbatch foi chamado ($SLURM_SUBMIT_DIR):\n",
      "/prj/ampemi/xxxx.xxxx/stnc/Fortran\n",
      "-- modulos ----------------------------\n",
      "$ source /scratch/app/modulos/intel-psxe-2019.sh\n",
      "Intel(R) Parallel Studio XE 2019 Update 3 for Linux*\n",
      "Copyright (C) 2009-2019 Intel Corporation. All rights reserved.\n",
      "-- srun -------------------------------\n",
      "$ srun -n  64 /scratch/ampemi/xxxx.xxxx/stnc/Fortran/stencil_mpi 4800 1 500\n",
      "Heat=1500.00 | Tempo=1.2564 | MPI_Size=64 | MPI_Dims=(8,8) | bx,by=(600,600)\n",
      "-- FIM --------------------------------\n",
      "-========================================\n",
      " Stencil Fortran Paralelo MPI\n",
      "- Job ID: 769879\n",
      "- Tarefas por no:\n",
      "- Qtd. de nos: 4\n",
      "- Tot. de tarefas: 81\n",
      "- Nos alocados: sdumont[1209-1210,1357-1358]\n",
      "- diretorio onde sbatch foi chamado ($SLURM_SUBMIT_DIR):\n",
      "/prj/ampemi/xxxx.xxxx/stnc/Fortran\n",
      "-- modulos ----------------------------\n",
      "$ source /scratch/app/modulos/intel-psxe-2019.sh\n",
      "Intel(R) Parallel Studio XE 2019 Update 3 for Linux*\n",
      "Copyright (C) 2009-2019 Intel Corporation. All rights reserved.\n",
      "-- srun -------------------------------\n",
      "$ srun -n  81 /scratch/ampemi/xxxx.xxxx/stnc/Fortran/stencil_mpi 4800 1 500\n",
      "Heat=1500.00 | Tempo=2.2841 | MPI_Size=81 | MPI_Dims=(9,9) | bx,by=(533,533)\n",
      "-- FIM --------------------------------\n",
      "========================================\n",
      "- Stencil Fortran Paralelo MPI\n",
      "- Job ID: 769864\n",
      "- Tarefas por no:\n",
      "- Qtd. de nos: 2\n",
      "- Tot. de tarefas: 25\n",
      "- Nos alocados: sdumont[1209-1210]\n",
      "- diretorio onde sbatch foi chamado ($SLURM_SUBMIT_DIR):\n",
      "/prj/ampemi/xxxx.xxxx/stnc/Fortran\n",
      "-- modulos ----------------------------\n",
      "$ source /scratch/app/modulos/intel-psxe-2019.sh\n",
      "Intel(R) Parallel Studio XE 2019 Update 3 for Linux*\n",
      "Copyright (C) 2009-2019 Intel Corporation. All rights reserved.\n",
      "-- srun -------------------------------\n",
      "$ srun -n  25 /scratch/ampemi/xxxx.xxxx/stnc/Fortran/stencil_mpi 4800 1 500\n",
      "Heat=1067.45 | Tempo=2.9348 | MPI_Size=25 | MPI_Dims=(5,5) | bx,by=(960,960)\n",
      "-- FIM --------------------------------\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "b='/stnc/Fortran'\n",
    "d='/scratch/ampemi/xxxx.xxxx'$b\n",
    "\n",
    "cat $d/slurm-769871.out\n",
    "cat $d/slurm-769872.out\n",
    "cat $d/slurm-769873.out\n",
    "cat $d/slurm-769874.out\n",
    "cat $d/slurm-769875.out\n",
    "cat $d/slurm-769876.out\n",
    "cat $d/slurm-769877.out\n",
    "cat $d/slurm-769878.out\n",
    "cat $d/slurm-769879.out\n",
    "cat $d/slurm-769864.out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================================\n",
      "- Stencil Fortran Paralelo MPI\n",
      "- Job ID: 770304\n",
      "- Tarefas por no: 5\n",
      "- Qtd. de nos: 5\n",
      "- Tot. de tarefas: 25\n",
      "- Nos alocados: sdumont[1117-1119,1344-1345]\n",
      "- diretorio onde sbatch foi chamado ($SLURM_SUBMIT_DIR):\n",
      "/prj/ampemi/xxxx.xxxx/stnc/Fortran\n",
      "-- modulos ----------------------------\n",
      "$ source /scratch/app/modulos/intel-psxe-2019.sh\n",
      "Intel(R) Parallel Studio XE 2019 Update 3 for Linux*\n",
      "Copyright (C) 2009-2019 Intel Corporation. All rights reserved.\n",
      "-- srun -------------------------------\n",
      "$ srun -n  25 /scratch/ampemi/xxxx.xxxx/stnc/Fortran/stencil_mpi 4800 1 500\n",
      "Heat=1067.45 | Tempo=2.3911 | MPI_Size=25 | MPI_Dims=(5,5) | bx,by=(960,960)\n",
      "-- FIM --------------------------------\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "# mostra o 25\n",
    "b='/stnc/Fortran'\n",
    "d='/scratch/ampemi/xxxx.xxxx'$b\n",
    "\n",
    "cat $d/slurm-770304.out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GNU 7.4.0 e OpenMPI 4.0.1\n",
    "## Não é para usar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting stfopagnu740_81.srm\n"
     ]
    }
   ],
   "source": [
    "%%writefile stfopagnu740_81.srm\n",
    "#!/bin/bash\n",
    "# limites das filas (1,0 UA):\n",
    "#   cpu_dev  : 20 min.,  1-4  nós, 1/1   tarefas em exec/fila máximo\n",
    "#   cpu_small: 72 horas, 1-20 nós, 16/96 tarefas em exec/fila máximo\n",
    "# 1x1=1, 2x2=4, 3x3=9, 4x4=16, 6x6=36, 7x7=49, 8x8=64, 9x9=81\n",
    "\n",
    "#SBATCH --ntasks=81            #Total de tarefas\n",
    "#SBATCH -p cpu_small           #Fila (partition) a ser utilizada\n",
    "#SBATCH -J stcforpa            #Nome do job, 8 caracteres\n",
    "#SBATCH --time=00:02:00        #Tempo max. de execução 2 minutos\n",
    "# #SBATCH --nodes=5              #Qtd de nós\n",
    "# #SBATCH --ntasks-per-node=5    #Qtd de tarefas por nó ($SLURM_NTASKS_PER_NODE)\n",
    "# #SBATCH --exclusive            #Utilização exclusiva dos nós\n",
    "\n",
    "echo '========================================'\n",
    "echo '- Job ID:' $SLURM_JOB_ID\n",
    "echo '- Tarefas por no:' $SLURM_NTASKS_PER_NODE\n",
    "echo '- Qtd. de nos:' $SLURM_JOB_NUM_NODES\n",
    "echo '- Tot. de tarefas:' $SLURM_NTASKS\n",
    "echo '- Nos alocados:' $SLURM_JOB_NODELIST\n",
    "echo '- diretorio onde sbatch foi chamado ($SLURM_SUBMIT_DIR):'\n",
    "echo $SLURM_SUBMIT_DIR\n",
    "cd $SLURM_SUBMIT_DIR\n",
    "nodeset -e $SLURM_JOB_NODELIST\n",
    "\n",
    "#Configura o ambiente\n",
    "echo '-- modulos ----------------------------'\n",
    "echo 'module load gcc/7.4'\n",
    "module load gcc/7.4\n",
    "echo 'module load openmpi/gnu/4.0.1'\n",
    "module load openmpi/gnu/4.0.1\n",
    "\n",
    "#Configura o executavel\n",
    "EXEC=\"/scratch/ampemi/xxxx.xxxx/stnc/Fortran/stencil_mpi 4800 1 500\"\n",
    "\n",
    "#Dispara a execucao\n",
    "echo '-- srun -------------------------------'\n",
    "echo '$ srun --mpi=pmi2 -n' $SLURM_NTASKS $EXEC\n",
    "srun --mpi=pmi2 -n $SLURM_NTASKS $EXEC\n",
    "echo '-- FIM --------------------------------'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submitted batch job 781022\n",
      "Submitted batch job 781023\n",
      "Submitted batch job 781024\n",
      "Submitted batch job 781025\n",
      "Submitted batch job 781026\n",
      "Submitted batch job 781027\n",
      "Submitted batch job 781028\n",
      "Submitted batch job 781029\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "# 1x1=1, 2x2=4, 3x3=9, 4x4=16, 6x6=36, 7x7=49, 8x8=64, 9x9=81\n",
    "sbatch stfopagnu740_01.srm\n",
    "sbatch stfopagnu740_04.srm\n",
    "sbatch stfopagnu740_09.srm\n",
    "sbatch stfopagnu740_16.srm\n",
    "sbatch stfopagnu740_36.srm\n",
    "sbatch stfopagnu740_49.srm\n",
    "sbatch stfopagnu740_64.srm\n",
    "sbatch stfopagnu740_81.srm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             JOBID PARTITION     NAME     USER ST       TIME  NODES NODELIST(REASON)\n",
      "            781022 cpu_small stcforpa xxxx. CG       0:24      1 sdumont1429\n",
      "            781020 cpu_small stcforpa xxxx. PD       0:00      4 (Resources)\n",
      "            781029 cpu_small stcforpa xxxx. PD       0:00      4 (Resources)\n",
      "            781028 cpu_small stcforpa xxxx. PD       0:00      3 (Resources)\n",
      "            781027 cpu_small stcforpa xxxx. PD       0:00      3 (Resources)\n",
      "            781025 cpu_small stcforpa xxxx. PD       0:00      1 (Resources)\n",
      "            781026 cpu_small stcforpa xxxx. PD       0:00      2 (Resources)\n"
     ]
    }
   ],
   "source": [
    "! squeue -n stcforpa  # verifica se já terminou"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             JOBID PARTITION     NAME     USER ST       TIME  NODES NODELIST(REASON)\n"
     ]
    }
   ],
   "source": [
    "! squeue -n stcforpa  # verifica se já terminou"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GNU Fortran (GCC) 7.4.0\n",
      "Copyright (C) 2017 Free Software Foundation, Inc.\n",
      "This is free software; see the source for copying conditions.  There is NO\n",
      "warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "module load gcc/7.4\n",
    "module load openmpi/gnu/4.0.1\n",
    "mpif90 --version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Não é para usar\n",
    "Mostra os arquivos de saída GNU Fortran (GCC) 7.4.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================================\n",
      "- Job ID: 781022\n",
      "- Tarefas por no:\n",
      "- Qtd. de nos: 1\n",
      "- Tot. de tarefas: 1\n",
      "- Nos alocados: sdumont1429\n",
      "- diretorio onde sbatch foi chamado ($SLURM_SUBMIT_DIR):\n",
      "/prj/ampemi/xxxx.xxxx/stnc/Fortran\n",
      "sdumont1429\n",
      "-- modulos ----------------------------\n",
      "module load gcc/7.4\n",
      "module load openmpi/gnu/4.0.1\n",
      "-- srun -------------------------------\n",
      "$ srun --mpi=pmi2 -n 1 /scratch/ampemi/xxxx.xxxx/stnc/Fortran/stencil_mpi 4800 1 500\n",
      "Heat=1500.00 | Tempo=21.8398 | MPI_Size=1 | MPI_Dims=(1,1) | bx,by=(4800,4800)\n",
      "-- FIM --------------------------------\n",
      "========================================\n",
      "- Job ID: 781023\n",
      "- Tarefas por no:\n",
      "- Qtd. de nos: 1\n",
      "- Tot. de tarefas: 4\n",
      "- Nos alocados: sdumont1454\n",
      "- diretorio onde sbatch foi chamado ($SLURM_SUBMIT_DIR):\n",
      "/prj/ampemi/xxxx.xxxx/stnc/Fortran\n",
      "sdumont1454\n",
      "-- modulos ----------------------------\n",
      "module load gcc/7.4\n",
      "module load openmpi/gnu/4.0.1\n",
      "-- srun -------------------------------\n",
      "$ srun --mpi=pmi2 -n 4 /scratch/ampemi/xxxx.xxxx/stnc/Fortran/stencil_mpi 4800 1 500\n",
      "Heat=1500.00 | Tempo=7.4191 | MPI_Size=4 | MPI_Dims=(2,2) | bx,by=(2400,2400)\n",
      "-- FIM --------------------------------\n",
      "========================================\n",
      "- Job ID: 781024\n",
      "- Tarefas por no:\n",
      "- Qtd. de nos: 1\n",
      "- Tot. de tarefas: 9\n",
      "- Nos alocados: sdumont1464\n",
      "- diretorio onde sbatch foi chamado ($SLURM_SUBMIT_DIR):\n",
      "/prj/ampemi/xxxx.xxxx/stnc/Fortran\n",
      "sdumont1464\n",
      "-- modulos ----------------------------\n",
      "module load gcc/7.4\n",
      "module load openmpi/gnu/4.0.1\n",
      "-- srun -------------------------------\n",
      "$ srun --mpi=pmi2 -n 9 /scratch/ampemi/xxxx.xxxx/stnc/Fortran/stencil_mpi 4800 1 500\n",
      "Heat=1500.00 | Tempo=6.1032 | MPI_Size=9 | MPI_Dims=(3,3) | bx,by=(1600,1600)\n",
      "-- FIM --------------------------------\n",
      "========================================\n",
      "- Job ID: 781025\n",
      "- Tarefas por no:\n",
      "- Qtd. de nos: 1\n",
      "- Tot. de tarefas: 16\n",
      "- Nos alocados: sdumont1429\n",
      "- diretorio onde sbatch foi chamado ($SLURM_SUBMIT_DIR):\n",
      "/prj/ampemi/xxxx.xxxx/stnc/Fortran\n",
      "sdumont1429\n",
      "-- modulos ----------------------------\n",
      "module load gcc/7.4\n",
      "module load openmpi/gnu/4.0.1\n",
      "-- srun -------------------------------\n",
      "$ srun --mpi=pmi2 -n 16 /scratch/ampemi/xxxx.xxxx/stnc/Fortran/stencil_mpi 4800 1 500\n",
      "Heat=1500.00 | Tempo=4.8368 | MPI_Size=16 | MPI_Dims=(4,4) | bx,by=(1200,1200)\n",
      "-- FIM --------------------------------\n",
      "========================================\n",
      "- Job ID: 781026\n",
      "- Tarefas por no:\n",
      "- Qtd. de nos: 2\n",
      "- Tot. de tarefas: 36\n",
      "- Nos alocados: sdumont[1454,1464]\n",
      "- diretorio onde sbatch foi chamado ($SLURM_SUBMIT_DIR):\n",
      "/prj/ampemi/xxxx.xxxx/stnc/Fortran\n",
      "sdumont1454 sdumont1464\n",
      "-- modulos ----------------------------\n",
      "module load gcc/7.4\n",
      "module load openmpi/gnu/4.0.1\n",
      "-- srun -------------------------------\n",
      "$ srun --mpi=pmi2 -n 36 /scratch/ampemi/xxxx.xxxx/stnc/Fortran/stencil_mpi 4800 1 500\n",
      "Heat=1500.00 | Tempo=2.0855 | MPI_Size=36 | MPI_Dims=(6,6) | bx,by=(800,800)\n",
      "-- FIM --------------------------------\n",
      "========================================\n",
      "- Job ID: 781027\n",
      "- Tarefas por no:\n",
      "- Qtd. de nos: 3\n",
      "- Tot. de tarefas: 49\n",
      "- Nos alocados: sdumont[1429,1454,1464]\n",
      "- diretorio onde sbatch foi chamado ($SLURM_SUBMIT_DIR):\n",
      "/prj/ampemi/xxxx.xxxx/stnc/Fortran\n",
      "sdumont1429 sdumont1454 sdumont1464\n",
      "-- modulos ----------------------------\n",
      "module load gcc/7.4\n",
      "module load openmpi/gnu/4.0.1\n",
      "-- srun -------------------------------\n",
      "$ srun --mpi=pmi2 -n 49 /scratch/ampemi/xxxx.xxxx/stnc/Fortran/stencil_mpi 4800 1 500\n",
      "Heat=1500.00 | Tempo=1.6647 | MPI_Size=49 | MPI_Dims=(7,7) | bx,by=(685,685)\n",
      "-- FIM --------------------------------\n",
      "========================================\n",
      "- Job ID: 781028\n",
      "- Tarefas por no:\n",
      "- Qtd. de nos: 3\n",
      "- Tot. de tarefas: 64\n",
      "- Nos alocados: sdumont[1429,1454,1464]\n",
      "- diretorio onde sbatch foi chamado ($SLURM_SUBMIT_DIR):\n",
      "/prj/ampemi/xxxx.xxxx/stnc/Fortran\n",
      "sdumont1429 sdumont1454 sdumont1464\n",
      "-- modulos ----------------------------\n",
      "module load gcc/7.4\n",
      "module load openmpi/gnu/4.0.1\n",
      "-- srun -------------------------------\n",
      "$ srun --mpi=pmi2 -n 64 /scratch/ampemi/xxxx.xxxx/stnc/Fortran/stencil_mpi 4800 1 500\n",
      "Heat=1500.00 | Tempo=1.2484 | MPI_Size=64 | MPI_Dims=(8,8) | bx,by=(600,600)\n",
      "-- FIM --------------------------------\n",
      "========================================\n",
      "- Job ID: 781029\n",
      "- Tarefas por no:\n",
      "- Qtd. de nos: 4\n",
      "- Tot. de tarefas: 81\n",
      "- Nos alocados: sdumont[1122-1123,1429,1454]\n",
      "- diretorio onde sbatch foi chamado ($SLURM_SUBMIT_DIR):\n",
      "/prj/ampemi/xxxx.xxxx/stnc/Fortran\n",
      "sdumont1122 sdumont1123 sdumont1429 sdumont1454\n",
      "-- modulos ----------------------------\n",
      "module load gcc/7.4\n",
      "module load openmpi/gnu/4.0.1\n",
      "-- srun -------------------------------\n",
      "$ srun --mpi=pmi2 -n 81 /scratch/ampemi/xxxx.xxxx/stnc/Fortran/stencil_mpi 4800 1 500\n",
      "Heat=1500.00 | Tempo=1.0296 | MPI_Size=81 | MPI_Dims=(9,9) | bx,by=(533,533)\n",
      "-- FIM --------------------------------\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "d='/scratch/ampemi/xxxx.xxxx/stnc/Fortran'\n",
    "cat $d/slurm-781022.out\n",
    "cat $d/slurm-781023.out\n",
    "cat $d/slurm-781024.out\n",
    "cat $d/slurm-781025.out\n",
    "cat $d/slurm-781026.out\n",
    "cat $d/slurm-781027.out\n",
    "cat $d/slurm-781028.out\n",
    "cat $d/slurm-781029.out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Usar esse\n",
    "# GNU 4.8.5 e OpenMPI 4.0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting stfopagnu485_16.srm\n"
     ]
    }
   ],
   "source": [
    "%%writefile stfopagnu485_16.srm\n",
    "#!/bin/bash\n",
    "# limites das filas (1,0 UA):\n",
    "#   cpu_dev  : 20 min.,  1-4  nós, 1/1   tarefas em exec/fila máximo\n",
    "#   cpu_small: 72 horas, 1-20 nós, 16/96 tarefas em exec/fila máximo 16,32,48,64,80,96\n",
    "# 1x1=1, 2x2=4, 3x3=9, 4x4=16, 6x6=36, 7x7=49, 8x8=64, 9x9=81\n",
    "\n",
    "#SBATCH --ntasks=16            #Total de tarefas\n",
    "#SBATCH -p cpu_small           #Fila (partition) a ser utilizada\n",
    "#SBATCH -J stcforpa            #Nome do job, 8 caracteres\n",
    "#SBATCH --time=00:02:00        #Tempo max. de execução 2 minutos\n",
    "# #SBATCH --nodes=5              #Qtd de nós\n",
    "# #SBATCH --ntasks-per-node=5    #Qtd de tarefas por nó ($SLURM_NTASKS_PER_NODE)\n",
    "# #SBATCH --exclusive            #Utilização exclusiva dos nós\n",
    "\n",
    "echo '========================================'\n",
    "echo '- Job ID:' $SLURM_JOB_ID\n",
    "echo '- Tarefas por no:' $SLURM_NTASKS_PER_NODE\n",
    "echo '- Qtd. de nos:' $SLURM_JOB_NUM_NODES\n",
    "echo '- Tot. de tarefas:' $SLURM_NTASKS\n",
    "echo '- Nos alocados:' $SLURM_JOB_NODELIST\n",
    "echo '- diretorio onde sbatch foi chamado ($SLURM_SUBMIT_DIR):'\n",
    "echo $SLURM_SUBMIT_DIR\n",
    "cd $SLURM_SUBMIT_DIR\n",
    "nodeset -e $SLURM_JOB_NODELIST\n",
    "\n",
    "#Configura o ambiente\n",
    "echo '-- modulos ----------------------------'\n",
    "# echo 'module load gcc/7.4'\n",
    "# module load gcc/7.4\n",
    "echo 'module load openmpi/gnu/4.0.1'\n",
    "module load openmpi/gnu/4.0.1\n",
    "\n",
    "#Configura o executavel\n",
    "EXEC=\"/scratch/ampemi/xxxx.xxxx/stnc/Fortran/stencil_mpi 4800 1 500\"\n",
    "\n",
    "#Dispara a execucao\n",
    "echo '-- srun -------------------------------'\n",
    "echo '$ srun --mpi=pmi2 -n' $SLURM_NTASKS $EXEC\n",
    "srun --mpi=pmi2 -n $SLURM_NTASKS $EXEC\n",
    "echo '-- FIM --------------------------------'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gcc (GCC) 4.8.5 20150623 (Red Hat 4.8.5-36)\n",
      "Copyright (C) 2015 Free Software Foundation, Inc.\n",
      "This is free software; see the source for copying conditions.  There is NO\n",
      "warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.\n",
      "\n",
      "GNU Fortran (GCC) 4.8.5 20150623 (Red Hat 4.8.5-36)\n",
      "Copyright (C) 2015 Free Software Foundation, Inc.\n",
      "\n",
      "GNU Fortran comes with NO WARRANTY, to the extent permitted by law.\n",
      "You may redistribute copies of GNU Fortran\n",
      "under the terms of the GNU General Public License.\n",
      "For more information about these matters, see the file named COPYING\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "module load openmpi/gnu/4.0.1\n",
    "gcc --version\n",
    "mpif90 --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-rwxr-xr-x 1 xxxx.xxxx ampemi 23K Dec  2 22:18 stencil_mpi\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "module load openmpi/gnu/4.0.1\n",
    "mpif90  -O3  -o stencil_mpi  stencil_mpi.f90\n",
    "ls -lh stencil_mpi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testa o funcionamento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Heat=1500.00 | Tempo=20.6497 | MPI_Size=1 | MPI_Dims=(1,1) | bx,by=(4800,4800)\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "module load openmpi/gnu/4.0.1\n",
    "mpiexec -n 1 ./stencil_mpi 4800 1 500"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Executa no nó de execução"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copia o executável para /scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 769K\n",
      "-rwxr-xr-x 1 xxxx.xxxx ampemi 817K Nov  8 21:12 stencil_mpi\n",
      "-rwxr-xr-x 1 xxxx.xxxx ampemi 765K Nov  8 18:59 stencil_seq\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "a='stencil_mpi'\n",
    "b='/stnc/Fortran'\n",
    "s='/prj/ampemi/xxxx.xxxx'$b\n",
    "d='/scratch/ampemi/xxxx.xxxx'$b\n",
    "#mkdir -p $d\n",
    "cp $s/$a $d\n",
    "ls -lh $d/$a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submitted batch job 781031\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "# 1x1=1, 2x2=4, 3x3=9, 4x4=16, 6x6=36, 7x7=49, 8x8=64, 9x9=81\n",
    "#sbatch stfopagnu485_01.srm\n",
    "#sbatch stfopagnu485_04.srm\n",
    "#sbatch stfopagnu485_09.srm\n",
    "sbatch stfopagnu485_16.srm\n",
    "#sbatch stfopagnu485_36.srm\n",
    "#sbatch stfopagnu485_49.srm\n",
    "#sbatch stfopagnu485_64.srm\n",
    "#sbatch stfopagnu485_81.srm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             JOBID PARTITION     NAME     USER ST       TIME  NODES NODELIST(REASON)\n",
      "            781020 cpu_small stcforpa xxxx. PD       0:00      4 (Resources)\n",
      "            781029 cpu_small stcforpa xxxx. PD       0:00      4 (Resources)\n",
      "            781031 cpu_small stcforpa xxxx. PD       0:00      1 (Resources)\n"
     ]
    }
   ],
   "source": [
    "! squeue -n stcforpa  # verifica se já terminou"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             JOBID PARTITION     NAME     USER ST       TIME  NODES NODELIST(REASON)\n",
      "            781020 cpu_small stcforpa xxxx. PD       0:00      4 (Resources)\n",
      "            781029 cpu_small stcforpa xxxx. PD       0:00      4 (Resources)\n"
     ]
    }
   ],
   "source": [
    "! squeue -n stcforpa  # verifica se já terminou"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Primeira tomada de tempo GNU 4.8.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================================\n",
      "- Job ID: 781013\n",
      "- Tarefas por no:\n",
      "- Qtd. de nos: 1\n",
      "- Tot. de tarefas: 1\n",
      "- Nos alocados: sdumont1429\n",
      "- diretorio onde sbatch foi chamado ($SLURM_SUBMIT_DIR):\n",
      "/prj/ampemi/xxxx.xxxx/stnc/Fortran\n",
      "sdumont1429\n",
      "-- modulos ----------------------------\n",
      "module load openmpi/gnu/4.0.1\n",
      "-- srun -------------------------------\n",
      "$ srun --mpi=pmi2 -n 1 /scratch/ampemi/xxxx.xxxx/stnc/Fortran/stencil_mpi 4800 1 500\n",
      "Heat=1500.00 | Tempo=21.8917 | MPI_Size=1 | MPI_Dims=(1,1) | bx,by=(4800,4800)\n",
      "-- FIM --------------------------------\n",
      "========================================\n",
      "- Job ID: 781014\n",
      "- Tarefas por no:\n",
      "- Qtd. de nos: 1\n",
      "- Tot. de tarefas: 4\n",
      "- Nos alocados: sdumont1454\n",
      "- diretorio onde sbatch foi chamado ($SLURM_SUBMIT_DIR):\n",
      "/prj/ampemi/xxxx.xxxx/stnc/Fortran\n",
      "sdumont1454\n",
      "-- modulos ----------------------------\n",
      "module load openmpi/gnu/4.0.1\n",
      "-- srun -------------------------------\n",
      "$ srun --mpi=pmi2 -n 4 /scratch/ampemi/xxxx.xxxx/stnc/Fortran/stencil_mpi 4800 1 500\n",
      "Heat=1500.00 | Tempo=7.3403 | MPI_Size=4 | MPI_Dims=(2,2) | bx,by=(2400,2400)\n",
      "-- FIM --------------------------------\n",
      "========================================\n",
      "- Job ID: 781015\n",
      "- Tarefas por no:\n",
      "- Qtd. de nos: 1\n",
      "- Tot. de tarefas: 9\n",
      "- Nos alocados: sdumont1464\n",
      "- diretorio onde sbatch foi chamado ($SLURM_SUBMIT_DIR):\n",
      "/prj/ampemi/xxxx.xxxx/stnc/Fortran\n",
      "sdumont1464\n",
      "-- modulos ----------------------------\n",
      "module load openmpi/gnu/4.0.1\n",
      "-- srun -------------------------------\n",
      "$ srun --mpi=pmi2 -n 9 /scratch/ampemi/xxxx.xxxx/stnc/Fortran/stencil_mpi 4800 1 500\n",
      "Heat=1500.00 | Tempo=6.0767 | MPI_Size=9 | MPI_Dims=(3,3) | bx,by=(1600,1600)\n",
      "-- FIM --------------------------------\n",
      "========================================\n",
      "- Job ID: 781031\n",
      "- Tarefas por no:\n",
      "- Qtd. de nos: 1\n",
      "- Tot. de tarefas: 16\n",
      "- Nos alocados: sdumont1429\n",
      "- diretorio onde sbatch foi chamado ($SLURM_SUBMIT_DIR):\n",
      "/prj/ampemi/xxxx.xxxx/stnc/Fortran\n",
      "sdumont1429\n",
      "-- modulos ----------------------------\n",
      "module load openmpi/gnu/4.0.1\n",
      "-- srun -------------------------------\n",
      "$ srun --mpi=pmi2 -n 16 /scratch/ampemi/xxxx.xxxx/stnc/Fortran/stencil_mpi 4800 1 500\n",
      "Heat=1500.00 | Tempo=4.7581 | MPI_Size=16 | MPI_Dims=(4,4) | bx,by=(1200,1200)\n",
      "-- FIM --------------------------------\n",
      "========================================\n",
      "- Job ID: 781017\n",
      "- Tarefas por no:\n",
      "- Qtd. de nos: 2\n",
      "- Tot. de tarefas: 36\n",
      "- Nos alocados: sdumont[1454,1464]\n",
      "- diretorio onde sbatch foi chamado ($SLURM_SUBMIT_DIR):\n",
      "/prj/ampemi/xxxx.xxxx/stnc/Fortran\n",
      "sdumont1454 sdumont1464\n",
      "-- modulos ----------------------------\n",
      "module load openmpi/gnu/4.0.1\n",
      "-- srun -------------------------------\n",
      "$ srun --mpi=pmi2 -n 36 /scratch/ampemi/xxxx.xxxx/stnc/Fortran/stencil_mpi 4800 1 500\n",
      "Heat=1500.00 | Tempo=2.1853 | MPI_Size=36 | MPI_Dims=(6,6) | bx,by=(800,800)\n",
      "-- FIM --------------------------------\n",
      "========================================\n",
      "- Job ID: 781018\n",
      "- Tarefas por no:\n",
      "- Qtd. de nos: 3\n",
      "- Tot. de tarefas: 49\n",
      "- Nos alocados: sdumont[1429,1454,1464]\n",
      "- diretorio onde sbatch foi chamado ($SLURM_SUBMIT_DIR):\n",
      "/prj/ampemi/xxxx.xxxx/stnc/Fortran\n",
      "sdumont1429 sdumont1454 sdumont1464\n",
      "-- modulos ----------------------------\n",
      "module load openmpi/gnu/4.0.1\n",
      "-- srun -------------------------------\n",
      "$ srun --mpi=pmi2 -n 49 /scratch/ampemi/xxxx.xxxx/stnc/Fortran/stencil_mpi 4800 1 500\n",
      "Heat=1500.00 | Tempo=1.5905 | MPI_Size=49 | MPI_Dims=(7,7) | bx,by=(685,685)\n",
      "-- FIM --------------------------------\n",
      "========================================\n",
      "- Job ID: 781019\n",
      "- Tarefas por no:\n",
      "- Qtd. de nos: 3\n",
      "- Tot. de tarefas: 64\n",
      "- Nos alocados: sdumont[1429,1454,1464]\n",
      "- diretorio onde sbatch foi chamado ($SLURM_SUBMIT_DIR):\n",
      "/prj/ampemi/xxxx.xxxx/stnc/Fortran\n",
      "sdumont1429 sdumont1454 sdumont1464\n",
      "-- modulos ----------------------------\n",
      "module load openmpi/gnu/4.0.1\n",
      "-- srun -------------------------------\n",
      "$ srun --mpi=pmi2 -n 64 /scratch/ampemi/xxxx.xxxx/stnc/Fortran/stencil_mpi 4800 1 500\n",
      "Heat=1500.00 | Tempo=1.2578 | MPI_Size=64 | MPI_Dims=(8,8) | bx,by=(600,600)\n",
      "-- FIM --------------------------------\n",
      "========================================\n",
      "- Job ID: 781020\n",
      "- Tarefas por no:\n",
      "- Qtd. de nos: 4\n",
      "- Tot. de tarefas: 81\n",
      "- Nos alocados: sdumont[1122-1123,1429,1454]\n",
      "- diretorio onde sbatch foi chamado ($SLURM_SUBMIT_DIR):\n",
      "/prj/ampemi/xxxx.xxxx/stnc/Fortran\n",
      "sdumont1122 sdumont1123 sdumont1429 sdumont1454\n",
      "-- modulos ----------------------------\n",
      "module load openmpi/gnu/4.0.1\n",
      "-- srun -------------------------------\n",
      "$ srun --mpi=pmi2 -n 81 /scratch/ampemi/xxxx.xxxx/stnc/Fortran/stencil_mpi 4800 1 500\n",
      "Heat=1500.00 | Tempo=2.0254 | MPI_Size=81 | MPI_Dims=(9,9) | bx,by=(533,533)\n",
      "-- FIM --------------------------------\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "d='/scratch/ampemi/xxxx.xxxx/stnc/Fortran'\n",
    "cat $d/slurm-781013.out  #01\n",
    "cat $d/slurm-781014.out  #04\n",
    "cat $d/slurm-781015.out  #09\n",
    "cat $d/slurm-781031.out  #16\n",
    "cat $d/slurm-781017.out  #36\n",
    "cat $d/slurm-781018.out  #49\n",
    "cat $d/slurm-781019.out  #64\n",
    "cat $d/slurm-781020.out  #81"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Segunda tomada de tempo GNU 4.8.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submitted batch job 788003\n",
      "Submitted batch job 788004\n",
      "Submitted batch job 788005\n",
      "Submitted batch job 788006\n",
      "Submitted batch job 788007\n",
      "Submitted batch job 788008\n",
      "Submitted batch job 788009\n",
      "Submitted batch job 788010\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "# 1x1=1, 2x2=4, 3x3=9, 4x4=16, 6x6=36, 7x7=49, 8x8=64, 9x9=81\n",
    "sbatch stfopagnu485_01.srm\n",
    "sbatch stfopagnu485_04.srm\n",
    "sbatch stfopagnu485_09.srm\n",
    "sbatch stfopagnu485_16.srm\n",
    "sbatch stfopagnu485_36.srm\n",
    "sbatch stfopagnu485_49.srm\n",
    "sbatch stfopagnu485_64.srm\n",
    "sbatch stfopagnu485_81.srm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             JOBID PARTITION     NAME     USER ST       TIME  NODES NODELIST(REASON)\n",
      "            788003 cpu_small stcforpa xxxx. PD       0:00      1 (Priority)\n",
      "            788004 cpu_small stcforpa xxxx. PD       0:00      1 (Priority)\n",
      "            788005 cpu_small stcforpa xxxx. PD       0:00      1 (Priority)\n",
      "            788006 cpu_small stcforpa xxxx. PD       0:00      1 (Priority)\n",
      "            788007 cpu_small stcforpa xxxx. PD       0:00      2 (Priority)\n",
      "            788008 cpu_small stcforpa xxxx. PD       0:00      3 (Priority)\n",
      "            788009 cpu_small stcforpa xxxx. PD       0:00      3 (Priority)\n",
      "            788010 cpu_small stcforpa xxxx. PD       0:00      4 (Priority)\n"
     ]
    }
   ],
   "source": [
    "! squeue -n stcforpa  # verifica se já terminou"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             JOBID PARTITION     NAME     USER ST       TIME  NODES NODELIST(REASON)\n"
     ]
    }
   ],
   "source": [
    "! squeue -n stcforpa  # verifica se já terminou"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================================\n",
      "- Job ID: 788003\n",
      "- Tarefas por no:\n",
      "- Qtd. de nos: 1\n",
      "- Tot. de tarefas: 1\n",
      "- Nos alocados: sdumont1149\n",
      "- diretorio onde sbatch foi chamado ($SLURM_SUBMIT_DIR):\n",
      "/prj/ampemi/xxxx.xxxx/stnc/Fortran\n",
      "sdumont1149\n",
      "-- modulos ----------------------------\n",
      "module load openmpi/gnu/4.0.1\n",
      "-- srun -------------------------------\n",
      "$ srun --mpi=pmi2 -n 1 /scratch/ampemi/xxxx.xxxx/stnc/Fortran/stencil_mpi 4800 1 500\n",
      "Heat=1500.00 | Tempo=21.9799 | MPI_Size=1 | MPI_Dims=(1,1) | bx,by=(4800,4800)\n",
      "-- FIM --------------------------------\n",
      "========================================\n",
      "- Job ID: 788004\n",
      "- Tarefas por no:\n",
      "- Qtd. de nos: 1\n",
      "- Tot. de tarefas: 4\n",
      "- Nos alocados: sdumont1149\n",
      "- diretorio onde sbatch foi chamado ($SLURM_SUBMIT_DIR):\n",
      "/prj/ampemi/xxxx.xxxx/stnc/Fortran\n",
      "sdumont1149\n",
      "-- modulos ----------------------------\n",
      "module load openmpi/gnu/4.0.1\n",
      "-- srun -------------------------------\n",
      "$ srun --mpi=pmi2 -n 4 /scratch/ampemi/xxxx.xxxx/stnc/Fortran/stencil_mpi 4800 1 500\n",
      "Heat=1500.00 | Tempo=7.3660 | MPI_Size=4 | MPI_Dims=(2,2) | bx,by=(2400,2400)\n",
      "-- FIM --------------------------------\n",
      "========================================\n",
      "- Job ID: 788005\n",
      "- Tarefas por no:\n",
      "- Qtd. de nos: 1\n",
      "- Tot. de tarefas: 9\n",
      "- Nos alocados: sdumont1149\n",
      "- diretorio onde sbatch foi chamado ($SLURM_SUBMIT_DIR):\n",
      "/prj/ampemi/xxxx.xxxx/stnc/Fortran\n",
      "sdumont1149\n",
      "-- modulos ----------------------------\n",
      "module load openmpi/gnu/4.0.1\n",
      "-- srun -------------------------------\n",
      "$ srun --mpi=pmi2 -n 9 /scratch/ampemi/xxxx.xxxx/stnc/Fortran/stencil_mpi 4800 1 500\n",
      "Heat=1500.00 | Tempo=6.1560 | MPI_Size=9 | MPI_Dims=(3,3) | bx,by=(1600,1600)\n",
      "-- FIM --------------------------------\n",
      "========================================\n",
      "- Job ID: 788006\n",
      "- Tarefas por no:\n",
      "- Qtd. de nos: 1\n",
      "- Tot. de tarefas: 16\n",
      "- Nos alocados: sdumont1149\n",
      "- diretorio onde sbatch foi chamado ($SLURM_SUBMIT_DIR):\n",
      "/prj/ampemi/xxxx.xxxx/stnc/Fortran\n",
      "sdumont1149\n",
      "-- modulos ----------------------------\n",
      "module load openmpi/gnu/4.0.1\n",
      "-- srun -------------------------------\n",
      "$ srun --mpi=pmi2 -n 16 /scratch/ampemi/xxxx.xxxx/stnc/Fortran/stencil_mpi 4800 1 500\n",
      "Heat=1500.00 | Tempo=4.7203 | MPI_Size=16 | MPI_Dims=(4,4) | bx,by=(1200,1200)\n",
      "-- FIM --------------------------------\n",
      "========================================\n",
      "- Job ID: 788007\n",
      "- Tarefas por no:\n",
      "- Qtd. de nos: 2\n",
      "- Tot. de tarefas: 36\n",
      "- Nos alocados: sdumont[1149,1272]\n",
      "- diretorio onde sbatch foi chamado ($SLURM_SUBMIT_DIR):\n",
      "/prj/ampemi/xxxx.xxxx/stnc/Fortran\n",
      "sdumont1149 sdumont1272\n",
      "-- modulos ----------------------------\n",
      "module load openmpi/gnu/4.0.1\n",
      "-- srun -------------------------------\n",
      "$ srun --mpi=pmi2 -n 36 /scratch/ampemi/xxxx.xxxx/stnc/Fortran/stencil_mpi 4800 1 500\n",
      "Heat=1500.00 | Tempo=2.1258 | MPI_Size=36 | MPI_Dims=(6,6) | bx,by=(800,800)\n",
      "-- FIM --------------------------------\n",
      "========================================\n",
      "- Job ID: 788008\n",
      "- Tarefas por no:\n",
      "- Qtd. de nos: 3\n",
      "- Tot. de tarefas: 49\n",
      "- Nos alocados: sdumont[1083,1149,1272]\n",
      "- diretorio onde sbatch foi chamado ($SLURM_SUBMIT_DIR):\n",
      "/prj/ampemi/xxxx.xxxx/stnc/Fortran\n",
      "sdumont1083 sdumont1149 sdumont1272\n",
      "-- modulos ----------------------------\n",
      "module load openmpi/gnu/4.0.1\n",
      "-- srun -------------------------------\n",
      "$ srun --mpi=pmi2 -n 49 /scratch/ampemi/xxxx.xxxx/stnc/Fortran/stencil_mpi 4800 1 500\n",
      "Heat=1500.00 | Tempo=1.5254 | MPI_Size=49 | MPI_Dims=(7,7) | bx,by=(685,685)\n",
      "-- FIM --------------------------------\n",
      "========================================\n",
      "- Job ID: 788009\n",
      "- Tarefas por no:\n",
      "- Qtd. de nos: 3\n",
      "- Tot. de tarefas: 64\n",
      "- Nos alocados: sdumont[1083,1149,1272]\n",
      "- diretorio onde sbatch foi chamado ($SLURM_SUBMIT_DIR):\n",
      "/prj/ampemi/xxxx.xxxx/stnc/Fortran\n",
      "sdumont1083 sdumont1149 sdumont1272\n",
      "-- modulos ----------------------------\n",
      "module load openmpi/gnu/4.0.1\n",
      "-- srun -------------------------------\n",
      "$ srun --mpi=pmi2 -n 64 /scratch/ampemi/xxxx.xxxx/stnc/Fortran/stencil_mpi 4800 1 500\n",
      "Heat=1500.00 | Tempo=1.2199 | MPI_Size=64 | MPI_Dims=(8,8) | bx,by=(600,600)\n",
      "-- FIM --------------------------------\n",
      "========================================\n",
      "- Job ID: 788010\n",
      "- Tarefas por no:\n",
      "- Qtd. de nos: 4\n",
      "- Tot. de tarefas: 81\n",
      "- Nos alocados: sdumont[1083,1149,1391,1410]\n",
      "- diretorio onde sbatch foi chamado ($SLURM_SUBMIT_DIR):\n",
      "/prj/ampemi/xxxx.xxxx/stnc/Fortran\n",
      "sdumont1083 sdumont1149 sdumont1391 sdumont1410\n",
      "-- modulos ----------------------------\n",
      "module load openmpi/gnu/4.0.1\n",
      "-- srun -------------------------------\n",
      "$ srun --mpi=pmi2 -n 81 /scratch/ampemi/xxxx.xxxx/stnc/Fortran/stencil_mpi 4800 1 500\n",
      "Heat=1500.00 | Tempo=2.0314 | MPI_Size=81 | MPI_Dims=(9,9) | bx,by=(533,533)\n",
      "-- FIM --------------------------------\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "d='/scratch/ampemi/xxxx.xxxx/stnc/Fortran'\n",
    "cat $d/slurm-788003.out  #01\n",
    "cat $d/slurm-788004.out  #04\n",
    "cat $d/slurm-788005.out  #09\n",
    "cat $d/slurm-788006.out  #16\n",
    "cat $d/slurm-788007.out  #36\n",
    "cat $d/slurm-788008.out  #49\n",
    "cat $d/slurm-788009.out  #64\n",
    "cat $d/slurm-788010.out  #81"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Terceira tomada de tempo GNU 4.8.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submitted batch job 788013\n",
      "Submitted batch job 788014\n",
      "Submitted batch job 788015\n",
      "Submitted batch job 788016\n",
      "Submitted batch job 788017\n",
      "Submitted batch job 788018\n",
      "Submitted batch job 788019\n",
      "Submitted batch job 788020\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "# 1x1=1, 2x2=4, 3x3=9, 4x4=16, 6x6=36, 7x7=49, 8x8=64, 9x9=81\n",
    "sbatch stfopagnu485_01.srm\n",
    "sbatch stfopagnu485_04.srm\n",
    "sbatch stfopagnu485_09.srm\n",
    "sbatch stfopagnu485_16.srm\n",
    "sbatch stfopagnu485_36.srm\n",
    "sbatch stfopagnu485_49.srm\n",
    "sbatch stfopagnu485_64.srm\n",
    "sbatch stfopagnu485_81.srm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             JOBID PARTITION     NAME     USER ST       TIME  NODES NODELIST(REASON)\n",
      "            788009 cpu_small stcforpa xxxx. PD       0:00      3 (Resources)\n",
      "            788010 cpu_small stcforpa xxxx. PD       0:00      4 (Resources)\n",
      "            788007 cpu_small stcforpa xxxx. PD       0:00      2 (Resources)\n",
      "            788008 cpu_small stcforpa xxxx. PD       0:00      3 (Resources)\n",
      "            788006 cpu_small stcforpa xxxx. PD       0:00      1 (Resources)\n",
      "            788005 cpu_small stcforpa xxxx. PD       0:00      1 (Resources)\n",
      "            788004 cpu_small stcforpa xxxx. PD       0:00      1 (Resources)\n",
      "            788003 cpu_small stcforpa xxxx. PD       0:00      1 (Resources)\n",
      "            788013 cpu_small stcforpa xxxx. PD       0:00      1 (Priority)\n",
      "            788014 cpu_small stcforpa xxxx. PD       0:00      1 (Priority)\n",
      "            788015 cpu_small stcforpa xxxx. PD       0:00      1 (Priority)\n",
      "            788016 cpu_small stcforpa xxxx. PD       0:00      1 (Priority)\n",
      "            788017 cpu_small stcforpa xxxx. PD       0:00      2 (Priority)\n",
      "            788018 cpu_small stcforpa xxxx. PD       0:00      3 (Priority)\n",
      "            788019 cpu_small stcforpa xxxx. PD       0:00      3 (Priority)\n",
      "            788020 cpu_small stcforpa xxxx. PD       0:00      4 (Priority)\n"
     ]
    }
   ],
   "source": [
    "! squeue -n stcforpa  # verifica se já terminou"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             JOBID PARTITION     NAME     USER ST       TIME  NODES NODELIST(REASON)\n"
     ]
    }
   ],
   "source": [
    "! squeue -n stcforpa  # verifica se já terminou"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mostra os arquivos de saída GNU Fortran (GCC) 4.8.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================================\n",
      "- Job ID: 788013\n",
      "- Tarefas por no:\n",
      "- Qtd. de nos: 1\n",
      "- Tot. de tarefas: 1\n",
      "- Nos alocados: sdumont1149\n",
      "- diretorio onde sbatch foi chamado ($SLURM_SUBMIT_DIR):\n",
      "/prj/ampemi/xxxx.xxxx/stnc/Fortran\n",
      "sdumont1149\n",
      "-- modulos ----------------------------\n",
      "module load openmpi/gnu/4.0.1\n",
      "-- srun -------------------------------\n",
      "$ srun --mpi=pmi2 -n 1 /scratch/ampemi/xxxx.xxxx/stnc/Fortran/stencil_mpi 4800 1 500\n",
      "Heat=1500.00 | Tempo=21.8653 | MPI_Size=1 | MPI_Dims=(1,1) | bx,by=(4800,4800)\n",
      "-- FIM --------------------------------\n",
      "========================================\n",
      "- Job ID: 788014\n",
      "- Tarefas por no:\n",
      "- Qtd. de nos: 1\n",
      "- Tot. de tarefas: 4\n",
      "- Nos alocados: sdumont1149\n",
      "- diretorio onde sbatch foi chamado ($SLURM_SUBMIT_DIR):\n",
      "/prj/ampemi/xxxx.xxxx/stnc/Fortran\n",
      "sdumont1149\n",
      "-- modulos ----------------------------\n",
      "module load openmpi/gnu/4.0.1\n",
      "-- srun -------------------------------\n",
      "$ srun --mpi=pmi2 -n 4 /scratch/ampemi/xxxx.xxxx/stnc/Fortran/stencil_mpi 4800 1 500\n",
      "Heat=1500.00 | Tempo=7.3280 | MPI_Size=4 | MPI_Dims=(2,2) | bx,by=(2400,2400)\n",
      "-- FIM --------------------------------\n",
      "========================================\n",
      "- Job ID: 788015\n",
      "- Tarefas por no:\n",
      "- Qtd. de nos: 1\n",
      "- Tot. de tarefas: 9\n",
      "- Nos alocados: sdumont1149\n",
      "- diretorio onde sbatch foi chamado ($SLURM_SUBMIT_DIR):\n",
      "/prj/ampemi/xxxx.xxxx/stnc/Fortran\n",
      "sdumont1149\n",
      "-- modulos ----------------------------\n",
      "module load openmpi/gnu/4.0.1\n",
      "-- srun -------------------------------\n",
      "$ srun --mpi=pmi2 -n 9 /scratch/ampemi/xxxx.xxxx/stnc/Fortran/stencil_mpi 4800 1 500\n",
      "Heat=1500.00 | Tempo=6.2291 | MPI_Size=9 | MPI_Dims=(3,3) | bx,by=(1600,1600)\n",
      "-- FIM --------------------------------\n",
      "========================================\n",
      "- Job ID: 788016\n",
      "- Tarefas por no:\n",
      "- Qtd. de nos: 1\n",
      "- Tot. de tarefas: 16\n",
      "- Nos alocados: sdumont1149\n",
      "- diretorio onde sbatch foi chamado ($SLURM_SUBMIT_DIR):\n",
      "/prj/ampemi/xxxx.xxxx/stnc/Fortran\n",
      "sdumont1149\n",
      "-- modulos ----------------------------\n",
      "module load openmpi/gnu/4.0.1\n",
      "-- srun -------------------------------\n",
      "$ srun --mpi=pmi2 -n 16 /scratch/ampemi/xxxx.xxxx/stnc/Fortran/stencil_mpi 4800 1 500\n",
      "Heat=1500.00 | Tempo=4.5761 | MPI_Size=16 | MPI_Dims=(4,4) | bx,by=(1200,1200)\n",
      "-- FIM --------------------------------\n",
      "========================================\n",
      "- Job ID: 788017\n",
      "- Tarefas por no:\n",
      "- Qtd. de nos: 2\n",
      "- Tot. de tarefas: 36\n",
      "- Nos alocados: sdumont[1083,1149]\n",
      "- diretorio onde sbatch foi chamado ($SLURM_SUBMIT_DIR):\n",
      "/prj/ampemi/xxxx.xxxx/stnc/Fortran\n",
      "sdumont1083 sdumont1149\n",
      "-- modulos ----------------------------\n",
      "module load openmpi/gnu/4.0.1\n",
      "-- srun -------------------------------\n",
      "$ srun --mpi=pmi2 -n 36 /scratch/ampemi/xxxx.xxxx/stnc/Fortran/stencil_mpi 4800 1 500\n",
      "Heat=1500.00 | Tempo=2.0752 | MPI_Size=36 | MPI_Dims=(6,6) | bx,by=(800,800)\n",
      "-- FIM --------------------------------\n",
      "========================================\n",
      "- Job ID: 788018\n",
      "- Tarefas por no:\n",
      "- Qtd. de nos: 3\n",
      "- Tot. de tarefas: 49\n",
      "- Nos alocados: sdumont[1083,1149,1272]\n",
      "- diretorio onde sbatch foi chamado ($SLURM_SUBMIT_DIR):\n",
      "/prj/ampemi/xxxx.xxxx/stnc/Fortran\n",
      "sdumont1083 sdumont1149 sdumont1272\n",
      "-- modulos ----------------------------\n",
      "module load openmpi/gnu/4.0.1\n",
      "-- srun -------------------------------\n",
      "$ srun --mpi=pmi2 -n 49 /scratch/ampemi/xxxx.xxxx/stnc/Fortran/stencil_mpi 4800 1 500\n",
      "Heat=1500.00 | Tempo=2.5394 | MPI_Size=49 | MPI_Dims=(7,7) | bx,by=(685,685)\n",
      "-- FIM --------------------------------\n",
      "========================================\n",
      "- Job ID: 788019\n",
      "- Tarefas por no:\n",
      "- Qtd. de nos: 3\n",
      "- Tot. de tarefas: 64\n",
      "- Nos alocados: sdumont[1083,1149,1272]\n",
      "- diretorio onde sbatch foi chamado ($SLURM_SUBMIT_DIR):\n",
      "/prj/ampemi/xxxx.xxxx/stnc/Fortran\n",
      "sdumont1083 sdumont1149 sdumont1272\n",
      "-- modulos ----------------------------\n",
      "module load openmpi/gnu/4.0.1\n",
      "-- srun -------------------------------\n",
      "$ srun --mpi=pmi2 -n 64 /scratch/ampemi/xxxx.xxxx/stnc/Fortran/stencil_mpi 4800 1 500\n",
      "Heat=1500.00 | Tempo=1.2071 | MPI_Size=64 | MPI_Dims=(8,8) | bx,by=(600,600)\n",
      "-- FIM --------------------------------\n",
      "========================================\n",
      "- Job ID: 788020\n",
      "- Tarefas por no:\n",
      "- Qtd. de nos: 4\n",
      "- Tot. de tarefas: 81\n",
      "- Nos alocados: sdumont[1083,1149,1391,1410]\n",
      "- diretorio onde sbatch foi chamado ($SLURM_SUBMIT_DIR):\n",
      "/prj/ampemi/xxxx.xxxx/stnc/Fortran\n",
      "sdumont1083 sdumont1149 sdumont1391 sdumont1410\n",
      "-- modulos ----------------------------\n",
      "module load openmpi/gnu/4.0.1\n",
      "-- srun -------------------------------\n",
      "$ srun --mpi=pmi2 -n 81 /scratch/ampemi/xxxx.xxxx/stnc/Fortran/stencil_mpi 4800 1 500\n",
      "Heat=1500.00 | Tempo=.9991 | MPI_Size=81 | MPI_Dims=(9,9) | bx,by=(533,533)\n",
      "-- FIM --------------------------------\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "d='/scratch/ampemi/xxxx.xxxx/stnc/Fortran'\n",
    "cat $d/slurm-788013.out  #01\n",
    "cat $d/slurm-788014.out  #04\n",
    "cat $d/slurm-788015.out  #09\n",
    "cat $d/slurm-788016.out  #16\n",
    "cat $d/slurm-788017.out  #36\n",
    "cat $d/slurm-788018.out  #49\n",
    "cat $d/slurm-788019.out  #64\n",
    "cat $d/slurm-788020.out  #81"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
