{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stencil Numba GPU B715 MPI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ng2.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ng2.py\n",
    "import numpy as np, math\n",
    "from time import time\n",
    "from mpi4py import MPI\n",
    "from numba import cuda, njit, prange, config\n",
    "\n",
    "# parameters\n",
    "n            = 4800    # n x n grid\n",
    "energy       = 1.0     # energy to be injected per iteration\n",
    "niters       = 500     # number of iterations\n",
    "# initialize three heat sources\n",
    "nsources     = 3       # number of sources of energy\n",
    "sources      = np.zeros((nsources, 2), np.int16)\n",
    "sources[:,:] = [ [n//2, n//2], [n//3, n//3], [n*4//5, n*8//9] ]\n",
    "# initialize the data arrays\n",
    "anew         = np.zeros((n + 2, n + 2), np.float64)\n",
    "aold         = np.zeros((n + 2, n + 2), np.float64)\n",
    "\n",
    "# configure blocks & grids\n",
    "## set the number of threads in a block\n",
    "threads_per_block = (16, 16)    # based on trial and error\n",
    "## calculate the number of thread blocks in the grid\n",
    "blocks_per_grid_x = math.ceil(aold.shape[0] / threads_per_block[0])\n",
    "blocks_per_grid_y = math.ceil(aold.shape[1] / threads_per_block[1])\n",
    "blocks_per_grid   = (blocks_per_grid_x, blocks_per_grid_y)\n",
    "\n",
    "# computationally intensive core\n",
    "@cuda.jit\n",
    "def kernel(A, B):\n",
    "    n = A.shape[0] - 1\n",
    "    i, j = cuda.grid(2)\n",
    "    if (i > 0 and j > 0) and (i < n and j < n) :\n",
    "        A[i,j]=B[i,j]*.5+(B[i-1,j]+B[i+1,j]+B[i,j-1]+B[i,j+1])*.125\n",
    "\n",
    "# start of main routine\n",
    "\n",
    "#---mpi4py---\n",
    "comm  = MPI.COMM_WORLD            # MPI default communicator\n",
    "size  = comm.Get_size()           # MPI size\n",
    "rank  = comm.Get_rank()           # MPI rank\n",
    "name  = MPI.Get_processor_name()  # core hostname (eg sdumont3170)\n",
    "\n",
    "#Only 2 processes per node are selected via Slurm. Within a node, color \n",
    "#rank 0 corresponds to the first process of this node, and color rank 1 \n",
    "#corresponds to the second process of this node, and the other nodes are \n",
    "#similar. Example:\n",
    "#  node      rank  color rank\n",
    "#----------- ----  ----------\n",
    "#sdumont3170   0        0\n",
    "#sdumont3170   1        1\n",
    "#sdumont3171   2        0\n",
    "#sdumont3171   3        1\n",
    "#sdumont3172   4        0\n",
    "#sdumont3172   5        1\n",
    "#sdumont3173   6        0\n",
    "#sdumont3173   7        1\n",
    "for i, c in enumerate(name) :     # find first digit in hostname\n",
    "    if c.isdigit() :\n",
    "        break\n",
    "mcol  = int(name[i:])             # extract number from hostname\n",
    "scomm = comm.Split(color = mcol)  # new communicator for the node\n",
    "crank = scomm.Get_rank()          # get the node color rank\n",
    "\n",
    "#---numba.cuda---\n",
    "#In this implementation, Slurm is configured to run only 2 processes on \n",
    "#each node. For each of these processes (cores), a single GPU is \n",
    "#associated. Thus, within a node, color rank 0 is associated with GPU 0, \n",
    "#and color rank 1 is associated with GPU 1.\n",
    "cuda.select_device(crank)         # 'color rank' 0 = 'gpu id' 0, etc.\n",
    "cid = cuda.current_context().device.id\n",
    "\n",
    "# time measurement for rank 0\n",
    "if not rank :\n",
    "    tt = -time()    # rank 0 time\n",
    "    tk = 0          # accumulate kernel time\n",
    "    tc = 0          # accumulate GPU communication time\n",
    "    te = 0          # energy insertion time\n",
    "\n",
    "# determine my coordinates (x,y)\n",
    "pdims = MPI.Compute_dims(size, 2)\n",
    "px    = pdims[0]\n",
    "py    = pdims[1]\n",
    "rx    = rank % px\n",
    "ry    = rank // px\n",
    "\n",
    "# determine my four neighbors\n",
    "north = (ry - 1) * px + rx\n",
    "if (ry - 1) < 0 :\n",
    "    north = MPI.PROC_NULL\n",
    "south = (ry + 1) * px + rx\n",
    "if (ry + 1) >= py :\n",
    "    south = MPI.PROC_NULL\n",
    "west = ry * px + rx - 1\n",
    "if (rx - 1) < 0 :\n",
    "    west = MPI.PROC_NULL\n",
    "east = ry * px + rx + 1\n",
    "if (rx + 1) >= px :\n",
    "    east = MPI.PROC_NULL\n",
    "\n",
    "# decompose the domain\n",
    "bx   = n // px          # block size in x\n",
    "by   = n // py          # block size in y\n",
    "offx = rx * bx + 1      # offset in x\n",
    "offy = ry * by + 1      # offset in y\n",
    "\n",
    "# sources in my area, local to my rank\n",
    "locnsources = 0\n",
    "locsources  = np.empty((nsources, 2), np.int16)\n",
    "\n",
    "# determine which sources are in my patch\n",
    "for i in range(nsources) :\n",
    "    locx = sources[i, 0] - offx\n",
    "    locy = sources[i, 1] - offy\n",
    "    if(locx >= 0 and locx <= bx and locy >= 0 and locy <= by) :\n",
    "        locsources[locnsources, 0] = locx\n",
    "        locsources[locnsources, 1] = locy\n",
    "        locnsources += 1\n",
    "\n",
    "# working arrays with 1-wide halo zones\n",
    "anew = np.zeros((bx+2, by+2), np.float64)\n",
    "aold = np.zeros((bx+2, by+2), np.float64)\n",
    "\n",
    "# system total heat\n",
    "rheat = np.zeros(1, np.float64)\n",
    "bheat = np.zeros(1, np.float64)\n",
    "\n",
    "# copy the first arrays to the device\n",
    "if not rank : tc -= time()\n",
    "anew_global_mem    = cuda.to_device(anew)\n",
    "aold_global_mem    = cuda.to_device(aold)\n",
    "if not rank : tc += time()\n",
    "   \n",
    "# main loop\n",
    "for _ in range(0, niters, 2) :\n",
    "\n",
    "    # exchange data with neighbors\n",
    "    if north != MPI.PROC_NULL :\n",
    "        r1=comm.irecv(source=north, tag=1)\n",
    "        s1=comm.isend(aold[1, 1:bx+1], dest=north, tag=1)\n",
    "    if south != MPI.PROC_NULL :\n",
    "        r2=comm.irecv(source=south, tag=1)\n",
    "        s2=comm.isend(aold[bx, 1:bx+1], dest=south, tag=1)\n",
    "    if east != MPI.PROC_NULL :\n",
    "        r3 = comm.irecv(source=east, tag=1)\n",
    "        s3 = comm.isend(aold[1:bx+1, bx], dest=east, tag=1)\n",
    "    if west != MPI.PROC_NULL :\n",
    "        r4 = comm.irecv(source=west, tag=1)\n",
    "        s4 = comm.isend(aold[1:bx+1, 1], dest=west, tag=1)\n",
    "    # wait for the end of communication\n",
    "    if north != MPI.PROC_NULL :\n",
    "        s1.wait()\n",
    "        aold[0, 1:bx+1] = r1.wait()\n",
    "    if south != MPI.PROC_NULL :\n",
    "        s2.wait()\n",
    "        aold[bx+1, 1:bx+1] = r2.wait()\n",
    "    if east != MPI.PROC_NULL :\n",
    "        s3.wait()\n",
    "        aold[1:bx+1, bx+1] = r3.wait()\n",
    "    if west != MPI.PROC_NULL :\n",
    "        s4.wait\n",
    "        aold[1:bx+1, 0] = r4.wait()\n",
    "\n",
    "    # copy the received array to the device\n",
    "    if not rank : tc -= time()\n",
    "    aold_global_mem = cuda.to_device(aold)\n",
    "    if not rank : tc += time()\n",
    "        \n",
    "    # update grid\n",
    "    if not rank : tk -= time()\n",
    "    kernel[blocks_per_grid, threads_per_block](\n",
    "        anew_global_mem, aold_global_mem)\n",
    "    if not rank : tk += time()\n",
    "        \n",
    "    # copy the result back to the host\n",
    "    if not rank : tc -= time()\n",
    "    anew = anew_global_mem.copy_to_host()\n",
    "    if not rank : tc += time()\n",
    "        \n",
    "    # refresh heat sources\n",
    "    if not rank : te -= time()\n",
    "    for i in range(locnsources) :\n",
    "        anew[locsources[i, 0]-1, locsources[i, 1]-1] += energy\n",
    "    if not rank : te += time()\n",
    "\n",
    "    # exchange data with neighbors\n",
    "    if north != MPI.PROC_NULL :\n",
    "        r1=comm.irecv(source=north, tag=1)\n",
    "        s1=comm.isend(anew[1, 1:bx+1], dest=north, tag=1)\n",
    "    if south != MPI.PROC_NULL :\n",
    "        r2=comm.irecv(source=south, tag=1)\n",
    "        s2=comm.isend(anew[bx, 1:bx+1], dest=south, tag=1)\n",
    "    if east != MPI.PROC_NULL :\n",
    "        r3 = comm.irecv(source=east, tag=1)\n",
    "        s3 = comm.isend(anew[1:bx+1, bx], dest=east, tag=1)\n",
    "    if west != MPI.PROC_NULL :\n",
    "        r4 = comm.irecv(source=west, tag=1)\n",
    "        s4 = comm.isend(anew[1:bx+1, 1], dest=west, tag=1)\n",
    "    # wait for the end of communication\n",
    "    if north != MPI.PROC_NULL :\n",
    "        s1.wait()\n",
    "        anew[0, 1:bx+1] = r1.wait()\n",
    "    if south != MPI.PROC_NULL :\n",
    "        s2.wait()\n",
    "        anew[bx+1, 1:bx+1] = r2.wait()\n",
    "    if east != MPI.PROC_NULL :\n",
    "        s3.wait()\n",
    "        anew[1:bx+1, bx+1] = r3.wait()\n",
    "    if west != MPI.PROC_NULL :\n",
    "        s4.wait\n",
    "        anew[1:bx+1, 0] = r4.wait()\n",
    "\n",
    "    # copy the received array to the device\n",
    "    if not rank : tc -= time()\n",
    "    anew_global_mem = cuda.to_device(anew)\n",
    "    if not rank : tc += time()\n",
    "\n",
    "    # update grid\n",
    "    if not rank : tk -= time()\n",
    "    kernel[blocks_per_grid, threads_per_block](\n",
    "        aold_global_mem, anew_global_mem)\n",
    "    if not rank : tk += time()\n",
    "        \n",
    "    # copy the result back to the host\n",
    "    if not rank : tc -= time()\n",
    "    aold = aold_global_mem.copy_to_host()\n",
    "    if not rank : tc += time()\n",
    "        \n",
    "    # refresh heat sources\n",
    "    if not rank : te -= time()\n",
    "    for i in range(locnsources) :\n",
    "        aold[locsources[i, 0]-1, locsources[i, 1]-1] += energy\n",
    "    if not rank : te += time()\n",
    "\n",
    "# end for\n",
    "\n",
    "# get final heat in the system\n",
    "bheat[0] = np.sum(aold[1:-1, 1:-1])\n",
    "comm.Reduce(bheat, rheat)\n",
    "\n",
    "# show the result\n",
    "print(f\"3. {name:11s}   {rank:02d}    {crank:02d}   {cid:02d}\")\n",
    "if not rank :\n",
    "    tt += time()\n",
    "    print( \"1. hostname    rank crank  cid\")\n",
    "    print( \"2. ----------- ---- ----- ----\")\n",
    "    print( \"4. ---------------------------\")\n",
    "    print(f\"5. Heat:{rheat[0]:.4f}\", end=\", \")\n",
    "    print(f\"TT:{tt:.4f}\", end=\", \")\n",
    "    print(f\"TK:{tk:.4f}\", end=\", \")\n",
    "    print(f\"TC:{tc:.4f}\", end=\", \")\n",
    "    print(f\"TE:{te:.4f}\", end=\", \")\n",
    "    print(f\"MPI:{size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ng2.py\n"
     ]
    }
   ],
   "source": [
    "# era só para fazer um teste\n",
    "%%writefile xxxxng2.py\n",
    "import numpy as np, math\n",
    "from time import time\n",
    "from mpi4py import MPI\n",
    "from numba import cuda, njit, prange, config\n",
    "\n",
    "# parameters\n",
    "n            = 480    # n x n grid\n",
    "energy       = 1.0     # energy to be injected per iteration\n",
    "niters       = 500     # number of iterations\n",
    "# initialize three heat sources\n",
    "nsources     = 3       # number of sources of energy\n",
    "sources      = np.zeros((nsources, 2), np.int16)\n",
    "sources[:,:] = [ [n//2, n//2], [n//3, n//3], [n*4//5, n*8//9] ]\n",
    "# initialize the data arrays\n",
    "anew         = np.zeros((n + 2, n + 2), np.float64)\n",
    "aold         = np.zeros((n + 2, n + 2), np.float64)\n",
    "\n",
    "# configure blocks & grids\n",
    "## set the number of threads in a block\n",
    "threads_per_block = (16, 16)    # based on trial and error\n",
    "## calculate the number of thread blocks in the grid\n",
    "blocks_per_grid_x = math.ceil(aold.shape[0] / threads_per_block[0])\n",
    "blocks_per_grid_y = math.ceil(aold.shape[1] / threads_per_block[1])\n",
    "blocks_per_grid   = (blocks_per_grid_x, blocks_per_grid_y)\n",
    "\n",
    "# computationally intensive core\n",
    "@cuda.jit\n",
    "def kernel(A, B):\n",
    "    n = A.shape[0] - 1\n",
    "    i, j = cuda.grid(2)\n",
    "    if (i > 0 and j > 0) and (i < n and j < n) :\n",
    "        A[i,j]=B[i,j]*.5+(B[i-1,j]+B[i+1,j]+B[i,j-1]+B[i,j+1])*.125\n",
    "\n",
    "# start of main routine\n",
    "\n",
    "#---mpi4py---\n",
    "comm  = MPI.COMM_WORLD            # MPI default communicator\n",
    "size  = comm.Get_size()           # MPI size\n",
    "rank  = comm.Get_rank()           # MPI rank\n",
    "name  = MPI.Get_processor_name()  # core hostname (eg sdumont3170)\n",
    "\n",
    "#Only 2 processes per node are selected via Slurm. Within a node, color \n",
    "#rank 0 corresponds to the first process of this node, and color rank 1 \n",
    "#corresponds to the second process of this node, and the other nodes are \n",
    "#similar. Example:\n",
    "#  node      rank  color rank\n",
    "#----------- ----  ----------\n",
    "#sdumont3170   0        0\n",
    "#sdumont3170   1        1\n",
    "#sdumont3171   2        0\n",
    "#sdumont3171   3        1\n",
    "#sdumont3172   4        0\n",
    "#sdumont3172   5        1\n",
    "#sdumont3173   6        0\n",
    "#sdumont3173   7        1\n",
    "for i, c in enumerate(name) :     # find first digit in hostname\n",
    "    if c.isdigit() :\n",
    "        break\n",
    "mcol  = int(name[i:])             # extract number from hostname\n",
    "scomm = comm.Split(color = mcol)  # new communicator for the node\n",
    "crank = scomm.Get_rank()          # get the node color rank\n",
    "\n",
    "#---numba.cuda---\n",
    "#In this implementation, Slurm is configured to run only 2 processes on \n",
    "#each node. For each of these processes (cores), a single GPU is \n",
    "#associated. Thus, within a node, color rank 0 is associated with GPU 0, \n",
    "#and color rank 1 is associated with GPU 1.\n",
    "\n",
    "# cuda.select_device(crank)         # 'color rank' 0 = 'gpu id' 0, etc.\n",
    "cid = cuda.current_context().device.id\n",
    "\n",
    "cuda.select_device(crank)\n",
    "print(size, rank, name, mcol, crank, cid)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Para testes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing ng3.srm\n"
     ]
    }
   ],
   "source": [
    "%%writefile ng3.srm\n",
    "#!/bin/bash\n",
    "#SBATCH --job-name ng3         # Job name\n",
    "#SBATCH --partition nvidia_dev # Select partition\n",
    "#SBATCH --ntasks-per-node=2    # Tasks per node\n",
    "#SBATCH --nodes=2              # Minimum to be allocated\n",
    "#SBATCH --ntasks=4             # Total tasks\n",
    "#SBATCH --time=00:01:00        # Limit execution time\n",
    "#SBATCH --exclusive            # Exclusive acccess to nodes\n",
    "\n",
    "echo '========================================'\n",
    "echo '- Job ID:' $SLURM_JOB_ID\n",
    "echo '- Tasks per node:' $SLURM_NTASKS_PER_NODE\n",
    "echo '- # of nodes in the job:' $SLURM_JOB_NUM_NODES\n",
    "echo '- # of tasks:' $SLURM_NTASKS\n",
    "echo '- Partition:' $SLURM_JOB_PARTITION\n",
    "echo '- Dir from which sbatch was invoked:' ${SLURM_SUBMIT_DIR##*/}\n",
    "cd $SLURM_SUBMIT_DIR\n",
    "echo -n '- List of nodes allocated to the job: '\n",
    "nodeset -e $SLURM_JOB_NODELIST\n",
    "\n",
    "# Environment\n",
    "echo '-- modules ----------------------------'\n",
    "echo 'conda activate env2, --stack env3'\n",
    "\n",
    "cd\n",
    "SCR=/scratch${PWD#/prj}\n",
    "cd $SCR\n",
    "source $SCR/env2/etc/profile.d/conda.sh\n",
    "conda activate $SCR/env2\n",
    "conda activate --stack $SCR/env3\n",
    "cd $SCR/b715\n",
    "\n",
    "# Executable\n",
    "EXEC=\"python ng2.py\"\n",
    "\n",
    "hostname\n",
    "nvidia-smi\n",
    "\n",
    "\n",
    "# Start\n",
    "OPT='--mpi=pmi2 --cpu_bind=cores --distribution=block:cyclic'\n",
    "echo '-- run --------------------------------'\n",
    "echo '$ srun -n' $SLURM_NTASKS ${EXEC##*/}\n",
    "echo '-- output -----------------------------'\n",
    "# srun  $OPT  -n $SLURM_NTASKS  $EXEC  | sort\n",
    "echo '~~ end ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "! cp ng2.py /scratch${PWD#/prj}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ng2.srm\n"
     ]
    }
   ],
   "source": [
    "%%writefile ng2.srm\n",
    "#!/bin/bash\n",
    "#SBATCH --job-name ng2         # Job name\n",
    "#SBATCH --partition nvidia_dev # Select partition\n",
    "#SBATCH --ntasks-per-node=2    # Tasks per node\n",
    "#SBATCH --nodes=2              # Minimum to be allocated\n",
    "#SBATCH --ntasks=4             # Total tasks\n",
    "#SBATCH --time=00:01:00        # Limit execution time\n",
    "#SBATCH --exclusive            # Exclusive acccess to nodes\n",
    "\n",
    "echo '========================================'\n",
    "echo '- Job ID:' $SLURM_JOB_ID\n",
    "echo '- Tasks per node:' $SLURM_NTASKS_PER_NODE\n",
    "echo '- # of nodes in the job:' $SLURM_JOB_NUM_NODES\n",
    "echo '- # of tasks:' $SLURM_NTASKS\n",
    "echo '- Partition:' $SLURM_JOB_PARTITION\n",
    "echo '- Dir from which sbatch was invoked:' ${SLURM_SUBMIT_DIR##*/}\n",
    "cd $SLURM_SUBMIT_DIR\n",
    "echo -n '- List of nodes allocated to the job: '\n",
    "nodeset -e $SLURM_JOB_NODELIST\n",
    "\n",
    "# Environment\n",
    "echo '-- modules ----------------------------'\n",
    "echo 'conda activate env2, --stack env3'\n",
    "\n",
    "cd\n",
    "SCR=/scratch${PWD#/prj}\n",
    "cd $SCR\n",
    "source $SCR/env2/etc/profile.d/conda.sh\n",
    "conda activate $SCR/env2\n",
    "conda activate --stack $SCR/env3\n",
    "cd $SCR/b715\n",
    "\n",
    "# Executable\n",
    "EXEC=\"python ng2.py\"\n",
    "\n",
    "# Start\n",
    "OPT='--mpi=pmi2 --cpu_bind=cores --distribution=block:cyclic'\n",
    "echo '-- run --------------------------------'\n",
    "echo '$ srun -n' $SLURM_NTASKS ${EXEC##*/}\n",
    "echo '-- output -----------------------------'\n",
    "srun  $OPT  -n $SLURM_NTASKS  $EXEC  | sort\n",
    "echo '~~ end ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1x1=1(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submitted batch job 10351723\n"
     ]
    }
   ],
   "source": [
    "! sbatch  --ntasks-per-node=1  --nodes=1  --ntasks=1  ng2.srm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         START_TIME     JOBID   PARTITION  NAME  ST  TIME NODES CPUS\n",
      "2021-12-18T09:46:33  10351723  nvidia_dev   ng2   R  0:02     1   24\n"
     ]
    }
   ],
   "source": [
    "! squeue --name ng2 --format \"%.19S  %.8i  %.10P %.5j  %.2t %.5M %.5D %.4C\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         START_TIME     JOBID   PARTITION  NAME  ST  TIME NODES CPUS\n"
     ]
    }
   ],
   "source": [
    "! squeue --name ng2 --format \"%.19S  %.8i  %.10P %.5j  %.2t %.5M %.5D %.4C\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================================\n",
      "- Job ID: 10351723\n",
      "- Tasks per node: 1\n",
      "- # of nodes in the job: 1\n",
      "- # of tasks: 1\n",
      "- Dir from which sbatch was invoked: b715\n",
      "- List of nodes allocated to the job: sdumont3054\n",
      "-- modules ----------------------------\n",
      "conda activate env2, --stack env3\n",
      "-- run --------------------------------\n",
      "$ srun -n 1 python ng2.py\n",
      "-- output -----------------------------\n",
      "1. hostname    rank crank  cid\n",
      "2. ----------- ---- ----- ----\n",
      "3. sdumont3054   00    00   00\n",
      "4. ---------------------------\n",
      "5. Heat:1500.0000, TT:105.5256, TK:2.3658, TC:103.0791, TE:0.0439, MPI:1\n",
      "~~ end ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n"
     ]
    }
   ],
   "source": [
    "! cat /scratch${PWD#/prj}/slurm-10351723.out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submitted batch job 10351740\n"
     ]
    }
   ],
   "source": [
    "! sbatch  --ntasks-per-node=1  --nodes=1  --ntasks=1  ng2.srm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         START_TIME     JOBID   PARTITION  NAME  ST  TIME NODES CPUS\n",
      "2021-12-18T09:50:19  10351740  nvidia_dev   ng2   R  0:02     1   24\n"
     ]
    }
   ],
   "source": [
    "! squeue --name ng2 --format \"%.19S  %.8i  %.10P %.5j  %.2t %.5M %.5D %.4C\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         START_TIME     JOBID   PARTITION  NAME  ST  TIME NODES CPUS\n"
     ]
    }
   ],
   "source": [
    "! squeue --name ng2 --format \"%.19S  %.8i  %.10P %.5j  %.2t %.5M %.5D %.4C\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================================\n",
      "- Job ID: 10351740\n",
      "- Tasks per node: 1\n",
      "- # of nodes in the job: 1\n",
      "- # of tasks: 1\n",
      "- Dir from which sbatch was invoked: b715\n",
      "- List of nodes allocated to the job: sdumont3054\n",
      "-- modules ----------------------------\n",
      "conda activate env2, --stack env3\n",
      "-- run --------------------------------\n",
      "$ srun -n 1 python ng2.py\n",
      "-- output -----------------------------\n",
      "1. hostname    rank crank  cid\n",
      "2. ----------- ---- ----- ----\n",
      "3. sdumont3054   00    00   00\n",
      "4. ---------------------------\n",
      "5. Heat:1500.0000, TT:105.1317, TK:2.2101, TC:102.8419, TE:0.0428, MPI:1\n",
      "~~ end ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n"
     ]
    }
   ],
   "source": [
    "! cat /scratch${PWD#/prj}/slurm-10351740.out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submitted batch job 10351767\n"
     ]
    }
   ],
   "source": [
    "! sbatch  --ntasks-per-node=1  --nodes=1  --ntasks=1  ng2.srm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         START_TIME     JOBID   PARTITION  NAME  ST  TIME NODES CPUS\n",
      "2021-12-18T09:54:32  10351767  nvidia_dev   ng2   R  0:01     1   24\n"
     ]
    }
   ],
   "source": [
    "! squeue --name ng2 --format \"%.19S  %.8i  %.10P %.5j  %.2t %.5M %.5D %.4C\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         START_TIME     JOBID   PARTITION  NAME  ST  TIME NODES CPUS\n"
     ]
    }
   ],
   "source": [
    "! squeue --name ng2 --format \"%.19S  %.8i  %.10P %.5j  %.2t %.5M %.5D %.4C\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================================\n",
      "- Job ID: 10351767\n",
      "- Tasks per node: 1\n",
      "- # of nodes in the job: 1\n",
      "- # of tasks: 1\n",
      "- Dir from which sbatch was invoked: b715\n",
      "- List of nodes allocated to the job: sdumont3054\n",
      "-- modules ----------------------------\n",
      "conda activate env2, --stack env3\n",
      "-- run --------------------------------\n",
      "$ srun -n 1 python ng2.py\n",
      "-- output -----------------------------\n",
      "1. hostname    rank crank  cid\n",
      "2. ----------- ---- ----- ----\n",
      "3. sdumont3054   00    00   00\n",
      "4. ---------------------------\n",
      "5. Heat:1500.0000, TT:103.7909, TK:0.7349, TC:102.9773, TE:0.0415, MPI:1\n",
      "~~ end ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n"
     ]
    }
   ],
   "source": [
    "! cat /scratch${PWD#/prj}/slurm-10351767.out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2x2=4(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submitted batch job 10352423\n"
     ]
    }
   ],
   "source": [
    "! sbatch  ng2.srm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         START_TIME     JOBID   PARTITION  NAME  ST  TIME NODES CPUS\n",
      "2021-12-18T11:52:48  10352423  nvidia_dev   ng2   R  0:01     2   48\n"
     ]
    }
   ],
   "source": [
    "! squeue --name ng2 --format \"%.19S  %.8i  %.10P %.5j  %.2t %.5M %.5D %.4C\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         START_TIME     JOBID   PARTITION  NAME  ST  TIME NODES CPUS\n"
     ]
    }
   ],
   "source": [
    "! squeue --name ng2 --format \"%.19S  %.8i  %.10P %.5j  %.2t %.5M %.5D %.4C\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 211218: a 2a TESLA 40 sumiu......."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================================\n",
      "- Job ID: 10352423\n",
      "- Tasks per node: 2\n",
      "- # of nodes in the job: 2\n",
      "- # of tasks: 4\n",
      "- Dir from which sbatch was invoked: b715\n",
      "- List of nodes allocated to the job: sdumont3170 sdumont3171\n",
      "-- lscpu ------------------------------\n",
      "Architecture:          x86_64\n",
      "CPU op-mode(s):        32-bit, 64-bit\n",
      "Byte Order:            Little Endian\n",
      "CPU(s):                24\n",
      "On-line CPU(s) list:   0-23\n",
      "Thread(s) per core:    1\n",
      "Core(s) per socket:    12\n",
      "Socket(s):             2\n",
      "NUMA node(s):          2\n",
      "Vendor ID:             GenuineIntel\n",
      "CPU family:            6\n",
      "Model:                 62\n",
      "Model name:            Intel(R) Xeon(R) CPU E5-2695 v2 @ 2.40GHz\n",
      "Stepping:              4\n",
      "CPU MHz:               2846.044\n",
      "CPU max MHz:           3200,0000\n",
      "CPU min MHz:           1200,0000\n",
      "BogoMIPS:              4799.74\n",
      "Virtualization:        VT-x\n",
      "L1d cache:             32K\n",
      "L1i cache:             32K\n",
      "L2 cache:              256K\n",
      "L3 cache:              30720K\n",
      "NUMA node0 CPU(s):     0-11\n",
      "NUMA node1 CPU(s):     12-23\n",
      "Flags:                 fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc aperfmperf eagerfpu pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm epb ssbd ibrs ibpb stibp tpr_shadow vnmi flexpriority ept vpid fsgsbase smep erms xsaveopt dtherm ida arat pln pts spec_ctrl intel_stibp flush_l1d\n",
      "-- meminfo -------------------------\n",
      "MemTotal:       65764776 kB\n",
      "-- nvidia-smi ----------------------\n",
      "Sat Dec 18 11:52:48 2021       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 460.73.01    Driver Version: 460.73.01    CUDA Version: 11.2     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Tesla K40t          On   | 00000000:01:00.0 Off |                    0 |\n",
      "| N/A   43C    P8    20W / 235W |      0MiB / 11441MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|  No running processes found                                                 |\n",
      "+-----------------------------------------------------------------------------+\n",
      "-- modules ----------------------------\n",
      "conda activate env2, --stack env3\n",
      "-- run --------------------------------\n",
      "$ srun -n 4 python ng2.py\n",
      "-- output -----------------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/scratch/ampemi/xxxx.xxxx/b715/ng2.py\", line 72, in <module>\n",
      "    cuda.select_device(crank)\n",
      "  File \"/scratch/ampemi/xxxx.xxxx/env3/lib/python3.9/site-packages/numba/cuda/api.py\", line 452, in select_device\n",
      "    context = devices.get_context(device_id)\n",
      "  File \"/scratch/ampemi/xxxx.xxxx/env3/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py\", line 212, in get_context\n",
      "    return _runtime.get_or_create_context(devnum)\n",
      "  File \"/scratch/ampemi/xxxx.xxxx/env3/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py\", line 142, in get_or_create_context\n",
      "    return self._activate_context_for(devnum)\n",
      "  File \"/scratch/ampemi/xxxx.xxxx/env3/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py\", line 168, in _activate_context_for\n",
      "    gpu = self.gpus[devnum]\n",
      "  File \"/scratch/ampemi/xxxx.xxxx/env3/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py\", line 40, in __getitem__\n",
      "    return self.lst[devnum]\n",
      "IndexError: list index out of range\n",
      "srun: error: sdumont3170: task 1: Exited with exit code 1\n",
      "4 0 sdumont3170 3170 0 0\n",
      "4 2 sdumont3171 3171 0 0\n",
      "4 3 sdumont3171 3171 1 0\n",
      "~~ end ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n"
     ]
    }
   ],
   "source": [
    "! cat /scratch${PWD#/prj}/slurm-10352423.out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# fila nvidia_small: também está faltando uma k40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submitted batch job 10352488\n"
     ]
    }
   ],
   "source": [
    "! sbatch  --partition=nvidia_small  --ntasks-per-node=2  --nodes=2  --ntasks=4  ng2.srm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         START_TIME     JOBID   PARTITION  NAME  ST  TIME NODES CPUS\n",
      "2021-12-18T12:03:58  10352488  nvidia_sma   ng2   R  0:04     2   48\n"
     ]
    }
   ],
   "source": [
    "! squeue --name ng2 --format \"%.19S  %.8i  %.12P %.5j  %.2t %.5M %.5D %.4C\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "! squeue -u $(whoami) -h -r | wc -l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "! squeue --partition=nvidia_small -h -r | wc -l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         START_TIME     JOBID   PARTITION  NAME  ST  TIME NODES CPUS\n"
     ]
    }
   ],
   "source": [
    "! squeue --name ng2 --format \"%.19V %.19S  %.8i  %.12P %.5j  %.2t %.5M %.5D %.4C\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================================\n",
      "- Job ID: 10352488\n",
      "- Tasks per node: 2\n",
      "- # of nodes in the job: 2\n",
      "- # of tasks: 4\n",
      "- Dir from which sbatch was invoked: b715\n",
      "- List of nodes allocated to the job: sdumont3170 sdumont3171\n",
      "-- modules ----------------------------\n",
      "conda activate env2, --stack env3\n",
      "-- run --------------------------------\n",
      "$ srun -n 4 python ng2.py\n",
      "-- output -----------------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/scratch/ampemi/xxxx.xxxx/b715/ng2.py\", line 72, in <module>\n",
      "    cuda.select_device(crank)\n",
      "  File \"/scratch/ampemi/xxxx.xxxx/env3/lib/python3.9/site-packages/numba/cuda/api.py\", line 452, in select_device\n",
      "    context = devices.get_context(device_id)\n",
      "  File \"/scratch/ampemi/xxxx.xxxx/env3/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py\", line 212, in get_context\n",
      "    return _runtime.get_or_create_context(devnum)\n",
      "  File \"/scratch/ampemi/xxxx.xxxx/env3/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py\", line 142, in get_or_create_context\n",
      "    return self._activate_context_for(devnum)\n",
      "  File \"/scratch/ampemi/xxxx.xxxx/env3/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py\", line 168, in _activate_context_for\n",
      "    gpu = self.gpus[devnum]\n",
      "  File \"/scratch/ampemi/xxxx.xxxx/env3/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py\", line 40, in __getitem__\n",
      "    return self.lst[devnum]\n",
      "IndexError: list index out of range\n",
      "srun: error: sdumont3170: task 1: Exited with exit code 1\n",
      "4 0 sdumont3170 3170 0 0\n",
      "4 2 sdumont3171 3171 0 0\n",
      "4 3 sdumont3171 3171 1 0\n",
      "~~ end ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n"
     ]
    }
   ],
   "source": [
    "! cat /scratch${PWD#/prj}/slurm-10352488.out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# module load não está mais funcionando no SDdumont"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- cudnn/7.6_cuda-10.0 ñ funciona.\n",
    "- mudar path tb ñ funciona.\n",
    "- fica bloqueado em cuda version 11.2 e uma única GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submitted batch job 10352851\n"
     ]
    }
   ],
   "source": [
    "! sbatch  --partition=nvidia_small  --ntasks-per-node=2  --nodes=2  --ntasks=4  ng2.srm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        SUBMIT_TIME           START_TIME     JOBID     PARTITION  NAME  ST  TIME NODES CPUS\n",
      "2021-12-18T13:14:32  2021-12-18T13:14:37  10352851  nvidia_small   ng2   R  0:06     2   48\n"
     ]
    }
   ],
   "source": [
    "! squeue --name ng2 --format \"%.19V  %.19S  %.8i  %.12P %.5j  %.2t %.5M %.5D %.4C\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "! squeue -u $(whoami) -h -r | wc -l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "! squeue --partition=nvidia_small -h -r | wc -l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        SUBMIT_TIME           START_TIME     JOBID     PARTITION  NAME  ST  TIME NODES CPUS\n"
     ]
    }
   ],
   "source": [
    "! squeue --name ng2 --format \"%.19V  %.19S  %.8i  %.12P %.5j  %.2t %.5M %.5D %.4C\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================================\n",
      "- Job ID: 10352851\n",
      "- Tasks per node: 2\n",
      "- # of nodes in the job: 2\n",
      "- # of tasks: 4\n",
      "- Partition: nvidia_small\n",
      "- Dir from which sbatch was invoked: b715\n",
      "- List of nodes allocated to the job: sdumont3170 sdumont3171\n",
      "-- nvidia-smi ----------------------\n",
      "Sat Dec 18 13:14:39 2021       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 460.73.01    Driver Version: 460.73.01    CUDA Version: 11.2     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Tesla K40t          On   | 00000000:01:00.0 Off |                    0 |\n",
      "| N/A   41C    P8    20W / 235W |      0MiB / 11441MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|  No running processes found                                                 |\n",
      "+-----------------------------------------------------------------------------+\n",
      "-- nvidia-smi ----------------------\n",
      "Sat Dec 18 13:14:39 2021       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 460.73.01    Driver Version: 460.73.01    CUDA Version: 11.2     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Tesla K40t          On   | 00000000:01:00.0 Off |                    0 |\n",
      "| N/A   41C    P8    20W / 235W |      0MiB / 11441MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|  No running processes found                                                 |\n",
      "+-----------------------------------------------------------------------------+\n",
      "-- nvidia-smi ----------------------\n",
      "Sat Dec 18 13:14:39 2021       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 460.73.01    Driver Version: 460.73.01    CUDA Version: 11.2     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Tesla K40t          On   | 00000000:01:00.0 Off |                    0 |\n",
      "| N/A   41C    P8    23W / 235W |      0MiB / 11441MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|  No running processes found                                                 |\n",
      "+-----------------------------------------------------------------------------+\n",
      "-- nvidia-smi ----------------------\n",
      "Sat Dec 18 13:14:39 2021       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 460.73.01    Driver Version: 460.73.01    CUDA Version: 11.2     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Tesla K40t          On   | 00000000:01:00.0 Off |                    0 |\n",
      "| N/A   41C    P8    23W / 235W |      0MiB / 11441MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|  No running processes found                                                 |\n",
      "+-----------------------------------------------------------------------------+\n",
      "-- nvidia-smi ----------------------\n",
      "Sat Dec 18 13:14:39 2021       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 460.73.01    Driver Version: 460.73.01    CUDA Version: 11.2     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Tesla K40t          On   | 00000000:01:00.0 Off |                    0 |\n",
      "| N/A   41C    P8    23W / 235W |      0MiB / 11441MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|  No running processes found                                                 |\n",
      "+-----------------------------------------------------------------------------+\n",
      "-- modules ----------------------------\n",
      "conda activate env2, --stack env3\n",
      "-- run --------------------------------\n",
      "$ srun -n 4 python ng2.py\n",
      "-- output -----------------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/scratch/ampemi/xxxx.xxxx/b715/ng2.py\", line 72, in <module>\n",
      "    cuda.select_device(crank)\n",
      "  File \"/scratch/ampemi/xxxx.xxxx/env3/lib/python3.9/site-packages/numba/cuda/api.py\", line 452, in select_device\n",
      "    context = devices.get_context(device_id)\n",
      "  File \"/scratch/ampemi/xxxx.xxxx/env3/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py\", line 212, in get_context\n",
      "    return _runtime.get_or_create_context(devnum)\n",
      "  File \"/scratch/ampemi/xxxx.xxxx/env3/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py\", line 142, in get_or_create_context\n",
      "    return self._activate_context_for(devnum)\n",
      "  File \"/scratch/ampemi/xxxx.xxxx/env3/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py\", line 168, in _activate_context_for\n",
      "    gpu = self.gpus[devnum]\n",
      "  File \"/scratch/ampemi/xxxx.xxxx/env3/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py\", line 40, in __getitem__\n",
      "    return self.lst[devnum]\n",
      "IndexError: list index out of range\n",
      "srun: error: sdumont3170: task 1: Exited with exit code 1\n",
      "4 0 sdumont3170 3170 0 0\n",
      "4 2 sdumont3171 3171 0 0\n",
      "4 3 sdumont3171 3171 1 0\n",
      "~~ end ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n"
     ]
    }
   ],
   "source": [
    "! cat /scratch${PWD#/prj}/slurm-10352851.out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submitted batch job 10353148\n"
     ]
    }
   ],
   "source": [
    "! sbatch  --partition=nvidia_small  --ntasks-per-node=2  --nodes=2  --ntasks=4  ng2.srm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        SUBMIT_TIME           START_TIME     JOBID     PARTITION  NAME  ST  TIME NODES CPUS\n",
      "2021-12-18T14:23:17  2021-12-19T18:17:17  10353148  nvidia_small   ng2  PD  0:00     2    4\n"
     ]
    }
   ],
   "source": [
    "! squeue --name ng2 --format \"%.19V  %.19S  %.8i  %.12P %.5j  %.2t %.5M %.5D %.4C\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "! squeue -u $(whoami) -h -r | wc -l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "! squeue --partition=nvidia_small -h -r | wc -l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        SUBMIT_TIME           START_TIME     JOBID     PARTITION  NAME  ST  TIME NODES CPUS\n",
      "2021-12-18T14:23:17  2021-12-19T18:17:17  10353148  nvidia_small   ng2  PD  0:00     2    4\n"
     ]
    }
   ],
   "source": [
    "! squeue --name ng2 --format \"%.19V  %.19S  %.8i  %.12P %.5j  %.2t %.5M %.5D %.4C\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! cat /scratch${PWD#/prj}/slurm-10353148.out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 211218: agora funcionou pq? voltou a funcionar de repente, sem explicação"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submitted batch job 10353218\n"
     ]
    }
   ],
   "source": [
    "! sbatch  --ntasks-per-node=2  --nodes=2  --ntasks=4  ng2.srm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        SUBMIT_TIME           START_TIME     JOBID     PARTITION  NAME  ST  TIME NODES CPUS\n",
      "2021-12-18T14:23:17  2021-12-19T18:17:17  10353148  nvidia_small   ng2  PD  0:00     2    4\n",
      "2021-12-18T14:39:00  2021-12-18T14:39:00  10353218    nvidia_dev   ng2   R  0:03     2   48\n"
     ]
    }
   ],
   "source": [
    "! squeue --name ng2 --format \"%.19V  %.19S  %.8i  %.12P %.5j  %.2t %.5M %.5D %.4C\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "! squeue -u $(whoami) -h -r | wc -l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "! squeue --partition=nvidia_dev -h -r | wc -l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        SUBMIT_TIME           START_TIME     JOBID     PARTITION  NAME  ST  TIME NODES CPUS\n",
      "2021-12-18T14:23:17  2021-12-19T18:17:17  10353148  nvidia_small   ng2  PD  0:00     2    4\n"
     ]
    }
   ],
   "source": [
    "! squeue --name ng2 --format \"%.19V  %.19S  %.8i  %.12P %.5j  %.2t %.5M %.5D %.4C\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================================\n",
      "- Job ID: 10353218\n",
      "- Tasks per node: 2\n",
      "- # of nodes in the job: 2\n",
      "- # of tasks: 4\n",
      "- Partition: nvidia_dev\n",
      "- Dir from which sbatch was invoked: b715\n",
      "- List of nodes allocated to the job: sdumont3075 sdumont3076\n",
      "-- nvidia-smi ----------------------\n",
      "sdumont3075\n",
      "01:00.0 3D controller: NVIDIA Corporation GK110BGL [Tesla K40t] (rev a1)\n",
      "\tSubsystem: NVIDIA Corporation GK110BGL [Tesla K40t]\n",
      "81:00.0 3D controller: NVIDIA Corporation GK110BGL [Tesla K40t] (rev a1)\n",
      "\tSubsystem: NVIDIA Corporation GK110BGL [Tesla K40t]\n",
      "-- nvidia-smi ----------------------\n",
      "Sat Dec 18 14:39:01 2021       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 460.73.01    Driver Version: 460.73.01    CUDA Version: 11.2     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Tesla K40t          On   | 00000000:01:00.0 Off |                    0 |\n",
      "| N/A   41C    P8    21W / 235W |      0MiB / 11441MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   1  Tesla K40t          On   | 00000000:81:00.0 Off |                    0 |\n",
      "| N/A   40C    P8    20W / 235W |      0MiB / 11441MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|  No running processes found                                                 |\n",
      "+-----------------------------------------------------------------------------+\n",
      "-- modules ----------------------------\n",
      "conda activate env2, --stack env3\n",
      "-- run --------------------------------\n",
      "$ srun -n 4 python ng2.py\n",
      "-- output -----------------------------\n",
      "4 0 sdumont3075 3075 0 0\n",
      "4 1 sdumont3075 3075 1 0\n",
      "4 2 sdumont3076 3076 0 0\n",
      "4 3 sdumont3076 3076 1 0\n",
      "~~ end ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n"
     ]
    }
   ],
   "source": [
    "! cat /scratch${PWD#/prj}/slurm-10353218.out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submitted batch job 10355992\n"
     ]
    }
   ],
   "source": [
    "! sbatch  --partition=nvidia_dev  --ntasks-per-node=2  --nodes=2  --ntasks=4  ng3.srm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        SUBMIT_TIME           START_TIME     JOBID     PARTITION  NAME  ST  TIME NODES CPUS\n",
      "2021-12-19T08:55:43  2021-12-19T08:55:43  10355992    nvidia_dev   ng3   R  0:02     2   48\n"
     ]
    }
   ],
   "source": [
    "! squeue --name=ng3 --format=\"%.19V  %.19S  %.8i  %.12P %.5j  %.2t %.5M %.5D %.4C\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    }
   ],
   "source": [
    "! squeue -u $(whoami) -h -r | wc -l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "! squeue --partition=nvidia_dev -h -r | wc -l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================================\n",
      "- Job ID: 10355992\n",
      "- Tasks per node: 2\n",
      "- # of nodes in the job: 2\n",
      "- # of tasks: 4\n",
      "- Partition: nvidia_dev\n",
      "- Dir from which sbatch was invoked: b715\n",
      "- List of nodes allocated to the job: sdumont3052 sdumont3053\n",
      "-- modules ----------------------------\n",
      "conda activate env2, --stack env3\n",
      "sdumont3052\n",
      "Sun Dec 19 08:55:52 2021       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 460.73.01    Driver Version: 460.73.01    CUDA Version: 11.2     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Tesla K40t          On   | 00000000:01:00.0 Off |                    0 |\n",
      "| N/A   42C    P8    21W / 235W |      0MiB / 11441MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   1  Tesla K40t          On   | 00000000:81:00.0 Off |                 ERR! |\n",
      "| N/A   41C    P8    20W / 235W |      0MiB / 11441MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|  No running processes found                                                 |\n",
      "+-----------------------------------------------------------------------------+\n",
      "WARNING: infoROM is corrupted at gpu 0000:81:00.0\n",
      "-- run --------------------------------\n",
      "$ srun -n 4 python ng2.py\n",
      "-- output -----------------------------\n",
      "~~ end ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n"
     ]
    }
   ],
   "source": [
    "! cat /scratch${PWD#/prj}/slurm-10355992.out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retomando as medições"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2x2=4(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submitted batch job 10353622\n"
     ]
    }
   ],
   "source": [
    "! sbatch  --partition=nvidia_dev  --ntasks-per-node=2  --nodes=2  --ntasks=4  ng2.srm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        SUBMIT_TIME           START_TIME     JOBID     PARTITION  NAME  ST  TIME NODES CPUS\n",
      "2021-12-18T16:02:43  2021-12-18T16:02:45  10353622    nvidia_dev   ng2   R  0:01     2   48\n"
     ]
    }
   ],
   "source": [
    "! squeue --name ng2 --format \"%.19V  %.19S  %.8i  %.12P %.5j  %.2t %.5M %.5D %.4C\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "! squeue -u $(whoami) -h -r | wc -l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "! squeue --partition=nvidia_dev -h -r | wc -l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        SUBMIT_TIME           START_TIME     JOBID     PARTITION  NAME  ST  TIME NODES CPUS\n"
     ]
    }
   ],
   "source": [
    "! squeue --name ng2 --format \"%.19V  %.19S  %.8i  %.12P %.5j  %.2t %.5M %.5D %.4C\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================================\n",
      "- Job ID: 10353622\n",
      "- Tasks per node: 2\n",
      "- # of nodes in the job: 2\n",
      "- # of tasks: 4\n",
      "- Partition: nvidia_dev\n",
      "- Dir from which sbatch was invoked: b715\n",
      "- List of nodes allocated to the job: sdumont3075 sdumont3076\n",
      "-- modules ----------------------------\n",
      "conda activate env2, --stack env3\n",
      "-- run --------------------------------\n",
      "$ srun -n 4 python ng2.py\n",
      "-- output -----------------------------\n",
      "1. hostname    rank crank  cid\n",
      "2. ----------- ---- ----- ----\n",
      "3. sdumont3075   00    00   00\n",
      "3. sdumont3075   01    01   01\n",
      "3. sdumont3076   02    00   00\n",
      "3. sdumont3076   03    01   01\n",
      "4. ---------------------------\n",
      "5. Heat:1500.0000, TT:28.2087, TK:2.3022, TC:25.5090, TE:0.0395, MPI:4\n",
      "~~ end ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n"
     ]
    }
   ],
   "source": [
    "! cat /scratch${PWD#/prj}/slurm-10353622.out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submitted batch job 10353628\n"
     ]
    }
   ],
   "source": [
    "! sbatch  --partition=nvidia_dev  --ntasks-per-node=2  --nodes=2  --ntasks=4  ng2.srm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        SUBMIT_TIME           START_TIME     JOBID     PARTITION  NAME  ST  TIME NODES CPUS\n",
      "2021-12-18T16:04:05  2021-12-18T16:04:08  10353628    nvidia_dev   ng2   R  0:11     2   48\n"
     ]
    }
   ],
   "source": [
    "! squeue --name ng2 --format \"%.19V  %.19S  %.8i  %.12P %.5j  %.2t %.5M %.5D %.4C\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        SUBMIT_TIME           START_TIME     JOBID     PARTITION  NAME  ST  TIME NODES CPUS\n"
     ]
    }
   ],
   "source": [
    "! squeue --name ng2 --format \"%.19V  %.19S  %.8i  %.12P %.5j  %.2t %.5M %.5D %.4C\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================================\n",
      "- Job ID: 10353628\n",
      "- Tasks per node: 2\n",
      "- # of nodes in the job: 2\n",
      "- # of tasks: 4\n",
      "- Partition: nvidia_dev\n",
      "- Dir from which sbatch was invoked: b715\n",
      "- List of nodes allocated to the job: sdumont3075 sdumont3076\n",
      "-- modules ----------------------------\n",
      "conda activate env2, --stack env3\n",
      "-- run --------------------------------\n",
      "$ srun -n 4 python ng2.py\n",
      "-- output -----------------------------\n",
      "1. hostname    rank crank  cid\n",
      "2. ----------- ---- ----- ----\n",
      "3. sdumont3075   00    00   00\n",
      "3. sdumont3075   01    01   01\n",
      "3. sdumont3076   02    00   00\n",
      "3. sdumont3076   03    01   01\n",
      "4. ---------------------------\n",
      "5. Heat:1500.0000, TT:26.5606, TK:0.6478, TC:25.5067, TE:0.0403, MPI:4\n",
      "~~ end ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n"
     ]
    }
   ],
   "source": [
    "! cat /scratch${PWD#/prj}/slurm-10353628.out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submitted batch job 10353638\n"
     ]
    }
   ],
   "source": [
    "! sbatch  --partition=nvidia_dev  --ntasks-per-node=2  --nodes=2  --ntasks=4  ng2.srm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        SUBMIT_TIME           START_TIME     JOBID     PARTITION  NAME  ST  TIME NODES CPUS\n",
      "2021-12-18T16:06:23  2021-12-18T16:06:23  10353638    nvidia_dev   ng2   R  0:02     2   48\n"
     ]
    }
   ],
   "source": [
    "! squeue --name ng2 --format \"%.19V  %.19S  %.8i  %.12P %.5j  %.2t %.5M %.5D %.4C\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        SUBMIT_TIME           START_TIME     JOBID     PARTITION  NAME  ST  TIME NODES CPUS\n"
     ]
    }
   ],
   "source": [
    "! squeue --job 10353638 --name ng2 --format \"%.19V  %.19S  %.8i  %.12P %.5j  %.2t %.5M %.5D %.4C\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================================\n",
      "- Job ID: 10353638\n",
      "- Tasks per node: 2\n",
      "- # of nodes in the job: 2\n",
      "- # of tasks: 4\n",
      "- Partition: nvidia_dev\n",
      "- Dir from which sbatch was invoked: b715\n",
      "- List of nodes allocated to the job: sdumont3075 sdumont3076\n",
      "-- modules ----------------------------\n",
      "conda activate env2, --stack env3\n",
      "-- run --------------------------------\n",
      "$ srun -n 4 python ng2.py\n",
      "-- output -----------------------------\n",
      "1. hostname    rank crank  cid\n",
      "2. ----------- ---- ----- ----\n",
      "3. sdumont3075   00    00   00\n",
      "3. sdumont3075   01    01   01\n",
      "3. sdumont3076   02    00   00\n",
      "3. sdumont3076   03    01   01\n",
      "4. ---------------------------\n",
      "5. Heat:1500.0000, TT:26.9706, TK:1.0652, TC:25.5190, TE:0.0393, MPI:4\n",
      "~~ end ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n"
     ]
    }
   ],
   "source": [
    "! cat /scratch${PWD#/prj}/slurm-10353638.out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3x3=9(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submitted batch job 10353639\n"
     ]
    }
   ],
   "source": [
    "! sbatch  --partition nvidia_small  --ntasks-per-node=2  --nodes=5  --ntasks=9  ng2.srm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================================\n",
      "- Job ID: 10353639\n",
      "- Tasks per node: 2\n",
      "- # of nodes in the job: 5\n",
      "- # of tasks: 9\n",
      "- Partition: nvidia_small\n",
      "- Dir from which sbatch was invoked: b715\n",
      "- List of nodes allocated to the job: sdumont3080 sdumont3137 sdumont3138 sdumont3139 sdumont3140\n",
      "-- modules ----------------------------\n",
      "conda activate env2, --stack env3\n",
      "-- run --------------------------------\n",
      "$ srun -n 9 python ng2.py\n",
      "-- output -----------------------------\n",
      "srun: Warning: can't honor --ntasks-per-node set to 2 which doesn't match the requested tasks 9 with the number of requested nodes 5. Ignoring --ntasks-per-node.\n",
      "1. hostname    rank crank  cid\n",
      "2. ----------- ---- ----- ----\n",
      "3. sdumont3080   00    00   00\n",
      "3. sdumont3080   01    01   01\n",
      "3. sdumont3137   02    00   00\n",
      "3. sdumont3137   03    01   01\n",
      "3. sdumont3138   04    00   00\n",
      "3. sdumont3138   05    01   01\n",
      "3. sdumont3139   06    00   00\n",
      "3. sdumont3139   07    01   01\n",
      "3. sdumont3140   08    00   00\n",
      "4. ---------------------------\n",
      "5. Heat:1500.0000, TT:12.5650, TK:1.8618, TC:9.7379, TE:0.0168, MPI:9\n",
      "~~ end ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n"
     ]
    }
   ],
   "source": [
    "! cat /scratch${PWD#/prj}/slurm-10353639.out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submitted batch job 10353640\n"
     ]
    }
   ],
   "source": [
    "! sbatch  --partition nvidia_small  --ntasks-per-node=2  --nodes=5  --ntasks=9  ng2.srm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================================\n",
      "- Job ID: 10353640\n",
      "- Tasks per node: 2\n",
      "- # of nodes in the job: 5\n",
      "- # of tasks: 9\n",
      "- Partition: nvidia_small\n",
      "- Dir from which sbatch was invoked: b715\n",
      "- List of nodes allocated to the job: sdumont3080 sdumont3137 sdumont3138 sdumont3139 sdumont3140\n",
      "-- modules ----------------------------\n",
      "conda activate env2, --stack env3\n",
      "-- run --------------------------------\n",
      "$ srun -n 9 python ng2.py\n",
      "-- output -----------------------------\n",
      "srun: Warning: can't honor --ntasks-per-node set to 2 which doesn't match the requested tasks 9 with the number of requested nodes 5. Ignoring --ntasks-per-node.\n",
      "1. hostname    rank crank  cid\n",
      "2. ----------- ---- ----- ----\n",
      "3. sdumont3080   00    00   00\n",
      "3. sdumont3080   01    01   01\n",
      "3. sdumont3137   02    00   00\n",
      "3. sdumont3137   03    01   01\n",
      "3. sdumont3138   04    00   00\n",
      "3. sdumont3138   05    01   01\n",
      "3. sdumont3139   06    00   00\n",
      "3. sdumont3139   07    01   01\n",
      "3. sdumont3140   08    00   00\n",
      "4. ---------------------------\n",
      "5. Heat:1500.0000, TT:11.2820, TK:0.5888, TC:9.7094, TE:0.0169, MPI:9\n",
      "~~ end ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n"
     ]
    }
   ],
   "source": [
    "! cat /scratch${PWD#/prj}/slurm-10353640.out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submitted batch job 10353641\n"
     ]
    }
   ],
   "source": [
    "! sbatch  --partition nvidia_small  --ntasks-per-node=2  --nodes=5  --ntasks=9  ng2.srm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================================\n",
      "- Job ID: 10353641\n",
      "- Tasks per node: 2\n",
      "- # of nodes in the job: 5\n",
      "- # of tasks: 9\n",
      "- Partition: nvidia_small\n",
      "- Dir from which sbatch was invoked: b715\n",
      "- List of nodes allocated to the job: sdumont3080 sdumont3137 sdumont3138 sdumont3139 sdumont3140\n",
      "-- modules ----------------------------\n",
      "conda activate env2, --stack env3\n",
      "-- run --------------------------------\n",
      "$ srun -n 9 python ng2.py\n",
      "-- output -----------------------------\n",
      "srun: Warning: can't honor --ntasks-per-node set to 2 which doesn't match the requested tasks 9 with the number of requested nodes 5. Ignoring --ntasks-per-node.\n",
      "1. hostname    rank crank  cid\n",
      "2. ----------- ---- ----- ----\n",
      "3. sdumont3080   00    00   00\n",
      "3. sdumont3080   01    01   01\n",
      "3. sdumont3137   02    00   00\n",
      "3. sdumont3137   03    01   01\n",
      "3. sdumont3138   04    00   00\n",
      "3. sdumont3138   05    01   01\n",
      "3. sdumont3139   06    00   00\n",
      "3. sdumont3139   07    01   01\n",
      "3. sdumont3140   08    00   00\n",
      "4. ---------------------------\n",
      "5. Heat:1500.0000, TT:11.2837, TK:0.5719, TC:9.7297, TE:0.0169, MPI:9\n",
      "~~ end ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n"
     ]
    }
   ],
   "source": [
    "! cat /scratch${PWD#/prj}/slurm-10353641.out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4x4=16(8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submitted batch job 10353642\n"
     ]
    }
   ],
   "source": [
    "! sbatch  --partition nvidia_small  --ntasks-per-node=2  --nodes=8  --ntasks=16  ng2.srm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================================\n",
      "- Job ID: 10353642\n",
      "- Tasks per node: 2\n",
      "- # of nodes in the job: 8\n",
      "- # of tasks: 16\n",
      "- Partition: nvidia_small\n",
      "- Dir from which sbatch was invoked: b715\n",
      "- List of nodes allocated to the job: sdumont3080 sdumont3081 sdumont3137 sdumont3138 sdumont3139 sdumont3140 sdumont3170 sdumont3171\n",
      "-- modules ----------------------------\n",
      "conda activate env2, --stack env3\n",
      "-- run --------------------------------\n",
      "$ srun -n 16 python ng2.py\n",
      "-- output -----------------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/scratch/ampemi/xxxx.xxxx/b715/ng2.py\", line 68, in <module>\n",
      "    cuda.select_device(crank)         # 'color rank' 0 = 'gpu id' 0, etc.\n",
      "  File \"/scratch/ampemi/xxxx.xxxx/env3/lib/python3.9/site-packages/numba/cuda/api.py\", line 452, in select_device\n",
      "    context = devices.get_context(device_id)\n",
      "  File \"/scratch/ampemi/xxxx.xxxx/env3/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py\", line 212, in get_context\n",
      "    return _runtime.get_or_create_context(devnum)\n",
      "  File \"/scratch/ampemi/xxxx.xxxx/env3/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py\", line 142, in get_or_create_context\n",
      "    return self._activate_context_for(devnum)\n",
      "  File \"/scratch/ampemi/xxxx.xxxx/env3/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py\", line 168, in _activate_context_for\n",
      "    gpu = self.gpus[devnum]\n",
      "  File \"/scratch/ampemi/xxxx.xxxx/env3/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py\", line 40, in __getitem__\n",
      "    return self.lst[devnum]\n",
      "IndexError: list index out of range\n",
      "Traceback (most recent call last):\n",
      "  File \"/scratch/ampemi/xxxx.xxxx/b715/ng2.py\", line 160, in <module>\n",
      "    aold[1:bx+1, 0] = r4.wait()\n",
      "  File \"mpi4py/MPI/Request.pyx\", line 235, in mpi4py.MPI.Request.wait\n",
      "  File \"mpi4py/MPI/msgpickle.pxi\", line 411, in mpi4py.MPI.PyMPI_wait\n",
      "mpi4py.MPI.Exception: Unknown error class, error stack:\n",
      "PMPI_Wait(216)........................: MPI_Wait(request=0x2b3db85a11d0, status=0x7ffcb4ded5b0) failed\n",
      "MPIR_Wait(112)........................: \n",
      "MPIDU_Complete_posted_with_error(1137): Process failed\n",
      "srun: Job step aborted: Waiting up to 302 seconds for job step to finish.\n",
      "slurmstepd: error: *** JOB 10353642 ON sdumont3080 CANCELLED AT 2021-12-19T04:25:24 DUE TO TIME LIMIT ***\n",
      "slurmstepd: error: *** STEP 10353642.0 ON sdumont3080 CANCELLED AT 2021-12-19T04:25:24 DUE TO TIME LIMIT ***\n",
      "srun: got SIGCONT\n",
      "srun: forcing job termination\n",
      "srun: error: sdumont3170: tasks 12-13: Terminated\n",
      "srun: error: sdumont3171: tasks 14-15: Terminated\n",
      "srun: error: sdumont3137: tasks 4-5: Terminated\n",
      "srun: error: sdumont3138: tasks 6-7: Terminated\n",
      "srun: error: sdumont3081: tasks 2-3: Terminated\n",
      "srun: error: sdumont3139: tasks 8-9: Terminated\n",
      "srun: error: sdumont3080: tasks 0-1: Terminated\n",
      "srun: error: sdumont3140: tasks 10-11: Terminated\n"
     ]
    }
   ],
   "source": [
    "! cat /scratch${PWD#/prj}/slurm-10353642.out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submitted batch job 10353643\n"
     ]
    }
   ],
   "source": [
    "! sbatch  --partition nvidia_small  --ntasks-per-node=2  --nodes=8  --ntasks=16  ng2.srm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================================\n",
      "- Job ID: 10353643\n",
      "- Tasks per node: 2\n",
      "- # of nodes in the job: 8\n",
      "- # of tasks: 16\n",
      "- Partition: nvidia_small\n",
      "- Dir from which sbatch was invoked: b715\n",
      "- List of nodes allocated to the job: sdumont3080 sdumont3081 sdumont3137 sdumont3138 sdumont3139 sdumont3140 sdumont3170 sdumont3171\n",
      "-- modules ----------------------------\n",
      "conda activate env2, --stack env3\n",
      "-- run --------------------------------\n",
      "$ srun -n 16 python ng2.py\n",
      "-- output -----------------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/scratch/ampemi/xxxx.xxxx/b715/ng2.py\", line 68, in <module>\n",
      "    cuda.select_device(crank)         # 'color rank' 0 = 'gpu id' 0, etc.\n",
      "  File \"/scratch/ampemi/xxxx.xxxx/env3/lib/python3.9/site-packages/numba/cuda/api.py\", line 452, in select_device\n",
      "    context = devices.get_context(device_id)\n",
      "  File \"/scratch/ampemi/xxxx.xxxx/env3/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py\", line 212, in get_context\n",
      "    return _runtime.get_or_create_context(devnum)\n",
      "  File \"/scratch/ampemi/xxxx.xxxx/env3/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py\", line 142, in get_or_create_context\n",
      "    return self._activate_context_for(devnum)\n",
      "  File \"/scratch/ampemi/xxxx.xxxx/env3/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py\", line 168, in _activate_context_for\n",
      "    gpu = self.gpus[devnum]\n",
      "  File \"/scratch/ampemi/xxxx.xxxx/env3/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py\", line 40, in __getitem__\n",
      "    return self.lst[devnum]\n",
      "IndexError: list index out of range\n",
      "Traceback (most recent call last):\n",
      "  File \"/scratch/ampemi/xxxx.xxxx/b715/ng2.py\", line 160, in <module>\n",
      "    aold[1:bx+1, 0] = r4.wait()\n",
      "  File \"mpi4py/MPI/Request.pyx\", line 235, in mpi4py.MPI.Request.wait\n",
      "  File \"mpi4py/MPI/msgpickle.pxi\", line 411, in mpi4py.MPI.PyMPI_wait\n",
      "mpi4py.MPI.Exception: Unknown error class, error stack:\n",
      "PMPI_Wait(216)........................: MPI_Wait(request=0x2ab81c87a1d0, status=0x7fff375fb0c0) failed\n",
      "MPIR_Wait(112)........................: \n",
      "MPIDU_Complete_posted_with_error(1137): Process failed\n",
      "srun: Job step aborted: Waiting up to 302 seconds for job step to finish.\n",
      "slurmstepd: error: *** JOB 10353643 ON sdumont3080 CANCELLED AT 2021-12-19T04:26:54 DUE TO TIME LIMIT ***\n",
      "slurmstepd: error: *** STEP 10353643.0 ON sdumont3080 CANCELLED AT 2021-12-19T04:26:54 DUE TO TIME LIMIT ***\n",
      "srun: got SIGCONT\n",
      "srun: forcing job termination\n",
      "srun: error: sdumont3170: tasks 12-13: Terminated\n",
      "srun: error: sdumont3080: tasks 0-1: Terminated\n",
      "srun: error: sdumont3137: tasks 4-5: Terminated\n",
      "srun: error: sdumont3171: tasks 14-15: Terminated\n",
      "srun: error: sdumont3138: tasks 6-7: Terminated\n",
      "srun: error: sdumont3081: tasks 2-3: Terminated\n",
      "srun: error: sdumont3139: tasks 8-9: Terminated\n",
      "srun: error: sdumont3140: tasks 10-11: Terminated\n"
     ]
    }
   ],
   "source": [
    "! cat /scratch${PWD#/prj}/slurm-10353643.out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! sbatch  --partition nvidia_small  --ntasks-per-node=2  --nodes=8  --ntasks=16  ng2.srm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================================\n",
      "- Job ID: 10353644\n",
      "- Tasks per node: 2\n",
      "- # of nodes in the job: 8\n",
      "- # of tasks: 16\n",
      "- Partition: nvidia_small\n",
      "- Dir from which sbatch was invoked: b715\n",
      "- List of nodes allocated to the job: sdumont3080 sdumont3081 sdumont3137 sdumont3138 sdumont3139 sdumont3140 sdumont3170 sdumont3171\n",
      "-- modules ----------------------------\n",
      "conda activate env2, --stack env3\n",
      "-- run --------------------------------\n",
      "$ srun -n 16 python ng2.py\n",
      "-- output -----------------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/scratch/ampemi/xxxx.xxxx/b715/ng2.py\", line 68, in <module>\n",
      "    cuda.select_device(crank)         # 'color rank' 0 = 'gpu id' 0, etc.\n",
      "  File \"/scratch/ampemi/xxxx.xxxx/env3/lib/python3.9/site-packages/numba/cuda/api.py\", line 452, in select_device\n",
      "    context = devices.get_context(device_id)\n",
      "  File \"/scratch/ampemi/xxxx.xxxx/env3/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py\", line 212, in get_context\n",
      "    return _runtime.get_or_create_context(devnum)\n",
      "  File \"/scratch/ampemi/xxxx.xxxx/env3/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py\", line 142, in get_or_create_context\n",
      "    return self._activate_context_for(devnum)\n",
      "  File \"/scratch/ampemi/xxxx.xxxx/env3/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py\", line 168, in _activate_context_for\n",
      "    gpu = self.gpus[devnum]\n",
      "  File \"/scratch/ampemi/xxxx.xxxx/env3/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py\", line 40, in __getitem__\n",
      "    return self.lst[devnum]\n",
      "IndexError: list index out of range\n",
      "Traceback (most recent call last):\n",
      "  File \"/scratch/ampemi/xxxx.xxxx/b715/ng2.py\", line 160, in <module>\n",
      "    aold[1:bx+1, 0] = r4.wait()\n",
      "  File \"mpi4py/MPI/Request.pyx\", line 235, in mpi4py.MPI.Request.wait\n",
      "  File \"mpi4py/MPI/msgpickle.pxi\", line 411, in mpi4py.MPI.PyMPI_wait\n",
      "mpi4py.MPI.Exception: Unknown error class, error stack:\n",
      "PMPI_Wait(216)........................: MPI_Wait(request=0x2b61e5a121d0, status=0x7ffc5166a220) failed\n",
      "MPIR_Wait(112)........................: \n",
      "MPIDU_Complete_posted_with_error(1137): Process failed\n",
      "srun: Job step aborted: Waiting up to 302 seconds for job step to finish.\n",
      "slurmstepd: error: *** STEP 10353644.0 ON sdumont3080 CANCELLED AT 2021-12-19T04:28:24 DUE TO TIME LIMIT ***\n",
      "slurmstepd: error: *** JOB 10353644 ON sdumont3080 CANCELLED AT 2021-12-19T04:28:24 DUE TO TIME LIMIT ***\n",
      "srun: got SIGCONT\n",
      "srun: forcing job termination\n",
      "srun: error: sdumont3170: tasks 12-13: Terminated\n",
      "srun: error: sdumont3080: tasks 0-1: Terminated\n",
      "srun: error: sdumont3137: tasks 4-5: Terminated\n",
      "srun: error: sdumont3171: tasks 14-15: Terminated\n",
      "srun: error: sdumont3139: tasks 8-9: Terminated\n",
      "srun: error: sdumont3081: tasks 2-3: Terminated\n",
      "srun: error: sdumont3138: tasks 6-7: Terminated\n",
      "srun: error: sdumont3140: tasks 10-11: Terminated\n"
     ]
    }
   ],
   "source": [
    "! cat /scratch${PWD#/prj}/slurm-10353644.out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SDumont está com erros intermitentes, se roda de novo funciona:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submitted batch job 10356006\n",
      "Submitted batch job 10356007\n",
      "Submitted batch job 10356008\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "sbatch  --partition nvidia_small  --ntasks-per-node=2  --nodes=8  --ntasks=16  ng2.srm\n",
    "sbatch  --partition nvidia_small  --ntasks-per-node=2  --nodes=8  --ntasks=16  ng2.srm\n",
    "sbatch  --partition nvidia_small  --ntasks-per-node=2  --nodes=8  --ntasks=16  ng2.srm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================================\n",
      "- Job ID: 10356006\n",
      "- Tasks per node: 2\n",
      "- # of nodes in the job: 8\n",
      "- # of tasks: 16\n",
      "- Partition: nvidia_small\n",
      "- Dir from which sbatch was invoked: b715\n",
      "- List of nodes allocated to the job: sdumont3047 sdumont3048 sdumont3049 sdumont3052 sdumont3137 sdumont3138 sdumont3139 sdumont3140\n",
      "-- modules ----------------------------\n",
      "conda activate env2, --stack env3\n",
      "-- run --------------------------------\n",
      "$ srun -n 16 python ng2.py\n",
      "-- output -----------------------------\n",
      "1. hostname    rank crank  cid\n",
      "2. ----------- ---- ----- ----\n",
      "3. sdumont3047   00    00   00\n",
      "3. sdumont3047   01    01   01\n",
      "3. sdumont3048   02    00   00\n",
      "3. sdumont3048   03    01   01\n",
      "3. sdumont3049   04    00   00\n",
      "3. sdumont3049   05    01   01\n",
      "3. sdumont3052   06    00   00\n",
      "3. sdumont3052   07    01   01\n",
      "3. sdumont3137   08    00   00\n",
      "3. sdumont3137   09    01   01\n",
      "3. sdumont3138   10    00   00\n",
      "3. sdumont3138   11    01   01\n",
      "3. sdumont3139   12    00   00\n",
      "3. sdumont3139   13    01   01\n",
      "3. sdumont3140   14    00   00\n",
      "3. sdumont3140   15    01   01\n",
      "4. ---------------------------\n",
      "5. Heat:1500.0000, TT:7.0936, TK:0.9806, TC:5.4181, TE:0.0010, MPI:16\n",
      "~~ end ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n"
     ]
    }
   ],
   "source": [
    "! cat /scratch${PWD#/prj}/slurm-10356006.out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================================\n",
      "- Job ID: 10356007\n",
      "- Tasks per node: 2\n",
      "- # of nodes in the job: 8\n",
      "- # of tasks: 16\n",
      "- Partition: nvidia_small\n",
      "- Dir from which sbatch was invoked: b715\n",
      "- List of nodes allocated to the job: sdumont3053 sdumont3156 sdumont3157 sdumont3168 sdumont3169 sdumont3172 sdumont3173 sdumont3174\n",
      "-- modules ----------------------------\n",
      "conda activate env2, --stack env3\n",
      "-- run --------------------------------\n",
      "$ srun -n 16 python ng2.py\n",
      "-- output -----------------------------\n",
      "1. hostname    rank crank  cid\n",
      "2. ----------- ---- ----- ----\n",
      "3. sdumont3053   00    00   00\n",
      "3. sdumont3053   01    01   01\n",
      "3. sdumont3156   02    00   00\n",
      "3. sdumont3156   03    01   01\n",
      "3. sdumont3157   04    00   00\n",
      "3. sdumont3157   05    01   01\n",
      "3. sdumont3168   06    00   00\n",
      "3. sdumont3168   07    01   01\n",
      "3. sdumont3169   08    00   00\n",
      "3. sdumont3169   09    01   01\n",
      "3. sdumont3172   10    00   00\n",
      "3. sdumont3172   11    01   01\n",
      "3. sdumont3173   12    00   00\n",
      "3. sdumont3173   13    01   01\n",
      "3. sdumont3174   14    00   00\n",
      "3. sdumont3174   15    01   01\n",
      "4. ---------------------------\n",
      "5. Heat:1500.0000, TT:10.0616, TK:0.9634, TC:5.5983, TE:0.0011, MPI:16\n",
      "~~ end ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n"
     ]
    }
   ],
   "source": [
    "! cat /scratch${PWD#/prj}/slurm-10356007.out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================================\n",
      "- Job ID: 10356008\n",
      "- Tasks per node: 2\n",
      "- # of nodes in the job: 8\n",
      "- # of tasks: 16\n",
      "- Partition: nvidia_small\n",
      "- Dir from which sbatch was invoked: b715\n",
      "- List of nodes allocated to the job: sdumont3047 sdumont3048 sdumont3049 sdumont3052 sdumont3137 sdumont3138 sdumont3139 sdumont3140\n",
      "-- modules ----------------------------\n",
      "conda activate env2, --stack env3\n",
      "-- run --------------------------------\n",
      "$ srun -n 16 python ng2.py\n",
      "-- output -----------------------------\n",
      "1. hostname    rank crank  cid\n",
      "2. ----------- ---- ----- ----\n",
      "3. sdumont3047   00    00   00\n",
      "3. sdumont3047   01    01   01\n",
      "3. sdumont3048   02    00   00\n",
      "3. sdumont3048   03    01   01\n",
      "3. sdumont3049   04    00   00\n",
      "3. sdumont3049   05    01   01\n",
      "3. sdumont3052   06    00   00\n",
      "3. sdumont3052   07    01   01\n",
      "3. sdumont3137   08    00   00\n",
      "3. sdumont3137   09    01   01\n",
      "3. sdumont3138   10    00   00\n",
      "3. sdumont3138   11    01   01\n",
      "3. sdumont3139   12    00   00\n",
      "3. sdumont3139   13    01   01\n",
      "3. sdumont3140   14    00   00\n",
      "3. sdumont3140   15    01   01\n",
      "4. ---------------------------\n",
      "5. Heat:1500.0000, TT:6.6176, TK:0.5901, TC:5.4329, TE:0.0010, MPI:16\n",
      "~~ end ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n"
     ]
    }
   ],
   "source": [
    "! cat /scratch${PWD#/prj}/slurm-10356008.out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6x6=36(18)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submitted batch job 10353645\n"
     ]
    }
   ],
   "source": [
    "! sbatch  --partition nvidia_small  --ntasks-per-node=2  --nodes=18  --ntasks=36  ng2.srm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! squeue --job 10353645 --name ng2 --partition nvidia_small --format \"%.19V  %.19S  %.8i  %.12P %.5j  %.2t %.5M %.5D %.4C\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================================\n",
      "- Job ID: 10353645\n",
      "- Tasks per node: 2\n",
      "- # of nodes in the job: 18\n",
      "- # of tasks: 36\n",
      "- Partition: nvidia_small\n",
      "- Dir from which sbatch was invoked: b715\n",
      "- List of nodes allocated to the job: sdumont3057 sdumont3058 sdumont3059 sdumont3060 sdumont3061 sdumont3062 sdumont3063 sdumont3148 sdumont3149 sdumont3150 sdumont3151 sdumont3152 sdumont3153 sdumont3154 sdumont3155 sdumont3156 sdumont3157 sdumont3166\n",
      "-- modules ----------------------------\n",
      "conda activate env2, --stack env3\n",
      "-- run --------------------------------\n",
      "$ srun -n 36 python ng2.py\n",
      "-- output -----------------------------\n",
      "1. hostname    rank crank  cid\n",
      "2. ----------- ---- ----- ----\n",
      "3. sdumont3057   00    00   00\n",
      "3. sdumont3057   01    01   01\n",
      "3. sdumont3058   02    00   00\n",
      "3. sdumont3058   03    01   01\n",
      "3. sdumont3059   04    00   00\n",
      "3. sdumont3059   05    01   01\n",
      "3. sdumont3060   06    00   00\n",
      "3. sdumont3060   07    01   01\n",
      "3. sdumont3061   08    00   00\n",
      "3. sdumont3061   09    01   01\n",
      "3. sdumont3062   10    00   00\n",
      "3. sdumont3062   11    01   01\n",
      "3. sdumont3063   12    00   00\n",
      "3. sdumont3063   13    01   01\n",
      "3. sdumont3148   14    00   00\n",
      "3. sdumont3148   15    01   01\n",
      "3. sdumont3149   16    00   00\n",
      "3. sdumont3149   17    01   01\n",
      "3. sdumont3150   18    00   00\n",
      "3. sdumont3150   19    01   01\n",
      "3. sdumont3151   20    00   00\n",
      "3. sdumont3151   21    01   01\n",
      "3. sdumont3152   22    00   00\n",
      "3. sdumont3152   23    01   01\n",
      "3. sdumont3153   24    00   00\n",
      "3. sdumont3153   25    01   01\n",
      "3. sdumont3154   26    00   00\n",
      "3. sdumont3154   27    01   01\n",
      "3. sdumont3155   28    00   00\n",
      "3. sdumont3155   29    01   01\n",
      "3. sdumont3156   30    00   00\n",
      "3. sdumont3156   31    01   01\n",
      "3. sdumont3157   32    00   00\n",
      "3. sdumont3157   33    01   01\n",
      "3. sdumont3166   34    00   00\n",
      "3. sdumont3166   35    01   01\n",
      "4. ---------------------------\n",
      "5. Heat:1500.0000, TT:4.7821, TK:1.6373, TC:2.7138, TE:0.0011, MPI:36\n",
      "~~ end ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n"
     ]
    }
   ],
   "source": [
    "! cat /scratch${PWD#/prj}/slurm-10353645.out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submitted batch job 10353647\n"
     ]
    }
   ],
   "source": [
    "! sbatch  --partition nvidia_small  --ntasks-per-node=2  --nodes=18  --ntasks=36  ng2.srm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! squeue --job 10353647 --name ng2 --partition nvidia_small --format \"%.19V  %.19S  %.8i  %.12P %.5j  %.2t %.5M %.5D %.4C\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================================\n",
      "- Job ID: 10353647\n",
      "- Tasks per node: 2\n",
      "- # of nodes in the job: 18\n",
      "- # of tasks: 36\n",
      "- Partition: nvidia_small\n",
      "- Dir from which sbatch was invoked: b715\n",
      "- List of nodes allocated to the job: sdumont3057 sdumont3058 sdumont3059 sdumont3060 sdumont3061 sdumont3062 sdumont3063 sdumont3148 sdumont3149 sdumont3150 sdumont3151 sdumont3152 sdumont3153 sdumont3154 sdumont3155 sdumont3156 sdumont3157 sdumont3166\n",
      "-- modules ----------------------------\n",
      "conda activate env2, --stack env3\n",
      "-- run --------------------------------\n",
      "$ srun -n 36 python ng2.py\n",
      "-- output -----------------------------\n",
      "1. hostname    rank crank  cid\n",
      "2. ----------- ---- ----- ----\n",
      "3. sdumont3057   00    00   00\n",
      "3. sdumont3057   01    01   01\n",
      "3. sdumont3058   02    00   00\n",
      "3. sdumont3058   03    01   01\n",
      "3. sdumont3059   04    00   00\n",
      "3. sdumont3059   05    01   01\n",
      "3. sdumont3060   06    00   00\n",
      "3. sdumont3060   07    01   01\n",
      "3. sdumont3061   08    00   00\n",
      "3. sdumont3061   09    01   01\n",
      "3. sdumont3062   10    00   00\n",
      "3. sdumont3062   11    01   01\n",
      "3. sdumont3063   12    00   00\n",
      "3. sdumont3063   13    01   01\n",
      "3. sdumont3148   14    00   00\n",
      "3. sdumont3148   15    01   01\n",
      "3. sdumont3149   16    00   00\n",
      "3. sdumont3149   17    01   01\n",
      "3. sdumont3150   18    00   00\n",
      "3. sdumont3150   19    01   01\n",
      "3. sdumont3151   20    00   00\n",
      "3. sdumont3151   21    01   01\n",
      "3. sdumont3152   22    00   00\n",
      "3. sdumont3152   23    01   01\n",
      "3. sdumont3153   24    00   00\n",
      "3. sdumont3153   25    01   01\n",
      "3. sdumont3154   26    00   00\n",
      "3. sdumont3154   27    01   01\n",
      "3. sdumont3155   28    00   00\n",
      "3. sdumont3155   29    01   01\n",
      "3. sdumont3156   30    00   00\n",
      "3. sdumont3156   31    01   01\n",
      "3. sdumont3157   32    00   00\n",
      "3. sdumont3157   33    01   01\n",
      "3. sdumont3166   34    00   00\n",
      "3. sdumont3166   35    01   01\n",
      "4. ---------------------------\n",
      "5. Heat:1500.0000, TT:3.8655, TK:0.6475, TC:2.7754, TE:0.0011, MPI:36\n",
      "~~ end ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n"
     ]
    }
   ],
   "source": [
    "! cat /scratch${PWD#/prj}/slurm-10353647.out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submitted batch job 10353648\n"
     ]
    }
   ],
   "source": [
    "! sbatch  --partition nvidia_small  --ntasks-per-node=2  --nodes=18  --ntasks=36  ng2.srm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! squeue --job 10353648 --name ng2 --partition nvidia_small --format \"%.19V  %.19S  %.8i  %.12P %.5j  %.2t %.5M %.5D %.4C\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================================\n",
      "- Job ID: 10353648\n",
      "- Tasks per node: 2\n",
      "- # of nodes in the job: 18\n",
      "- # of tasks: 36\n",
      "- Partition: nvidia_small\n",
      "- Dir from which sbatch was invoked: b715\n",
      "- List of nodes allocated to the job: sdumont3057 sdumont3058 sdumont3059 sdumont3060 sdumont3061 sdumont3062 sdumont3063 sdumont3148 sdumont3149 sdumont3150 sdumont3151 sdumont3152 sdumont3153 sdumont3154 sdumont3155 sdumont3156 sdumont3157 sdumont3166\n",
      "-- modules ----------------------------\n",
      "conda activate env2, --stack env3\n",
      "-- run --------------------------------\n",
      "$ srun -n 36 python ng2.py\n",
      "-- output -----------------------------\n",
      "1. hostname    rank crank  cid\n",
      "2. ----------- ---- ----- ----\n",
      "3. sdumont3057   00    00   00\n",
      "3. sdumont3057   01    01   01\n",
      "3. sdumont3058   02    00   00\n",
      "3. sdumont3058   03    01   01\n",
      "3. sdumont3059   04    00   00\n",
      "3. sdumont3059   05    01   01\n",
      "3. sdumont3060   06    00   00\n",
      "3. sdumont3060   07    01   01\n",
      "3. sdumont3061   08    00   00\n",
      "3. sdumont3061   09    01   01\n",
      "3. sdumont3062   10    00   00\n",
      "3. sdumont3062   11    01   01\n",
      "3. sdumont3063   12    00   00\n",
      "3. sdumont3063   13    01   01\n",
      "3. sdumont3148   14    00   00\n",
      "3. sdumont3148   15    01   01\n",
      "3. sdumont3149   16    00   00\n",
      "3. sdumont3149   17    01   01\n",
      "3. sdumont3150   18    00   00\n",
      "3. sdumont3150   19    01   01\n",
      "3. sdumont3151   20    00   00\n",
      "3. sdumont3151   21    01   01\n",
      "3. sdumont3152   22    00   00\n",
      "3. sdumont3152   23    01   01\n",
      "3. sdumont3153   24    00   00\n",
      "3. sdumont3153   25    01   01\n",
      "3. sdumont3154   26    00   00\n",
      "3. sdumont3154   27    01   01\n",
      "3. sdumont3155   28    00   00\n",
      "3. sdumont3155   29    01   01\n",
      "3. sdumont3156   30    00   00\n",
      "3. sdumont3156   31    01   01\n",
      "3. sdumont3157   32    00   00\n",
      "3. sdumont3157   33    01   01\n",
      "3. sdumont3166   34    00   00\n",
      "3. sdumont3166   35    01   01\n",
      "4. ---------------------------\n",
      "5. Heat:1500.0000, TT:4.4013, TK:1.2745, TC:2.7594, TE:0.0010, MPI:36\n",
      "~~ end ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n"
     ]
    }
   ],
   "source": [
    "! cat /scratch${PWD#/prj}/slurm-10353648.out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        SUBMIT_TIME           START_TIME     JOBID     PARTITION  NAME  ST  TIME NODES CPUS\n",
      "2021-12-18T16:06:37  2021-12-19T18:17:17  10353639  nvidia_small   ng2  PD  0:00     5    9\n",
      "2021-12-18T16:06:40  2021-12-19T18:17:17  10353640  nvidia_small   ng2  PD  0:00     5    9\n",
      "2021-12-18T16:06:43  2021-12-19T18:17:17  10353641  nvidia_small   ng2  PD  0:00     5    9\n",
      "2021-12-18T16:06:57  2021-12-19T18:17:17  10353642  nvidia_small   ng2  PD  0:00     8   16\n",
      "2021-12-18T16:07:01  2021-12-19T18:17:17  10353643  nvidia_small   ng2  PD  0:00     8   16\n",
      "2021-12-18T16:07:03  2021-12-19T18:17:17  10353644  nvidia_small   ng2  PD  0:00     8   16\n",
      "2021-12-18T16:07:04  2021-12-19T18:17:17  10353645  nvidia_small   ng2  PD  0:00    18   36\n",
      "2021-12-18T16:07:06  2021-12-19T18:17:17  10353647  nvidia_small   ng2  PD  0:00    18   36\n",
      "2021-12-18T16:07:08  2021-12-19T18:17:17  10353648  nvidia_small   ng2  PD  0:00    18   36\n"
     ]
    }
   ],
   "source": [
    "! squeue --name ng2 --partition nvidia_small --format \"%.19V  %.19S  %.8i  %.12P %.5j  %.2t %.5M %.5D %.4C\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        SUBMIT_TIME           START_TIME     JOBID     PARTITION  NAME  ST  TIME NODES CPUS\n"
     ]
    }
   ],
   "source": [
    "! squeue --name ng2 --partition nvidia_small --format \"%.19V  %.19S  %.8i  %.12P %.5j  %.2t %.5M %.5D %.4C\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
