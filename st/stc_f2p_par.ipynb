{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stencil F2Py Parallel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting stc_f2p_par.f90\n"
     ]
    }
   ],
   "source": [
    "%%writefile stc_f2p_par.f90\n",
    "subroutine stm(n, energy, niters, oheat, otime, orank)\n",
    "    use MPI\n",
    "    implicit none\n",
    "    integer, intent(in) :: n, energy, niters\n",
    "    double precision, intent(out) :: oheat, otime\n",
    "    integer, intent(out) :: orank\n",
    "    \n",
    "    integer :: iters, i, j, px, py, rx, ry\n",
    "    integer :: north, south, west, east, bx, by, offx, offy\n",
    "    integer :: mpirank, mpisize, mpitag=1, mpierror\n",
    "    integer, dimension(2) :: pdims=0\n",
    "    integer, dimension(4) :: sendrequest, recvrequest\n",
    "    double precision :: mpiwtime=0.0, heat=0.0, rheat=0.0\n",
    "    double precision, dimension(:), allocatable   :: sendnorthgz, sendsouthgz\n",
    "    double precision, dimension(:), allocatable   :: recvnorthgz, recvsouthgz\n",
    "    double precision, dimension(:,:), allocatable :: aold, anew\n",
    "\n",
    "    integer, parameter  :: nsources=3        ! three heat sources\n",
    "    ! locnsources = number of sources in my area\n",
    "    integer             :: locnsources=0, locx, locy\n",
    "    ! locsources = sources local to my rank\n",
    "    integer, dimension(nsources, 2) :: locsources=0, sources\n",
    "\n",
    "    call MPI_Init(mpierror)\n",
    "    call MPI_Comm_rank(MPI_COMM_WORLD, mpirank, mpierror)\n",
    "    call MPI_Comm_size(MPI_COMM_WORLD, mpisize, mpierror)\n",
    "\n",
    "    if (mpirank == 0) then\n",
    "        mpiwtime = -MPI_Wtime()     ! inicializa contador de tempo\n",
    "    endif\n",
    "    \n",
    "    ! Creates a division of processors in a Cartesian grid\n",
    "    ! MPI_DIMS_CREATE(NNODES, NDIMS, DIMS, IERROR)\n",
    "    !   NNODES - number of nodes in a grid\n",
    "    !   NDIMS - number of Cartesian dimensions \n",
    "    !   DIMS - array specifying the number of nodes in each dimension\n",
    "    ! Examples:\n",
    "    !   MPI_Dims_create(6, 2, dims)  ->  (3,2)\n",
    "    !   MPI_Dims_create(7, 2, dims)  ->  (7,1)\n",
    "    call MPI_Dims_create(mpisize, 2, pdims, mpierror)\n",
    "\n",
    "    ! determine my coordinates (x,y)\n",
    "    px = pdims(1)\n",
    "    py = pdims(2)\n",
    "    rx = mod(mpirank, px)\n",
    "    ry = mpirank / px\n",
    "\n",
    "    ! determine my four neighbors\n",
    "    north = (ry - 1) * px + rx; if( (ry - 1) < 0  ) north = MPI_PROC_NULL\n",
    "    south = (ry + 1) * px + rx; if( (ry + 1) >= py) south = MPI_PROC_NULL\n",
    "    west = ry * px + rx - 1;    if( (rx - 1) < 0  ) west  = MPI_PROC_NULL\n",
    "    east = ry * px + rx + 1;    if( (rx + 1) >= px) east  = MPI_PROC_NULL\n",
    "\n",
    "    ! decompose the domain   \n",
    "    bx = n / px             ! block size in x\n",
    "    by = n / py             ! block size in y\n",
    "    offx = (rx * bx) + 1    ! offset in x\n",
    "    offy = (ry * by) + 1    ! offset in y\n",
    "\n",
    "    ! initialize heat sources\n",
    "    sources = reshape( [ n/2,   n/2,        &\n",
    "                         n/3,   n/3,        &\n",
    "                         n*4/5, n*8/9 ],    &\n",
    "              shape(sources), order=[2, 1])\n",
    "\n",
    "    do i = 1, nsources      ! determine which sources are in my patch\n",
    "        locx = sources(i, 1) - offx\n",
    "        locy = sources(i, 2) - offy    \n",
    "        if(locx >= 0 .and. locx <= bx .and. locy >= 0 .and. locy <= by) then\n",
    "            locnsources = locnsources + 1\n",
    "            locsources(locnsources, 1) = locx + 2\n",
    "            locsources(locnsources, 2) = locy + 2\n",
    "        endif\n",
    "    enddo\n",
    "\n",
    "    ! allocate communication buffers\n",
    "    allocate(sendnorthgz(bx))   ! send buffers\n",
    "    allocate(sendsouthgz(bx))\n",
    "    allocate(recvnorthgz(bx))   ! receive buffers\n",
    "    allocate(recvsouthgz(bx))\n",
    "    ! allocate two work arrays\n",
    "    allocate(aold(bx+2, by+2)); aold = 0.0   ! 1-wide halo zones!\n",
    "    allocate(anew(bx+2, by+2)); anew = 0.0   ! 1-wide halo zones!\n",
    "\n",
    "    ! laco principal das iteracoes\n",
    "    do iters = 1, niters, 2\n",
    "\n",
    "        ! --- anew <- stencil(aold) ---\n",
    "        if(north /= MPI_PROC_NULL) then \n",
    "            sendnorthgz = aold(2, 2:bx+1)\n",
    "            recvnorthgz = 0.0\n",
    "            call MPI_IRecv(recvnorthgz, bx, MPI_DOUBLE_PRECISION, north,  &\n",
    "                            mpitag, MPI_COMM_WORLD, recvrequest(1), mpierror)\n",
    "            call MPI_ISend(sendnorthgz, bx, MPI_DOUBLE_PRECISION, north,  &\n",
    "                            mpitag, MPI_COMM_WORLD, sendrequest(1), mpierror)\n",
    "        endif   \n",
    "        if(south /= MPI_PROC_NULL) then \n",
    "            sendsouthgz = aold(bx+1, 2:bx+1)\n",
    "            recvsouthgz(:) = 0.0\n",
    "            call MPI_IRecv(recvsouthgz, bx, MPI_DOUBLE_PRECISION, south,  &\n",
    "                            mpitag, MPI_COMM_WORLD, recvrequest(2), mpierror)\n",
    "            call MPI_ISend(sendsouthgz, bx, MPI_DOUBLE_PRECISION, south,  &\n",
    "                            mpitag, MPI_COMM_WORLD, sendrequest(2), mpierror)\n",
    "        endif    \n",
    "        if(east /= MPI_PROC_NULL) then \n",
    "            call MPI_IRecv(aold(2:bx+1, bx+2), bx, MPI_DOUBLE_PRECISION, east, &\n",
    "                            mpitag, MPI_COMM_WORLD, recvrequest(3), mpierror)\n",
    "            call MPI_ISend(aold(2:bx+1, bx+1), bx, MPI_DOUBLE_PRECISION, east, &\n",
    "                            mpitag, MPI_COMM_WORLD, sendrequest(3), mpierror)\n",
    "        endif    \n",
    "        if(west /= MPI_PROC_NULL) then \n",
    "            call MPI_IRecv(aold(2:bx+1, 1), bx, MPI_DOUBLE_PRECISION, west, &\n",
    "                           mpitag, MPI_COMM_WORLD, recvrequest(4), mpierror)\n",
    "            call MPI_ISend(aold(2:bx+1, 2), bx, MPI_DOUBLE_PRECISION, west, &\n",
    "                           mpitag, MPI_COMM_WORLD, sendrequest(4), mpierror)\n",
    "            endif\n",
    "        if(north /= MPI_PROC_NULL) then \n",
    "            call MPI_Wait(recvrequest(1), MPI_STATUS_IGNORE, mpierror)\n",
    "            call MPI_Wait(sendrequest(1), MPI_STATUS_IGNORE, mpierror)\n",
    "            aold(1, 2:bx+1)=recvnorthgz\n",
    "        endif\n",
    "        if(south /= MPI_PROC_NULL) then \n",
    "            call MPI_Wait(recvrequest(2), MPI_STATUS_IGNORE, mpierror)\n",
    "            call MPI_Wait(sendrequest(2), MPI_STATUS_IGNORE, mpierror)\n",
    "            aold(bx+2, 2:bx+1)=recvsouthgz\n",
    "        endif\n",
    "        if(east /= MPI_PROC_NULL) then \n",
    "            call MPI_Wait(recvrequest(3), MPI_STATUS_IGNORE, mpierror)\n",
    "            call MPI_Wait(sendrequest(3), MPI_STATUS_IGNORE, mpierror)\n",
    "        endif\n",
    "        if(west /= MPI_PROC_NULL) then \n",
    "            call MPI_Wait(recvrequest(4), MPI_STATUS_IGNORE, mpierror)\n",
    "            call MPI_Wait(sendrequest(4), MPI_STATUS_IGNORE, mpierror)\n",
    "        endif  \n",
    "\n",
    "        ! update grid points\n",
    "        do j = 2, by+1 \n",
    "            do i = 2, bx+1\n",
    "                anew(i, j) = aold(i, j)/2.0 + (aold(i-1, j) + aold(i+1, j) +  &\n",
    "                             aold(i, j-1) + aold(i, j+1)) / 4.0 / 2.0\n",
    "            enddo\n",
    "        enddo\n",
    "\n",
    "        ! adiciona calor a malha\n",
    "        do i = 1, locnsources\n",
    "            anew(locsources(i, 1), locsources(i, 2)) =   &\n",
    "                anew(locsources(i, 1), locsources(i, 2)) + energy\n",
    "        enddo\n",
    "\n",
    "        ! --- aold <- stencil(anew) ---\n",
    "        if(north /= MPI_PROC_NULL) then \n",
    "            sendnorthgz=anew(2, 2:bx+1)\n",
    "            call MPI_IRecv(recvnorthgz, bx, MPI_DOUBLE_PRECISION, north, mpitag,  &\n",
    "                            MPI_COMM_WORLD, recvrequest(1), mpierror)\n",
    "            call MPI_ISend(sendnorthgz, bx, MPI_DOUBLE_PRECISION, north, mpitag,  &\n",
    "                            MPI_COMM_WORLD, sendrequest(1), mpierror)\n",
    "        endif\n",
    "        if(south /= MPI_PROC_NULL) then \n",
    "            sendsouthgz=anew(bx+1, 2:bx+1)\n",
    "            call MPI_IRecv(recvsouthgz, bx, MPI_DOUBLE_PRECISION, south, mpitag,  &\n",
    "                            MPI_COMM_WORLD, recvrequest(2), mpierror)   \n",
    "            call MPI_ISend(sendsouthgz, bx, MPI_DOUBLE_PRECISION, south, mpitag,  &\n",
    "                            MPI_COMM_WORLD, sendrequest(2), mpierror)\n",
    "        endif\n",
    "        if(east /= MPI_PROC_NULL) then \n",
    "            call MPI_IRecv(anew(2:bx+1, bx+2), bx, MPI_DOUBLE_PRECISION, east,  &\n",
    "                            mpitag, MPI_COMM_WORLD, recvrequest(3), mpierror)\n",
    "            call MPI_ISend(anew(2:bx+1, bx+1), bx, MPI_DOUBLE_PRECISION, east,  &\n",
    "                            mpitag, MPI_COMM_WORLD, sendrequest(3), mpierror)\n",
    "        endif\n",
    "        if(west /= MPI_PROC_NULL) then \n",
    "            call MPI_IRecv(anew(2:bx+1, 1), bx, MPI_DOUBLE_PRECISION, west, mpitag,  &\n",
    "                            MPI_COMM_WORLD, recvrequest(4), mpierror)\n",
    "            call MPI_ISend(anew(2:bx+1, 2), bx, MPI_DOUBLE_PRECISION, west, mpitag,  &\n",
    "                            MPI_COMM_WORLD, sendrequest(4), mpierror)\n",
    "        endif\n",
    "        if(north /= MPI_PROC_NULL) then \n",
    "            call MPI_Wait(recvrequest(1), MPI_STATUS_IGNORE, mpierror)\n",
    "            call MPI_Wait(sendrequest(1), MPI_STATUS_IGNORE, mpierror)\n",
    "            anew(1, 2:bx+1)=recvnorthgz\n",
    "        endif\n",
    "        if(south /= MPI_PROC_NULL) then \n",
    "            call MPI_Wait(recvrequest(2), MPI_STATUS_IGNORE, mpierror)\n",
    "            call MPI_Wait(sendrequest(2), MPI_STATUS_IGNORE, mpierror)\n",
    "            anew(bx+2, 2:bx+1)=recvsouthgz\n",
    "        endif\n",
    "        if(east /= MPI_PROC_NULL) then \n",
    "            call MPI_Wait(recvrequest(3), MPI_STATUS_IGNORE, mpierror)\n",
    "            call MPI_Wait(sendrequest(3), MPI_STATUS_IGNORE, mpierror)\n",
    "        endif\n",
    "        if(west /= MPI_PROC_NULL) then \n",
    "            call MPI_Wait(recvrequest(4), MPI_STATUS_IGNORE, mpierror)\n",
    "            call MPI_Wait(sendrequest(4), MPI_STATUS_IGNORE, mpierror)\n",
    "        endif\n",
    "\n",
    "        ! update grid points\n",
    "        do j = 2, by+1 \n",
    "            do i = 2, bx+1\n",
    "                aold(i, j) = anew(i, j)/2.0 + (anew(i-1, j) + anew(i+1, j) +  &\n",
    "                             anew(i, j-1) + anew(i, j+1)) / 4.0 / 2.0\n",
    "            enddo\n",
    "        enddo\n",
    "\n",
    "        ! adiciona calor a malha:\n",
    "        do i = 1, locnsources\n",
    "            aold(locsources(i, 1), locsources(i, 2)) =  &\n",
    "                aold(locsources(i, 1), locsources(i, 2)) + energy\n",
    "        enddo\n",
    "\n",
    "    enddo\n",
    "   \n",
    "    ! ALL REDUCE:\n",
    "    heat = 0.0\n",
    "    do j = 2, by+1 \n",
    "        do i = 2, bx+1\n",
    "            heat = heat + aold(i, j)\n",
    "        enddo\n",
    "    enddo\n",
    "    call MPI_Allreduce(heat, rheat, 1, MPI_DOUBLE_PRECISION, MPI_SUM,  &\n",
    "                       MPI_COMM_WORLD, mpierror)\n",
    "\n",
    "    orank = mpirank\n",
    "    if(mpirank == 0) then\n",
    "        otime = mpiwtime + MPI_Wtime()\n",
    "        oheat = rheat\n",
    "!        write(*, \"('Heat='     f0.2' | ')\", advance=\"no\") rheat\n",
    "!        write(*, \"('Tempo='    f0.4' | ')\", advance=\"no\") mpiwtime\n",
    "!        write(*, \"('MPI_Size=' i0  ' | ')\", advance=\"no\") mpisize\n",
    "!        write(*, \"('MPI_Dims=('i0','i0') | ')\", advance=\"no\") pdims\n",
    "!        write(*, \"('bx,by=('i0','i0')')\") bx,by\n",
    "    \n",
    "    endif\n",
    "\n",
    "    call MPI_Finalize(mpierror)\n",
    "end subroutine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compilado na máquina com o hardware de destino"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running build\n",
      "running config_cc\n",
      "unifing config_cc, config, build_clib, build_ext, build commands --compiler options\n",
      "running config_fc\n",
      "unifing config_fc, config, build_clib, build_ext, build commands --fcompiler options\n",
      "running build_src\n",
      "build_src\n",
      "building extension \"stc_f2p_par\" sources\n",
      "f2py options: []\n",
      "f2py:> /tmp/tmp6y5mae9k/src.linux-x86_64-3.7/stc_f2p_parmodule.c\n",
      "creating /tmp/tmp6y5mae9k/src.linux-x86_64-3.7\n",
      "Reading fortran codes...\n",
      "\tReading file 'stc_f2p_par.f90' (format:free)\n",
      "Post-processing...\n",
      "\tBlock: stc_f2p_par\n",
      "\t\t\tBlock: stm\n",
      "In: :stc_f2p_par:stc_f2p_par.f90:stm\n",
      "get_useparameters: no module mpi info used by stm\n",
      "Post-processing (stage 2)...\n",
      "Building modules...\n",
      "\tBuilding module \"stc_f2p_par\"...\n",
      "\t\tConstructing wrapper function \"stm\"...\n",
      "\t\t  oheat,otime,orank = stm(n,energy,niters)\n",
      "\tWrote C/API module \"stc_f2p_par\" to file \"/tmp/tmp6y5mae9k/src.linux-x86_64-3.7/stc_f2p_parmodule.c\"\n",
      "  adding '/tmp/tmp6y5mae9k/src.linux-x86_64-3.7/fortranobject.c' to sources.\n",
      "  adding '/tmp/tmp6y5mae9k/src.linux-x86_64-3.7' to include_dirs.\n",
      "copying /scratch/app/anaconda3/2018.12/lib/python3.7/site-packages/numpy/f2py/src/fortranobject.c -> /tmp/tmp6y5mae9k/src.linux-x86_64-3.7\n",
      "copying /scratch/app/anaconda3/2018.12/lib/python3.7/site-packages/numpy/f2py/src/fortranobject.h -> /tmp/tmp6y5mae9k/src.linux-x86_64-3.7\n",
      "build_src: building npy-pkg config files\n",
      "running build_ext\n",
      "customize UnixCCompiler\n",
      "customize UnixCCompiler using build_ext\n",
      "get_default_fcompiler: matching types: '['gnu95', 'intel', 'lahey', 'pg', 'absoft', 'nag', 'vast', 'compaq', 'intele', 'intelem', 'gnu', 'g95', 'pathf95', 'nagfor']'\n",
      "customize Gnu95FCompiler\n",
      "Found executable /scratch/app/openmpi/4.0_gnu/bin/mpif90\n",
      "Found executable /usr/bin/gfortran\n",
      "customize Gnu95FCompiler\n",
      "customize Gnu95FCompiler using build_ext\n",
      "building 'stc_f2p_par' extension\n",
      "compiling C sources\n",
      "C compiler: gcc -pthread -B /scratch/app/anaconda3/2018.12/compiler_compat -Wl,--sysroot=/ -Wsign-compare -DNDEBUG -g -fwrapv -O3 -Wall -Wstrict-prototypes -I/scratch/app/openmpi/4.0_gnu/include -fPIC\n",
      "\n",
      "creating /tmp/tmp6y5mae9k/tmp\n",
      "creating /tmp/tmp6y5mae9k/tmp/tmp6y5mae9k\n",
      "creating /tmp/tmp6y5mae9k/tmp/tmp6y5mae9k/src.linux-x86_64-3.7\n",
      "compile options: '-I/tmp/tmp6y5mae9k/src.linux-x86_64-3.7 -I/scratch/app/anaconda3/2018.12/lib/python3.7/site-packages/numpy/core/include -I/scratch/app/anaconda3/2018.12/include/python3.7m -c'\n",
      "gcc: /tmp/tmp6y5mae9k/src.linux-x86_64-3.7/stc_f2p_parmodule.c\n",
      "gcc: /tmp/tmp6y5mae9k/src.linux-x86_64-3.7/fortranobject.c\n",
      "compiling Fortran sources\n",
      "Fortran f77 compiler: /scratch/app/openmpi/4.0_gnu/bin/mpif90 -Wall -g -ffixed-form -fno-second-underscore -fPIC -O3 -funroll-loops\n",
      "Fortran f90 compiler: mpif90 -Wall -g -fno-second-underscore -fPIC -O3 -funroll-loops\n",
      "Fortran fix compiler: mpif90 -Wall -g -ffixed-form -fno-second-underscore -Wall -g -fno-second-underscore -fPIC -O3 -funroll-loops\n",
      "compile options: '-I/tmp/tmp6y5mae9k/src.linux-x86_64-3.7 -I/scratch/app/anaconda3/2018.12/lib/python3.7/site-packages/numpy/core/include -I/scratch/app/anaconda3/2018.12/include/python3.7m -c'\n",
      "mpif90:f90: stc_f2p_par.f90\n",
      "/scratch/app/openmpi/4.0_gnu/bin/mpif90 -Wall -g -Wall -g -shared -L/scratch/app/openmpi/4.0_gnu/lib /tmp/tmp6y5mae9k/tmp/tmp6y5mae9k/src.linux-x86_64-3.7/stc_f2p_parmodule.o /tmp/tmp6y5mae9k/tmp/tmp6y5mae9k/src.linux-x86_64-3.7/fortranobject.o /tmp/tmp6y5mae9k/stc_f2p_par.o -L/usr/lib/gcc/x86_64-redhat-linux/4.8.5 -L/usr/lib/gcc/x86_64-redhat-linux/4.8.5 -lgfortran -o ./stc_f2p_par.cpython-37m-x86_64-linux-gnu.so\n",
      "Removing build directory /tmp/tmp6y5mae9k\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "In file included from /scratch/app/anaconda3/2018.12/lib/python3.7/site-packages/numpy/core/include/numpy/ndarraytypes.h:1832:0,\n",
      "                 from /scratch/app/anaconda3/2018.12/lib/python3.7/site-packages/numpy/core/include/numpy/ndarrayobject.h:12,\n",
      "                 from /scratch/app/anaconda3/2018.12/lib/python3.7/site-packages/numpy/core/include/numpy/arrayobject.h:4,\n",
      "                 from /tmp/tmp6y5mae9k/src.linux-x86_64-3.7/fortranobject.h:13,\n",
      "                 from /tmp/tmp6y5mae9k/src.linux-x86_64-3.7/fortranobject.c:2:\n",
      "/scratch/app/anaconda3/2018.12/lib/python3.7/site-packages/numpy/core/include/numpy/npy_1_7_deprecated_api.h:17:2: warning: #warning \"Using deprecated NumPy API, disable it with \" \"#define NPY_NO_DEPRECATED_API NPY_1_7_API_VERSION\" [-Wcpp]\n",
      " #warning \"Using deprecated NumPy API, disable it with \" \\\n",
      "  ^\n",
      "In file included from /scratch/app/anaconda3/2018.12/lib/python3.7/site-packages/numpy/core/include/numpy/ndarraytypes.h:1832:0,\n",
      "                 from /scratch/app/anaconda3/2018.12/lib/python3.7/site-packages/numpy/core/include/numpy/ndarrayobject.h:12,\n",
      "                 from /scratch/app/anaconda3/2018.12/lib/python3.7/site-packages/numpy/core/include/numpy/arrayobject.h:4,\n",
      "                 from /tmp/tmp6y5mae9k/src.linux-x86_64-3.7/fortranobject.h:13,\n",
      "                 from /tmp/tmp6y5mae9k/src.linux-x86_64-3.7/stc_f2p_parmodule.c:15:\n",
      "/scratch/app/anaconda3/2018.12/lib/python3.7/site-packages/numpy/core/include/numpy/npy_1_7_deprecated_api.h:17:2: warning: #warning \"Using deprecated NumPy API, disable it with \" \"#define NPY_NO_DEPRECATED_API NPY_1_7_API_VERSION\" [-Wcpp]\n",
      " #warning \"Using deprecated NumPy API, disable it with \" \\\n",
      "  ^\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "rm stc_f2p_par.*.so\n",
    "module load openmpi/gnu/4.0.1\n",
    "f2py -c stc_f2p_par.f90 -m stc_f2p_par --f90exec=mpif90"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "oheat,otime,orank = stm(n,energy,niters)\n",
      "\n",
      "Wrapper for ``stm``.\n",
      "\n",
      "Parameters\n",
      "----------\n",
      "n : input int\n",
      "energy : input int\n",
      "niters : input int\n",
      "\n",
      "Returns\n",
      "-------\n",
      "oheat : float\n",
      "otime : float\n",
      "orank : int\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# para funcionar precisa resetar o kernel\n",
    "from stc_f2p_par import stm\n",
    "print(stm.__doc__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting stencil_mpi.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile stencil_mpi.py\n",
    "from time import time\n",
    "from stc_f2p_par import stm\n",
    "\n",
    "n            = 4800    # nxn grid; 4800,1,500→1500; 100,1,10→30; [4800]\n",
    "energy       = 1       # energy to be injected per iteration; [1]\n",
    "niters       = 500     # number of iterations; [500]\n",
    "heat         = 0.0\n",
    "t            = 0.0\n",
    "t0           = 0.0\n",
    "rank         = 0\n",
    "\n",
    "t0 = time()\n",
    "heat, t, rank = stm(n, energy, niters)\n",
    "t0 = time() - t0\n",
    "\n",
    "if not rank :\n",
    "    print(\"Heat = %0.4f | Tempo = %0.4f | TempoPyt = %0.4f\" %(heat, t, t0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Heat = 1500.0000 | Tempo = 5.4505 | TempoPyt = 5.5963\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "module load openmpi/gnu/4.0.1\n",
    "mpiexec -n 4 python stencil_mpi.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "s='/prj/ampemi/eduardo.miranda2/stnc/F2Py'\n",
    "d='/scratch/ampemi/eduardo.miranda2/stnc/F2Py'\n",
    "rm  $d/stc_f2p_par.*.so\n",
    "cp  $s/stc_f2p_par.*.so  $s/stencil_mpi.py  $d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting stc_f2p_par_81.srm\n"
     ]
    }
   ],
   "source": [
    "%%writefile stc_f2p_par_81.srm\n",
    "#!/bin/bash\n",
    "# limites das filas (1,0 UA):\n",
    "#   cpu_dev  : 20 min.,  1-4  nós, 1/1   tarefas em exec/fila máximo\n",
    "#   cpu_small: 72 horas, 1-20 nós, 16/96 tarefas em exec/fila máximo\n",
    "# 1x1=1, 2x2=4, 3x3=9, 4x4=16, 6x6=36, 7x7=49, 8x8=64, 9x9=81\n",
    "\n",
    "#SBATCH --ntasks=81            #Total de tarefas\n",
    "#SBATCH -p cpu_small           #Fila (partition) a ser utilizada\n",
    "#SBATCH -J stf2pa              #Nome do job, 8 caracteres\n",
    "#SBATCH --time=00:02:00        #Tempo max. de execução 2 minutos\n",
    "# #SBATCH --nodes=5              #Qtd de nós\n",
    "# #SBATCH --ntasks-per-node=5    #Qtd de tarefas por nó ($SLURM_NTASKS_PER_NODE)\n",
    "# #SBATCH --exclusive            #Utilização exclusiva dos nós\n",
    "\n",
    "echo '========================================'\n",
    "echo '- Job ID:' $SLURM_JOB_ID\n",
    "echo '- Tarefas por no:' $SLURM_NTASKS_PER_NODE\n",
    "echo '- Qtd. de nos:' $SLURM_JOB_NUM_NODES\n",
    "echo '- Tot. de tarefas:' $SLURM_NTASKS\n",
    "echo '- Nos alocados:' $SLURM_JOB_NODELIST\n",
    "echo '- diretorio onde sbatch foi chamado ($SLURM_SUBMIT_DIR):'\n",
    "echo $SLURM_SUBMIT_DIR\n",
    "cd $SLURM_SUBMIT_DIR\n",
    "nodeset -e $SLURM_JOB_NODELIST\n",
    "\n",
    "#Configura o ambiente\n",
    "echo '-- modulos ----------------------------'\n",
    "echo 'module load openmpi/gnu/4.0.1'\n",
    "module load openmpi/gnu/4.0.1\n",
    "\n",
    "#Acessa o diretório onde o script está localizado \n",
    "cd /scratch/ampemi/eduardo.miranda2/stnc/F2Py/\n",
    "\n",
    "#Configura o executavel\n",
    "EXEC='python stencil_mpi.py'\n",
    "\n",
    "#Dispara a execucao\n",
    "echo '-- srun -------------------------------'\n",
    "echo '$ srun --mpi=pmi2 -n' $SLURM_NTASKS $EXEC\n",
    "srun --mpi=pmi2 -n $SLURM_NTASKS $EXEC\n",
    "echo '-- FIM --------------------------------'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testa se funciona"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submitted batch job 781260\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "sbatch stc_f2p_par_04.srm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             JOBID PARTITION     NAME     USER ST       TIME  NODES NODELIST(REASON)\n",
      "            781260 cpu_small   stf2pa eduardo. PD       0:00      1 (Priority)\n"
     ]
    }
   ],
   "source": [
    "! squeue -n stf2pa  # verifica se já terminou"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             JOBID PARTITION     NAME     USER ST       TIME  NODES NODELIST(REASON)\n"
     ]
    }
   ],
   "source": [
    "! squeue -n stf2pa  # verifica se já terminou"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================================\n",
      "- Job ID: 781260\n",
      "- Tarefas por no:\n",
      "- Qtd. de nos: 1\n",
      "- Tot. de tarefas: 4\n",
      "- Nos alocados: sdumont1124\n",
      "- diretorio onde sbatch foi chamado ($SLURM_SUBMIT_DIR):\n",
      "/prj/ampemi/eduardo.miranda2/stnc/F2Py\n",
      "sdumont1124\n",
      "-- modulos ----------------------------\n",
      "module load openmpi/gnu/4.0.1\n",
      "-- srun -------------------------------\n",
      "$ srun --mpi=pmi2 -n 4 python stencil_mpi.py\n",
      "Heat = 1500.0000 | Tempo = 7.4491 | TempoPyt = 7.6862\n",
      "-- FIM --------------------------------\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "d='/scratch/ampemi/eduardo.miranda2/stnc/F2Py'\n",
    "cat $d/slurm-781260.out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Envia demais"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submitted batch job 781265\n",
      "Submitted batch job 781266\n",
      "Submitted batch job 781267\n",
      "Submitted batch job 781268\n",
      "Submitted batch job 781269\n",
      "Submitted batch job 781270\n",
      "Submitted batch job 781271\n",
      "Submitted batch job 781272\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "# 1x1=1, 3x3=9, 4x4=16, 6x6=36, 7x7=49, 8x8=64, 9x9=81\n",
    "sbatch stc_f2p_par_01.srm\n",
    "sbatch stc_f2p_par_09.srm\n",
    "sbatch stc_f2p_par_16.srm\n",
    "sbatch stc_f2p_par_25.srm  # enviado por engano\n",
    "sbatch stc_f2p_par_36.srm\n",
    "sbatch stc_f2p_par_49.srm\n",
    "sbatch stc_f2p_par_64.srm\n",
    "sbatch stc_f2p_par_81.srm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             JOBID PARTITION     NAME     USER ST       TIME  NODES NODELIST(REASON)\n",
      "            781265 cpu_small   stf2pa eduardo.  R       0:05      1 sdumont1186\n",
      "            781266 cpu_small   stf2pa eduardo.  R       0:05      1 sdumont1328\n",
      "            781267 cpu_small   stf2pa eduardo.  R       0:05      1 sdumont1329\n",
      "            781269 cpu_small   stf2pa eduardo.  R       0:05      2 sdumont[1206-1207]\n",
      "            781270 cpu_small   stf2pa eduardo.  R       0:05      3 sdumont[1208-1210]\n",
      "            781271 cpu_small   stf2pa eduardo.  R       0:05      3 sdumont[1211-1213]\n",
      "            781272 cpu_small   stf2pa eduardo.  R       0:05      4 sdumont[1214-1217]\n"
     ]
    }
   ],
   "source": [
    "! squeue -n stf2pa  # verifica se já terminou"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             JOBID PARTITION     NAME     USER ST       TIME  NODES NODELIST(REASON)\n"
     ]
    }
   ],
   "source": [
    "! squeue -n stf2pa  # verifica se já terminou"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================================\n",
      "- Job ID: 781265\n",
      "- Tarefas por no:\n",
      "- Qtd. de nos: 1\n",
      "- Tot. de tarefas: 1\n",
      "- Nos alocados: sdumont1186\n",
      "- diretorio onde sbatch foi chamado ($SLURM_SUBMIT_DIR):\n",
      "/prj/ampemi/eduardo.miranda2/stnc/F2Py\n",
      "sdumont1186\n",
      "-- modulos ----------------------------\n",
      "module load openmpi/gnu/4.0.1\n",
      "-- srun -------------------------------\n",
      "$ srun --mpi=pmi2 -n 1 python stencil_mpi.py\n",
      "Heat = 1500.0000 | Tempo = 23.4926 | TempoPyt = 23.6593\n",
      "-- FIM --------------------------------\n",
      "========================================\n",
      "- Job ID: 781260\n",
      "- Tarefas por no:\n",
      "- Qtd. de nos: 1\n",
      "- Tot. de tarefas: 4\n",
      "- Nos alocados: sdumont1124\n",
      "- diretorio onde sbatch foi chamado ($SLURM_SUBMIT_DIR):\n",
      "/prj/ampemi/eduardo.miranda2/stnc/F2Py\n",
      "sdumont1124\n",
      "-- modulos ----------------------------\n",
      "module load openmpi/gnu/4.0.1\n",
      "-- srun -------------------------------\n",
      "$ srun --mpi=pmi2 -n 4 python stencil_mpi.py\n",
      "Heat = 1500.0000 | Tempo = 7.4491 | TempoPyt = 7.6862\n",
      "-- FIM --------------------------------\n",
      "========================================\n",
      "- Job ID: 781266\n",
      "- Tarefas por no:\n",
      "- Qtd. de nos: 1\n",
      "- Tot. de tarefas: 9\n",
      "- Nos alocados: sdumont1328\n",
      "- diretorio onde sbatch foi chamado ($SLURM_SUBMIT_DIR):\n",
      "/prj/ampemi/eduardo.miranda2/stnc/F2Py\n",
      "sdumont1328\n",
      "-- modulos ----------------------------\n",
      "module load openmpi/gnu/4.0.1\n",
      "-- srun -------------------------------\n",
      "$ srun --mpi=pmi2 -n 9 python stencil_mpi.py\n",
      "Heat = 2434.6916 | Tempo = 6.1362 | TempoPyt = 6.4922\n",
      "-- FIM --------------------------------\n",
      "========================================\n",
      "- Job ID: 781267\n",
      "- Tarefas por no:\n",
      "- Qtd. de nos: 1\n",
      "- Tot. de tarefas: 16\n",
      "- Nos alocados: sdumont1329\n",
      "- diretorio onde sbatch foi chamado ($SLURM_SUBMIT_DIR):\n",
      "/prj/ampemi/eduardo.miranda2/stnc/F2Py\n",
      "sdumont1329\n",
      "-- modulos ----------------------------\n",
      "module load openmpi/gnu/4.0.1\n",
      "-- srun -------------------------------\n",
      "$ srun --mpi=pmi2 -n 16 python stencil_mpi.py\n",
      "Heat = 1500.0000 | Tempo = 4.6088 | TempoPyt = 5.1436\n",
      "-- FIM --------------------------------\n",
      "========================================\n",
      "- Job ID: 781269\n",
      "- Tarefas por no:\n",
      "- Qtd. de nos: 2\n",
      "- Tot. de tarefas: 36\n",
      "- Nos alocados: sdumont[1206-1207]\n",
      "- diretorio onde sbatch foi chamado ($SLURM_SUBMIT_DIR):\n",
      "/prj/ampemi/eduardo.miranda2/stnc/F2Py\n",
      "sdumont1206 sdumont1207\n",
      "-- modulos ----------------------------\n",
      "module load openmpi/gnu/4.0.1\n",
      "-- srun -------------------------------\n",
      "$ srun --mpi=pmi2 -n 36 python stencil_mpi.py\n",
      "Heat = 1500.0000 | Tempo = 2.1302 | TempoPyt = 3.2395\n",
      "-- FIM --------------------------------\n",
      "========================================\n",
      "- Job ID: 781270\n",
      "- Tarefas por no:\n",
      "- Qtd. de nos: 3\n",
      "- Tot. de tarefas: 49\n",
      "- Nos alocados: sdumont[1208-1210]\n",
      "- diretorio onde sbatch foi chamado ($SLURM_SUBMIT_DIR):\n",
      "/prj/ampemi/eduardo.miranda2/stnc/F2Py\n",
      "sdumont1208 sdumont1209 sdumont1210\n",
      "-- modulos ----------------------------\n",
      "module load openmpi/gnu/4.0.1\n",
      "-- srun -------------------------------\n",
      "$ srun --mpi=pmi2 -n 49 python stencil_mpi.py\n",
      "Heat = 1497.8521 | Tempo = 1.6158 | TempoPyt = 2.5932\n",
      "-- FIM --------------------------------\n",
      "========================================\n",
      "- Job ID: 781271\n",
      "- Tarefas por no:\n",
      "- Qtd. de nos: 3\n",
      "- Tot. de tarefas: 64\n",
      "- Nos alocados: sdumont[1211-1213]\n",
      "- diretorio onde sbatch foi chamado ($SLURM_SUBMIT_DIR):\n",
      "/prj/ampemi/eduardo.miranda2/stnc/F2Py\n",
      "sdumont1211 sdumont1212 sdumont1213\n",
      "-- modulos ----------------------------\n",
      "module load openmpi/gnu/4.0.1\n",
      "-- srun -------------------------------\n",
      "$ srun --mpi=pmi2 -n 64 python stencil_mpi.py\n",
      "Heat = 1500.0000 | Tempo = 1.2601 | TempoPyt = 2.4187\n",
      "-- FIM --------------------------------\n",
      "========================================\n",
      "- Job ID: 781272\n",
      "- Tarefas por no:\n",
      "- Qtd. de nos: 4\n",
      "- Tot. de tarefas: 81\n",
      "- Nos alocados: sdumont[1214-1217]\n",
      "- diretorio onde sbatch foi chamado ($SLURM_SUBMIT_DIR):\n",
      "/prj/ampemi/eduardo.miranda2/stnc/F2Py\n",
      "sdumont1214 sdumont1215 sdumont1216 sdumont1217\n",
      "-- modulos ----------------------------\n",
      "module load openmpi/gnu/4.0.1\n",
      "-- srun -------------------------------\n",
      "$ srun --mpi=pmi2 -n 81 python stencil_mpi.py\n",
      "Heat = 1500.0000 | Tempo = 0.9868 | TempoPyt = 1.9261\n",
      "-- FIM --------------------------------\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "d='/scratch/ampemi/eduardo.miranda2/stnc/F2Py'\n",
    "cat $d/slurm-781265.out  #1\n",
    "cat $d/slurm-781260.out  #4\n",
    "cat $d/slurm-781266.out  #9\n",
    "cat $d/slurm-781267.out  #16\n",
    "cat $d/slurm-781269.out  #36\n",
    "cat $d/slurm-781270.out  #49\n",
    "cat $d/slurm-781271.out  #64\n",
    "cat $d/slurm-781272.out  #81"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Segunda tomada de tempo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submitted batch job 788047\n",
      "Submitted batch job 788048\n",
      "Submitted batch job 788049\n",
      "Submitted batch job 788050\n",
      "Submitted batch job 788051\n",
      "Submitted batch job 788052\n",
      "Submitted batch job 788053\n",
      "Submitted batch job 788054\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "# 1x1=1, 3x3=9, 4x4=16, 6x6=36, 7x7=49, 8x8=64, 9x9=81\n",
    "sbatch stc_f2p_par_01.srm\n",
    "sbatch stc_f2p_par_04.srm\n",
    "sbatch stc_f2p_par_09.srm\n",
    "sbatch stc_f2p_par_16.srm\n",
    "sbatch stc_f2p_par_36.srm\n",
    "sbatch stc_f2p_par_49.srm\n",
    "sbatch stc_f2p_par_64.srm\n",
    "sbatch stc_f2p_par_81.srm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             JOBID PARTITION     NAME     USER ST       TIME  NODES NODELIST(REASON)\n",
      "            788047 cpu_small   stf2pa eduardo. PD       0:00      1 (Priority)\n",
      "            788048 cpu_small   stf2pa eduardo. PD       0:00      1 (Priority)\n",
      "            788049 cpu_small   stf2pa eduardo. PD       0:00      1 (Priority)\n",
      "            788050 cpu_small   stf2pa eduardo. PD       0:00      1 (Priority)\n",
      "            788051 cpu_small   stf2pa eduardo. PD       0:00      2 (Priority)\n",
      "            788052 cpu_small   stf2pa eduardo. PD       0:00      3 (Priority)\n",
      "            788053 cpu_small   stf2pa eduardo. PD       0:00      3 (Priority)\n",
      "            788054 cpu_small   stf2pa eduardo. PD       0:00      4 (Priority)\n"
     ]
    }
   ],
   "source": [
    "! squeue -n stf2pa  # verifica se já terminou"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             JOBID PARTITION     NAME     USER ST       TIME  NODES NODELIST(REASON)\n"
     ]
    }
   ],
   "source": [
    "! squeue -n stf2pa  # verifica se já terminou"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================================\n",
      "- Job ID: 788047\n",
      "- Tarefas por no:\n",
      "- Qtd. de nos: 1\n",
      "- Tot. de tarefas: 1\n",
      "- Nos alocados: sdumont1149\n",
      "- diretorio onde sbatch foi chamado ($SLURM_SUBMIT_DIR):\n",
      "/prj/ampemi/eduardo.miranda2/stnc/F2Py\n",
      "sdumont1149\n",
      "-- modulos ----------------------------\n",
      "module load openmpi/gnu/4.0.1\n",
      "-- srun -------------------------------\n",
      "$ srun --mpi=pmi2 -n 1 python stencil_mpi.py\n",
      "Heat = 1500.0000 | Tempo = 23.6711 | TempoPyt = 23.8236\n",
      "-- FIM --------------------------------\n",
      "========================================\n",
      "- Job ID: 788048\n",
      "- Tarefas por no:\n",
      "- Qtd. de nos: 1\n",
      "- Tot. de tarefas: 4\n",
      "- Nos alocados: sdumont1149\n",
      "- diretorio onde sbatch foi chamado ($SLURM_SUBMIT_DIR):\n",
      "/prj/ampemi/eduardo.miranda2/stnc/F2Py\n",
      "sdumont1149\n",
      "-- modulos ----------------------------\n",
      "module load openmpi/gnu/4.0.1\n",
      "-- srun -------------------------------\n",
      "$ srun --mpi=pmi2 -n 4 python stencil_mpi.py\n",
      "Heat = 1500.0000 | Tempo = 7.4518 | TempoPyt = 7.6480\n",
      "-- FIM --------------------------------\n",
      "========================================\n",
      "- Job ID: 788049\n",
      "- Tarefas por no:\n",
      "- Qtd. de nos: 1\n",
      "- Tot. de tarefas: 9\n",
      "- Nos alocados: sdumont1149\n",
      "- diretorio onde sbatch foi chamado ($SLURM_SUBMIT_DIR):\n",
      "/prj/ampemi/eduardo.miranda2/stnc/F2Py\n",
      "sdumont1149\n",
      "-- modulos ----------------------------\n",
      "module load openmpi/gnu/4.0.1\n",
      "-- srun -------------------------------\n",
      "$ srun --mpi=pmi2 -n 9 python stencil_mpi.py\n",
      "Heat = 2434.6916 | Tempo = 6.1345 | TempoPyt = 6.4371\n",
      "-- FIM --------------------------------\n",
      "========================================\n",
      "- Job ID: 788050\n",
      "- Tarefas por no:\n",
      "- Qtd. de nos: 1\n",
      "- Tot. de tarefas: 16\n",
      "- Nos alocados: sdumont1149\n",
      "- diretorio onde sbatch foi chamado ($SLURM_SUBMIT_DIR):\n",
      "/prj/ampemi/eduardo.miranda2/stnc/F2Py\n",
      "sdumont1149\n",
      "-- modulos ----------------------------\n",
      "module load openmpi/gnu/4.0.1\n",
      "-- srun -------------------------------\n",
      "$ srun --mpi=pmi2 -n 16 python stencil_mpi.py\n",
      "Heat = 1500.0000 | Tempo = 4.6300 | TempoPyt = 5.2079\n",
      "-- FIM --------------------------------\n",
      "========================================\n",
      "- Job ID: 788051\n",
      "- Tarefas por no:\n",
      "- Qtd. de nos: 2\n",
      "- Tot. de tarefas: 36\n",
      "- Nos alocados: sdumont[1083,1149]\n",
      "- diretorio onde sbatch foi chamado ($SLURM_SUBMIT_DIR):\n",
      "/prj/ampemi/eduardo.miranda2/stnc/F2Py\n",
      "sdumont1083 sdumont1149\n",
      "-- modulos ----------------------------\n",
      "module load openmpi/gnu/4.0.1\n",
      "-- srun -------------------------------\n",
      "$ srun --mpi=pmi2 -n 36 python stencil_mpi.py\n",
      "Heat = 1500.0000 | Tempo = 2.1601 | TempoPyt = 3.4227\n",
      "-- FIM --------------------------------\n",
      "========================================\n",
      "- Job ID: 788052\n",
      "- Tarefas por no:\n",
      "- Qtd. de nos: 3\n",
      "- Tot. de tarefas: 49\n",
      "- Nos alocados: sdumont[1083,1149,1272]\n",
      "- diretorio onde sbatch foi chamado ($SLURM_SUBMIT_DIR):\n",
      "/prj/ampemi/eduardo.miranda2/stnc/F2Py\n",
      "sdumont1083 sdumont1149 sdumont1272\n",
      "-- modulos ----------------------------\n",
      "module load openmpi/gnu/4.0.1\n",
      "-- srun -------------------------------\n",
      "$ srun --mpi=pmi2 -n 49 python stencil_mpi.py\n",
      "Heat = 1497.8521 | Tempo = 1.6377 | TempoPyt = 2.6867\n",
      "-- FIM --------------------------------\n",
      "========================================\n",
      "- Job ID: 788053\n",
      "- Tarefas por no:\n",
      "- Qtd. de nos: 3\n",
      "- Tot. de tarefas: 64\n",
      "- Nos alocados: sdumont[1083,1149,1272]\n",
      "- diretorio onde sbatch foi chamado ($SLURM_SUBMIT_DIR):\n",
      "/prj/ampemi/eduardo.miranda2/stnc/F2Py\n",
      "sdumont1083 sdumont1149 sdumont1272\n",
      "-- modulos ----------------------------\n",
      "module load openmpi/gnu/4.0.1\n",
      "-- srun -------------------------------\n",
      "$ srun --mpi=pmi2 -n 64 python stencil_mpi.py\n",
      "Heat = 1500.0000 | Tempo = 1.2680 | TempoPyt = 2.2962\n",
      "-- FIM --------------------------------\n",
      "========================================\n",
      "- Job ID: 788054\n",
      "- Tarefas por no:\n",
      "- Qtd. de nos: 4\n",
      "- Tot. de tarefas: 81\n",
      "- Nos alocados: sdumont[1083,1149,1272,1391]\n",
      "- diretorio onde sbatch foi chamado ($SLURM_SUBMIT_DIR):\n",
      "/prj/ampemi/eduardo.miranda2/stnc/F2Py\n",
      "sdumont1083 sdumont1149 sdumont1272 sdumont1391\n",
      "-- modulos ----------------------------\n",
      "module load openmpi/gnu/4.0.1\n",
      "-- srun -------------------------------\n",
      "$ srun --mpi=pmi2 -n 81 python stencil_mpi.py\n",
      "Heat = 1500.0000 | Tempo = 1.0519 | TempoPyt = 2.1022\n",
      "-- FIM --------------------------------\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "d='/scratch/ampemi/eduardo.miranda2/stnc/F2Py'\n",
    "cat $d/slurm-788047.out  #1\n",
    "cat $d/slurm-788048.out  #4\n",
    "cat $d/slurm-788049.out  #9\n",
    "cat $d/slurm-788050.out  #16\n",
    "cat $d/slurm-788051.out  #36\n",
    "cat $d/slurm-788052.out  #49\n",
    "cat $d/slurm-788053.out  #64\n",
    "cat $d/slurm-788054.out  #81"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Terceira tomada de tempo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submitted batch job 788055\n",
      "Submitted batch job 788056\n",
      "Submitted batch job 788057\n",
      "Submitted batch job 788058\n",
      "Submitted batch job 788059\n",
      "Submitted batch job 788060\n",
      "Submitted batch job 788061\n",
      "Submitted batch job 788062\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "# 1x1=1, 3x3=9, 4x4=16, 6x6=36, 7x7=49, 8x8=64, 9x9=81\n",
    "sbatch stc_f2p_par_01.srm\n",
    "sbatch stc_f2p_par_04.srm\n",
    "sbatch stc_f2p_par_09.srm\n",
    "sbatch stc_f2p_par_16.srm\n",
    "sbatch stc_f2p_par_36.srm\n",
    "sbatch stc_f2p_par_49.srm\n",
    "sbatch stc_f2p_par_64.srm\n",
    "sbatch stc_f2p_par_81.srm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             JOBID PARTITION     NAME     USER ST       TIME  NODES NODELIST(REASON)\n",
      "            788053 cpu_small   stf2pa eduardo. PD       0:00      3 (Resources)\n",
      "            788054 cpu_small   stf2pa eduardo. PD       0:00      4 (Resources)\n",
      "            788051 cpu_small   stf2pa eduardo. PD       0:00      2 (Resources)\n",
      "            788052 cpu_small   stf2pa eduardo. PD       0:00      3 (Resources)\n",
      "            788050 cpu_small   stf2pa eduardo. PD       0:00      1 (Resources)\n",
      "            788049 cpu_small   stf2pa eduardo. PD       0:00      1 (Resources)\n",
      "            788048 cpu_small   stf2pa eduardo. PD       0:00      1 (Resources)\n",
      "            788047 cpu_small   stf2pa eduardo. PD       0:00      1 (Resources)\n",
      "            788055 cpu_small   stf2pa eduardo. PD       0:00      1 (Priority)\n",
      "            788056 cpu_small   stf2pa eduardo. PD       0:00      1 (Priority)\n",
      "            788057 cpu_small   stf2pa eduardo. PD       0:00      1 (Priority)\n",
      "            788058 cpu_small   stf2pa eduardo. PD       0:00      1 (Priority)\n",
      "            788059 cpu_small   stf2pa eduardo. PD       0:00      2 (Priority)\n",
      "            788060 cpu_small   stf2pa eduardo. PD       0:00      3 (Priority)\n",
      "            788061 cpu_small   stf2pa eduardo. PD       0:00      3 (Priority)\n",
      "            788062 cpu_small   stf2pa eduardo. PD       0:00      4 (Priority)\n"
     ]
    }
   ],
   "source": [
    "! squeue -n stf2pa  # verifica se já terminou"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             JOBID PARTITION     NAME     USER ST       TIME  NODES NODELIST(REASON)\n"
     ]
    }
   ],
   "source": [
    "! squeue -n stf2pa  # verifica se já terminou"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================================\n",
      "- Job ID: 788055\n",
      "- Tarefas por no:\n",
      "- Qtd. de nos: 1\n",
      "- Tot. de tarefas: 1\n",
      "- Nos alocados: sdumont1149\n",
      "- diretorio onde sbatch foi chamado ($SLURM_SUBMIT_DIR):\n",
      "/prj/ampemi/eduardo.miranda2/stnc/F2Py\n",
      "sdumont1149\n",
      "-- modulos ----------------------------\n",
      "module load openmpi/gnu/4.0.1\n",
      "-- srun -------------------------------\n",
      "$ srun --mpi=pmi2 -n 1 python stencil_mpi.py\n",
      "Heat = 1500.0000 | Tempo = 23.6284 | TempoPyt = 23.7599\n",
      "-- FIM --------------------------------\n",
      "========================================\n",
      "- Job ID: 788056\n",
      "- Tarefas por no:\n",
      "- Qtd. de nos: 1\n",
      "- Tot. de tarefas: 4\n",
      "- Nos alocados: sdumont1149\n",
      "- diretorio onde sbatch foi chamado ($SLURM_SUBMIT_DIR):\n",
      "/prj/ampemi/eduardo.miranda2/stnc/F2Py\n",
      "sdumont1149\n",
      "-- modulos ----------------------------\n",
      "module load openmpi/gnu/4.0.1\n",
      "-- srun -------------------------------\n",
      "$ srun --mpi=pmi2 -n 4 python stencil_mpi.py\n",
      "Heat = 1500.0000 | Tempo = 7.4559 | TempoPyt = 7.6475\n",
      "-- FIM --------------------------------\n",
      "========================================\n",
      "- Job ID: 788057\n",
      "- Tarefas por no:\n",
      "- Qtd. de nos: 1\n",
      "- Tot. de tarefas: 9\n",
      "- Nos alocados: sdumont1149\n",
      "- diretorio onde sbatch foi chamado ($SLURM_SUBMIT_DIR):\n",
      "/prj/ampemi/eduardo.miranda2/stnc/F2Py\n",
      "sdumont1149\n",
      "-- modulos ----------------------------\n",
      "module load openmpi/gnu/4.0.1\n",
      "-- srun -------------------------------\n",
      "$ srun --mpi=pmi2 -n 9 python stencil_mpi.py\n",
      "Heat = 2434.6916 | Tempo = 6.2465 | TempoPyt = 6.6065\n",
      "-- FIM --------------------------------\n",
      "========================================\n",
      "- Job ID: 788058\n",
      "- Tarefas por no:\n",
      "- Qtd. de nos: 1\n",
      "- Tot. de tarefas: 16\n",
      "- Nos alocados: sdumont1149\n",
      "- diretorio onde sbatch foi chamado ($SLURM_SUBMIT_DIR):\n",
      "/prj/ampemi/eduardo.miranda2/stnc/F2Py\n",
      "sdumont1149\n",
      "-- modulos ----------------------------\n",
      "module load openmpi/gnu/4.0.1\n",
      "-- srun -------------------------------\n",
      "$ srun --mpi=pmi2 -n 16 python stencil_mpi.py\n",
      "Heat = 1500.0000 | Tempo = 4.6324 | TempoPyt = 5.5499\n",
      "-- FIM --------------------------------\n",
      "========================================\n",
      "- Job ID: 788059\n",
      "- Tarefas por no:\n",
      "- Qtd. de nos: 2\n",
      "- Tot. de tarefas: 36\n",
      "- Nos alocados: sdumont[1083,1149]\n",
      "- diretorio onde sbatch foi chamado ($SLURM_SUBMIT_DIR):\n",
      "/prj/ampemi/eduardo.miranda2/stnc/F2Py\n",
      "sdumont1083 sdumont1149\n",
      "-- modulos ----------------------------\n",
      "module load openmpi/gnu/4.0.1\n",
      "-- srun -------------------------------\n",
      "$ srun --mpi=pmi2 -n 36 python stencil_mpi.py\n",
      "Heat = 1500.0000 | Tempo = 2.1546 | TempoPyt = 3.0605\n",
      "-- FIM --------------------------------\n",
      "========================================\n",
      "- Job ID: 788060\n",
      "- Tarefas por no:\n",
      "- Qtd. de nos: 3\n",
      "- Tot. de tarefas: 49\n",
      "- Nos alocados: sdumont[1083,1149,1272]\n",
      "- diretorio onde sbatch foi chamado ($SLURM_SUBMIT_DIR):\n",
      "/prj/ampemi/eduardo.miranda2/stnc/F2Py\n",
      "sdumont1083 sdumont1149 sdumont1272\n",
      "-- modulos ----------------------------\n",
      "module load openmpi/gnu/4.0.1\n",
      "-- srun -------------------------------\n",
      "$ srun --mpi=pmi2 -n 49 python stencil_mpi.py\n",
      "Heat = 1497.8521 | Tempo = 1.6269 | TempoPyt = 2.6176\n",
      "-- FIM --------------------------------\n",
      "========================================\n",
      "- Job ID: 788061\n",
      "- Tarefas por no:\n",
      "- Qtd. de nos: 3\n",
      "- Tot. de tarefas: 64\n",
      "- Nos alocados: sdumont[1083,1149,1272]\n",
      "- diretorio onde sbatch foi chamado ($SLURM_SUBMIT_DIR):\n",
      "/prj/ampemi/eduardo.miranda2/stnc/F2Py\n",
      "sdumont1083 sdumont1149 sdumont1272\n",
      "-- modulos ----------------------------\n",
      "module load openmpi/gnu/4.0.1\n",
      "-- srun -------------------------------\n",
      "$ srun --mpi=pmi2 -n 64 python stencil_mpi.py\n",
      "Heat = 1500.0000 | Tempo = 1.2851 | TempoPyt = 2.3614\n",
      "-- FIM --------------------------------\n",
      "========================================\n",
      "- Job ID: 788062\n",
      "- Tarefas por no:\n",
      "- Qtd. de nos: 4\n",
      "- Tot. de tarefas: 81\n",
      "- Nos alocados: sdumont[1083,1149,1272,1391]\n",
      "- diretorio onde sbatch foi chamado ($SLURM_SUBMIT_DIR):\n",
      "/prj/ampemi/eduardo.miranda2/stnc/F2Py\n",
      "sdumont1083 sdumont1149 sdumont1272 sdumont1391\n",
      "-- modulos ----------------------------\n",
      "module load openmpi/gnu/4.0.1\n",
      "-- srun -------------------------------\n",
      "$ srun --mpi=pmi2 -n 81 python stencil_mpi.py\n",
      "Heat = 1500.0000 | Tempo = 0.9959 | TempoPyt = 1.9473\n",
      "-- FIM --------------------------------\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "d='/scratch/ampemi/eduardo.miranda2/stnc/F2Py'\n",
    "cat $d/slurm-788055.out  #1\n",
    "cat $d/slurm-788056.out  #4\n",
    "cat $d/slurm-788057.out  #9\n",
    "cat $d/slurm-788058.out  #16\n",
    "cat $d/slurm-788059.out  #36\n",
    "cat $d/slurm-788060.out  #49\n",
    "cat $d/slurm-788061.out  #64\n",
    "cat $d/slurm-788062.out  #81"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
