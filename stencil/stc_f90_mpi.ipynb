{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stencil F90 MPI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting stencil_mpi.f90\n"
     ]
    }
   ],
   "source": [
    "%%writefile stencil_mpi.f90\n",
    "program stencil\n",
    "    use MPI\n",
    "    implicit none\n",
    "    integer :: n=0          ! nxn grid\n",
    "    integer :: energy=0     ! energy to be injected per iteration\n",
    "    integer :: niters=0     ! number of iterations\n",
    "    integer :: iters, i, j, px, py, rx, ry\n",
    "    integer :: north, south, west, east, bx, by, offx, offy\n",
    "    integer :: nargs=0, iargc\n",
    "    integer :: mpirank, mpisize, mpitag=1, mpierror\n",
    "    integer, dimension(3) :: args\n",
    "    integer, dimension(2) :: pdims=0\n",
    "    integer, dimension(4) :: sendrequest, recvrequest\n",
    "    double precision :: mpiwtime=0.0, heat=0.0, rheat=0.0\n",
    "    double precision, dimension(:), allocatable   :: sendnorthgz, sendsouthgz\n",
    "    double precision, dimension(:), allocatable   :: recvnorthgz, recvsouthgz\n",
    "    double precision, dimension(:,:), allocatable :: aold, anew\n",
    "    character(len=50)                             :: argv\n",
    "\n",
    "    integer, parameter  :: nsources=3        ! three heat sources\n",
    "    ! locnsources = number of sources in my area\n",
    "    integer             :: locnsources=0, locx, locy\n",
    "    ! locsources = sources local to my rank\n",
    "    integer, dimension(nsources, 2) :: locsources=0, sources\n",
    "\n",
    "    call MPI_Init(mpierror)\n",
    "    call MPI_Comm_rank(MPI_COMM_WORLD, mpirank, mpierror)\n",
    "    call MPI_Comm_size(MPI_COMM_WORLD, mpisize, mpierror)\n",
    "\n",
    "    if (mpirank == 0) then                          ! rank 0 argument checking\n",
    "        mpiwtime = -MPI_Wtime()     ! inicializa contador de tempo\n",
    "        nargs = iargc()\n",
    "        call getarg(1, argv); read(argv, *) n       ! nxn grid\n",
    "        call getarg(2, argv); read(argv, *) energy  ! energy to be injected\n",
    "        call getarg(3, argv); read(argv, *) niters  ! number of iterations\n",
    "        args = [ n, energy, niters ]                ! distribute arguments\n",
    "        call MPI_Bcast(args, 3, MPI_INTEGER, 0, MPI_COMM_WORLD, mpierror)\n",
    "    else\n",
    "        call MPI_Bcast(args, 3, MPI_INTEGER, 0, MPI_COMM_WORLD, mpierror)\n",
    "        n = args(1); energy = args(2); niters  = args(3)\n",
    "    endif\n",
    "    \n",
    "    ! Creates a division of processors in a Cartesian grid\n",
    "    ! MPI_DIMS_CREATE(NNODES, NDIMS, DIMS, IERROR)\n",
    "    !   NNODES - number of nodes in a grid\n",
    "    !   NDIMS - number of Cartesian dimensions \n",
    "    !   DIMS - array specifying the number of nodes in each dimension\n",
    "    ! Examples:\n",
    "    !   MPI_Dims_create(6, 2, dims)  ->  (3,2)\n",
    "    !   MPI_Dims_create(7, 2, dims)  ->  (7,1)\n",
    "    call MPI_Dims_create(mpisize, 2, pdims, mpierror)\n",
    "\n",
    "    ! determine my coordinates (x,y)\n",
    "    px = pdims(1)\n",
    "    py = pdims(2)\n",
    "    rx = mod(mpirank, px)\n",
    "    ry = mpirank / px\n",
    "\n",
    "    ! determine my four neighbors\n",
    "    north = (ry - 1) * px + rx; if( (ry - 1) < 0  ) north = MPI_PROC_NULL\n",
    "    south = (ry + 1) * px + rx; if( (ry + 1) >= py) south = MPI_PROC_NULL\n",
    "    west = ry * px + rx - 1;    if( (rx - 1) < 0  ) west  = MPI_PROC_NULL\n",
    "    east = ry * px + rx + 1;    if( (rx + 1) >= px) east  = MPI_PROC_NULL\n",
    "\n",
    "    ! decompose the domain   \n",
    "    bx = n / px             ! block size in x\n",
    "    by = n / py             ! block size in y\n",
    "    offx = (rx * bx) + 1    ! offset in x\n",
    "    offy = (ry * by) + 1    ! offset in y\n",
    "\n",
    "    ! initialize heat sources\n",
    "    sources = reshape( [ n/2,   n/2,        &\n",
    "                         n/3,   n/3,        &\n",
    "                         n*4/5, n*8/9 ],    &\n",
    "              shape(sources), order=[2, 1])\n",
    "\n",
    "    do i = 1, nsources      ! determine which sources are in my patch\n",
    "        locx = sources(i, 1) - offx\n",
    "        locy = sources(i, 2) - offy    \n",
    "        if(locx >= 0 .and. locx <= bx .and. locy >= 0 .and. locy <= by) then\n",
    "            locnsources = locnsources + 1\n",
    "            locsources(locnsources, 1) = locx + 2\n",
    "            locsources(locnsources, 2) = locy + 2\n",
    "        endif\n",
    "    enddo\n",
    "\n",
    "    ! allocate communication buffers\n",
    "    allocate(sendnorthgz(bx))   ! send buffers\n",
    "    allocate(sendsouthgz(bx))\n",
    "    allocate(recvnorthgz(bx))   ! receive buffers\n",
    "    allocate(recvsouthgz(bx))\n",
    "    ! allocate two work arrays\n",
    "    allocate(aold(bx+2, by+2)); aold = 0.0   ! 1-wide halo zones!\n",
    "    allocate(anew(bx+2, by+2)); anew = 0.0   ! 1-wide halo zones!\n",
    "\n",
    "    ! laco principal das iteracoes\n",
    "    do iters = 1, niters, 2\n",
    "\n",
    "        ! --- anew <- stencil(aold) ---\n",
    "        if(north /= MPI_PROC_NULL) then \n",
    "            sendnorthgz = aold(2, 2:bx+1)\n",
    "            recvnorthgz = 0.0\n",
    "            call MPI_IRecv(recvnorthgz, bx, MPI_DOUBLE_PRECISION, north,  &\n",
    "                            mpitag, MPI_COMM_WORLD, recvrequest(1), mpierror)\n",
    "            call MPI_ISend(sendnorthgz, bx, MPI_DOUBLE_PRECISION, north,  &\n",
    "                            mpitag, MPI_COMM_WORLD, sendrequest(1), mpierror)\n",
    "        endif   \n",
    "        if(south /= MPI_PROC_NULL) then \n",
    "            sendsouthgz = aold(bx+1, 2:bx+1)\n",
    "            recvsouthgz(:) = 0.0\n",
    "            call MPI_IRecv(recvsouthgz, bx, MPI_DOUBLE_PRECISION, south,  &\n",
    "                            mpitag, MPI_COMM_WORLD, recvrequest(2), mpierror)\n",
    "            call MPI_ISend(sendsouthgz, bx, MPI_DOUBLE_PRECISION, south,  &\n",
    "                            mpitag, MPI_COMM_WORLD, sendrequest(2), mpierror)\n",
    "        endif    \n",
    "        if(east /= MPI_PROC_NULL) then \n",
    "            call MPI_IRecv(aold(2:bx+1, bx+2), bx, MPI_DOUBLE_PRECISION, east, &\n",
    "                            mpitag, MPI_COMM_WORLD, recvrequest(3), mpierror)\n",
    "            call MPI_ISend(aold(2:bx+1, bx+1), bx, MPI_DOUBLE_PRECISION, east, &\n",
    "                            mpitag, MPI_COMM_WORLD, sendrequest(3), mpierror)\n",
    "        endif    \n",
    "        if(west /= MPI_PROC_NULL) then \n",
    "            call MPI_IRecv(aold(2:bx+1, 1), bx, MPI_DOUBLE_PRECISION, west, &\n",
    "                           mpitag, MPI_COMM_WORLD, recvrequest(4), mpierror)\n",
    "            call MPI_ISend(aold(2:bx+1, 2), bx, MPI_DOUBLE_PRECISION, west, &\n",
    "                           mpitag, MPI_COMM_WORLD, sendrequest(4), mpierror)\n",
    "            endif\n",
    "        if(north /= MPI_PROC_NULL) then \n",
    "            call MPI_Wait(recvrequest(1), MPI_STATUS_IGNORE, mpierror)\n",
    "            call MPI_Wait(sendrequest(1), MPI_STATUS_IGNORE, mpierror)\n",
    "            aold(1, 2:bx+1)=recvnorthgz\n",
    "        endif\n",
    "        if(south /= MPI_PROC_NULL) then \n",
    "            call MPI_Wait(recvrequest(2), MPI_STATUS_IGNORE, mpierror)\n",
    "            call MPI_Wait(sendrequest(2), MPI_STATUS_IGNORE, mpierror)\n",
    "            aold(bx+2, 2:bx+1)=recvsouthgz\n",
    "        endif\n",
    "        if(east /= MPI_PROC_NULL) then \n",
    "            call MPI_Wait(recvrequest(3), MPI_STATUS_IGNORE, mpierror)\n",
    "            call MPI_Wait(sendrequest(3), MPI_STATUS_IGNORE, mpierror)\n",
    "        endif\n",
    "        if(west /= MPI_PROC_NULL) then \n",
    "            call MPI_Wait(recvrequest(4), MPI_STATUS_IGNORE, mpierror)\n",
    "            call MPI_Wait(sendrequest(4), MPI_STATUS_IGNORE, mpierror)\n",
    "        endif  \n",
    "\n",
    "        ! update grid points\n",
    "        do j = 2, by+1 \n",
    "            do i = 2, bx+1\n",
    "                anew(i, j) = aold(i, j)/2.0 + (aold(i-1, j) + aold(i+1, j) +  &\n",
    "                             aold(i, j-1) + aold(i, j+1)) / 4.0 / 2.0\n",
    "            enddo\n",
    "        enddo\n",
    "\n",
    "        ! adiciona calor a malha\n",
    "        do i = 1, locnsources\n",
    "            anew(locsources(i, 1), locsources(i, 2)) =   &\n",
    "                anew(locsources(i, 1), locsources(i, 2)) + energy\n",
    "        enddo\n",
    "\n",
    "        ! --- aold <- stencil(anew) ---\n",
    "        if(north /= MPI_PROC_NULL) then \n",
    "            sendnorthgz=anew(2, 2:bx+1)\n",
    "            call MPI_IRecv(recvnorthgz, bx, MPI_DOUBLE_PRECISION, north, mpitag,  &\n",
    "                            MPI_COMM_WORLD, recvrequest(1), mpierror)\n",
    "            call MPI_ISend(sendnorthgz, bx, MPI_DOUBLE_PRECISION, north, mpitag,  &\n",
    "                            MPI_COMM_WORLD, sendrequest(1), mpierror)\n",
    "        endif\n",
    "        if(south /= MPI_PROC_NULL) then \n",
    "            sendsouthgz=anew(bx+1, 2:bx+1)\n",
    "            call MPI_IRecv(recvsouthgz, bx, MPI_DOUBLE_PRECISION, south, mpitag,  &\n",
    "                            MPI_COMM_WORLD, recvrequest(2), mpierror)   \n",
    "            call MPI_ISend(sendsouthgz, bx, MPI_DOUBLE_PRECISION, south, mpitag,  &\n",
    "                            MPI_COMM_WORLD, sendrequest(2), mpierror)\n",
    "        endif\n",
    "        if(east /= MPI_PROC_NULL) then \n",
    "            call MPI_IRecv(anew(2:bx+1, bx+2), bx, MPI_DOUBLE_PRECISION, east,  &\n",
    "                            mpitag, MPI_COMM_WORLD, recvrequest(3), mpierror)\n",
    "            call MPI_ISend(anew(2:bx+1, bx+1), bx, MPI_DOUBLE_PRECISION, east,  &\n",
    "                            mpitag, MPI_COMM_WORLD, sendrequest(3), mpierror)\n",
    "        endif\n",
    "        if(west /= MPI_PROC_NULL) then \n",
    "            call MPI_IRecv(anew(2:bx+1, 1), bx, MPI_DOUBLE_PRECISION, west, mpitag,  &\n",
    "                            MPI_COMM_WORLD, recvrequest(4), mpierror)\n",
    "            call MPI_ISend(anew(2:bx+1, 2), bx, MPI_DOUBLE_PRECISION, west, mpitag,  &\n",
    "                            MPI_COMM_WORLD, sendrequest(4), mpierror)\n",
    "        endif\n",
    "        if(north /= MPI_PROC_NULL) then \n",
    "            call MPI_Wait(recvrequest(1), MPI_STATUS_IGNORE, mpierror)\n",
    "            call MPI_Wait(sendrequest(1), MPI_STATUS_IGNORE, mpierror)\n",
    "            anew(1, 2:bx+1)=recvnorthgz\n",
    "        endif\n",
    "        if(south /= MPI_PROC_NULL) then \n",
    "            call MPI_Wait(recvrequest(2), MPI_STATUS_IGNORE, mpierror)\n",
    "            call MPI_Wait(sendrequest(2), MPI_STATUS_IGNORE, mpierror)\n",
    "            anew(bx+2, 2:bx+1)=recvsouthgz\n",
    "        endif\n",
    "        if(east /= MPI_PROC_NULL) then \n",
    "            call MPI_Wait(recvrequest(3), MPI_STATUS_IGNORE, mpierror)\n",
    "            call MPI_Wait(sendrequest(3), MPI_STATUS_IGNORE, mpierror)\n",
    "        endif\n",
    "        if(west /= MPI_PROC_NULL) then \n",
    "            call MPI_Wait(recvrequest(4), MPI_STATUS_IGNORE, mpierror)\n",
    "            call MPI_Wait(sendrequest(4), MPI_STATUS_IGNORE, mpierror)\n",
    "        endif\n",
    "\n",
    "        ! update grid points\n",
    "        do j = 2, by+1 \n",
    "            do i = 2, bx+1\n",
    "                aold(i, j) = anew(i, j)/2.0 + (anew(i-1, j) + anew(i+1, j) +  &\n",
    "                             anew(i, j-1) + anew(i, j+1)) / 4.0 / 2.0\n",
    "            enddo\n",
    "        enddo\n",
    "\n",
    "        ! adiciona calor a malha:\n",
    "        do i = 1, locnsources\n",
    "            aold(locsources(i, 1), locsources(i, 2)) =  &\n",
    "                aold(locsources(i, 1), locsources(i, 2)) + energy\n",
    "        enddo\n",
    "\n",
    "    enddo\n",
    "   \n",
    "    ! ALL REDUCE:\n",
    "    heat = 0.0\n",
    "    do j = 2, by+1 \n",
    "        do i = 2, bx+1\n",
    "            heat = heat + aold(i, j)\n",
    "        enddo\n",
    "    enddo\n",
    "    call MPI_Allreduce(heat, rheat, 1, MPI_DOUBLE_PRECISION, MPI_SUM,  &\n",
    "                       MPI_COMM_WORLD, mpierror)\n",
    "\n",
    "    if(mpirank == 0) then\n",
    "        mpiwtime = mpiwtime + MPI_Wtime()\n",
    "        write(*, \"('Heat='     f0.2' | ')\", advance=\"no\") rheat\n",
    "        write(*, \"('Time='    f0.4' | ')\", advance=\"no\") mpiwtime\n",
    "        write(*, \"('MPI_Size=' i0  ' | ')\", advance=\"no\") mpisize\n",
    "        write(*, \"('MPI_Dims=('i0','i0') | ')\", advance=\"no\") pdims\n",
    "        write(*, \"('bx,by=('i0','i0')')\") bx,by\n",
    "    endif\n",
    "\n",
    "    call MPI_Finalize(mpierror)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-rwxr-xr-x 1 xxxx.xxxx ampemi 23K Dec  2 22:18 stencil_mpi\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "module load openmpi/gnu/4.0.1\n",
    "mpif90  -O3  -o stencil_mpi  stencil_mpi.f90\n",
    "ls -lh stencil_mpi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 769K\n",
      "-rwxr-xr-x 1 xxxx.xxxx ampemi 817K Nov  8 21:12 stencil_mpi\n",
      "-rwxr-xr-x 1 xxxx.xxxx ampemi 765K Nov  8 18:59 stencil_seq\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "a='stencil_mpi'\n",
    "b='/stnc/Fortran'\n",
    "s='/prj/ampemi/xxxx.xxxx'$b\n",
    "d='/scratch/ampemi/xxxx.xxxx'$b\n",
    "cp $s/$a $d\n",
    "ls -lh $d/$a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting stfopagnu485_16.srm\n"
     ]
    }
   ],
   "source": [
    "%%writefile stfopagnu485_16.srm\n",
    "#!/bin/bash\n",
    "#SBATCH --ntasks=16            # Total tasks(CPUs)\n",
    "#SBATCH -p cpu_small           # Select partition\n",
    "#SBATCH -J stcforpa            # Job name\n",
    "#SBATCH --time=00:02:00        # Limit execution time\n",
    "\n",
    "echo '========================================'\n",
    "echo '- Job ID:' $SLURM_JOB_ID\n",
    "echo '- Tasks per node:' $SLURM_NTASKS_PER_NODE\n",
    "echo '- Number of nodes:' $SLURM_JOB_NUM_NODES\n",
    "echo '- Total tasks:' $SLURM_NTASKS\n",
    "echo '- Nodes alocated:' $SLURM_JOB_NODELIST\n",
    "echo '- Directory where sbatch was called ($SLURM_SUBMIT_DIR):'\n",
    "echo $SLURM_SUBMIT_DIR\n",
    "cd $SLURM_SUBMIT_DIR\n",
    "nodeset -e $SLURM_JOB_NODELIST\n",
    "\n",
    "# Module\n",
    "echo '-- modules ----------------------------'\n",
    "echo 'module load openmpi/gnu/4.0.1'\n",
    "module load openmpi/gnu/4.0.1\n",
    "\n",
    "# Executable\n",
    "EXEC=\"/scratch/ampemi/xxxx.xxxx/stnc/Fortran/stencil_mpi 4800 1 500\"\n",
    "\n",
    "# Run\n",
    "echo '-- srun -------------------------------'\n",
    "echo '$ srun --mpi=pmi2 -n' $SLURM_NTASKS $EXEC\n",
    "srun --mpi=pmi2 -n $SLURM_NTASKS $EXEC\n",
    "echo '-- END --------------------------------'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"height:10px;border-width:0;background-color:green\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First measurement:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================================\n",
      "- Job ID: 781013\n",
      "- Tasks per node:\n",
      "- Number of nodes: 1\n",
      "- Total tasks: 1\n",
      "- Nodes alocated: sdumont1429\n",
      "- Directory where sbatch was called ($SLURM_SUBMIT_DIR):\n",
      "/prj/ampemi/xxxx.xxxx/stnc/Fortran\n",
      "sdumont1429\n",
      "-- modules ----------------------------\n",
      "module load openmpi/gnu/4.0.1\n",
      "-- srun -------------------------------\n",
      "$ srun --mpi=pmi2 -n 1 /scratch/ampemi/xxxx.xxxx/stnc/Fortran/stencil_mpi 4800 1 500\n",
      "Heat=1500.00 | Time=21.8917 | MPI_Size=1 | MPI_Dims=(1,1) | bx,by=(4800,4800)\n",
      "-- END --------------------------------\n",
      "========================================\n",
      "- Job ID: 781014\n",
      "- Tasks per node:\n",
      "- Number of nodes: 1\n",
      "- Total tasks: 4\n",
      "- Nodes alocated: sdumont1454\n",
      "- Directory where sbatch was called ($SLURM_SUBMIT_DIR):\n",
      "/prj/ampemi/xxxx.xxxx/stnc/Fortran\n",
      "sdumont1454\n",
      "-- modules ----------------------------\n",
      "module load openmpi/gnu/4.0.1\n",
      "-- srun -------------------------------\n",
      "$ srun --mpi=pmi2 -n 4 /scratch/ampemi/xxxx.xxxx/stnc/Fortran/stencil_mpi 4800 1 500\n",
      "Heat=1500.00 | Time=7.3403 | MPI_Size=4 | MPI_Dims=(2,2) | bx,by=(2400,2400)\n",
      "-- END --------------------------------\n",
      "========================================\n",
      "- Job ID: 781015\n",
      "- Tasks per node:\n",
      "- Number of nodes: 1\n",
      "- Total tasks: 9\n",
      "- Nodes alocated: sdumont1464\n",
      "- Directory where sbatch was called ($SLURM_SUBMIT_DIR):\n",
      "/prj/ampemi/xxxx.xxxx/stnc/Fortran\n",
      "sdumont1464\n",
      "-- modules ----------------------------\n",
      "module load openmpi/gnu/4.0.1\n",
      "-- srun -------------------------------\n",
      "$ srun --mpi=pmi2 -n 9 /scratch/ampemi/xxxx.xxxx/stnc/Fortran/stencil_mpi 4800 1 500\n",
      "Heat=1500.00 | Time=6.0767 | MPI_Size=9 | MPI_Dims=(3,3) | bx,by=(1600,1600)\n",
      "-- END --------------------------------\n",
      "========================================\n",
      "- Job ID: 781031\n",
      "- Tasks per node:\n",
      "- Number of nodes: 1\n",
      "- Total tasks: 16\n",
      "- Nodes alocated: sdumont1429\n",
      "- Directory where sbatch was called ($SLURM_SUBMIT_DIR):\n",
      "/prj/ampemi/xxxx.xxxx/stnc/Fortran\n",
      "sdumont1429\n",
      "-- modules ----------------------------\n",
      "module load openmpi/gnu/4.0.1\n",
      "-- srun -------------------------------\n",
      "$ srun --mpi=pmi2 -n 16 /scratch/ampemi/xxxx.xxxx/stnc/Fortran/stencil_mpi 4800 1 500\n",
      "Heat=1500.00 | Time=4.7581 | MPI_Size=16 | MPI_Dims=(4,4) | bx,by=(1200,1200)\n",
      "-- END --------------------------------\n",
      "========================================\n",
      "- Job ID: 781017\n",
      "- Tasks per node:\n",
      "- Number of nodes: 2\n",
      "- Total tasks: 36\n",
      "- Nodes alocated: sdumont[1454,1464]\n",
      "- Directory where sbatch was called ($SLURM_SUBMIT_DIR):\n",
      "/prj/ampemi/xxxx.xxxx/stnc/Fortran\n",
      "sdumont1454 sdumont1464\n",
      "-- modules ----------------------------\n",
      "module load openmpi/gnu/4.0.1\n",
      "-- srun -------------------------------\n",
      "$ srun --mpi=pmi2 -n 36 /scratch/ampemi/xxxx.xxxx/stnc/Fortran/stencil_mpi 4800 1 500\n",
      "Heat=1500.00 | Time=2.1853 | MPI_Size=36 | MPI_Dims=(6,6) | bx,by=(800,800)\n",
      "-- END --------------------------------\n",
      "========================================\n",
      "- Job ID: 781018\n",
      "- Tasks per node:\n",
      "- Number of nodes: 3\n",
      "- Total tasks: 49\n",
      "- Nodes alocated: sdumont[1429,1454,1464]\n",
      "- Directory where sbatch was called ($SLURM_SUBMIT_DIR):\n",
      "/prj/ampemi/xxxx.xxxx/stnc/Fortran\n",
      "sdumont1429 sdumont1454 sdumont1464\n",
      "-- modules ----------------------------\n",
      "module load openmpi/gnu/4.0.1\n",
      "-- srun -------------------------------\n",
      "$ srun --mpi=pmi2 -n 49 /scratch/ampemi/xxxx.xxxx/stnc/Fortran/stencil_mpi 4800 1 500\n",
      "Heat=1500.00 | Time=1.5905 | MPI_Size=49 | MPI_Dims=(7,7) | bx,by=(685,685)\n",
      "-- END --------------------------------\n",
      "========================================\n",
      "- Job ID: 781019\n",
      "- Tasks per node:\n",
      "- Number of nodes: 3\n",
      "- Total tasks: 64\n",
      "- Nodes alocated: sdumont[1429,1454,1464]\n",
      "- Directory where sbatch was called ($SLURM_SUBMIT_DIR):\n",
      "/prj/ampemi/xxxx.xxxx/stnc/Fortran\n",
      "sdumont1429 sdumont1454 sdumont1464\n",
      "-- modules ----------------------------\n",
      "module load openmpi/gnu/4.0.1\n",
      "-- srun -------------------------------\n",
      "$ srun --mpi=pmi2 -n 64 /scratch/ampemi/xxxx.xxxx/stnc/Fortran/stencil_mpi 4800 1 500\n",
      "Heat=1500.00 | Time=1.2578 | MPI_Size=64 | MPI_Dims=(8,8) | bx,by=(600,600)\n",
      "-- END --------------------------------\n",
      "========================================\n",
      "- Job ID: 781020\n",
      "- Tasks per node:\n",
      "- Number of nodes: 4\n",
      "- Total tasks: 81\n",
      "- Nodes alocated: sdumont[1122-1123,1429,1454]\n",
      "- Directory where sbatch was called ($SLURM_SUBMIT_DIR):\n",
      "/prj/ampemi/xxxx.xxxx/stnc/Fortran\n",
      "sdumont1122 sdumont1123 sdumont1429 sdumont1454\n",
      "-- modules ----------------------------\n",
      "module load openmpi/gnu/4.0.1\n",
      "-- srun -------------------------------\n",
      "$ srun --mpi=pmi2 -n 81 /scratch/ampemi/xxxx.xxxx/stnc/Fortran/stencil_mpi 4800 1 500\n",
      "Heat=1500.00 | Time=2.0254 | MPI_Size=81 | MPI_Dims=(9,9) | bx,by=(533,533)\n",
      "-- END --------------------------------\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "d='/scratch/ampemi/xxxx.xxxx/stnc/Fortran'\n",
    "cat $d/slurm-781013.out  #01\n",
    "cat $d/slurm-781014.out  #04\n",
    "cat $d/slurm-781015.out  #09\n",
    "cat $d/slurm-781031.out  #16\n",
    "cat $d/slurm-781017.out  #36\n",
    "cat $d/slurm-781018.out  #49\n",
    "cat $d/slurm-781019.out  #64\n",
    "cat $d/slurm-781020.out  #81"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"height:10px;border-width:0;background-color:green\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Second measurement:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submitted batch job 788003\n",
      "Submitted batch job 788004\n",
      "Submitted batch job 788005\n",
      "Submitted batch job 788006\n",
      "Submitted batch job 788007\n",
      "Submitted batch job 788008\n",
      "Submitted batch job 788009\n",
      "Submitted batch job 788010\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "# 1x1=1, 2x2=4, 3x3=9, 4x4=16, 6x6=36, 7x7=49, 8x8=64, 9x9=81\n",
    "sbatch stfopagnu485_01.srm\n",
    "sbatch stfopagnu485_04.srm\n",
    "sbatch stfopagnu485_09.srm\n",
    "sbatch stfopagnu485_16.srm\n",
    "sbatch stfopagnu485_36.srm\n",
    "sbatch stfopagnu485_49.srm\n",
    "sbatch stfopagnu485_64.srm\n",
    "sbatch stfopagnu485_81.srm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             JOBID PARTITION     NAME     USER ST       TIME  NODES NODELIST(REASON)\n",
      "            788003 cpu_small stcforpa xxxx. PD       0:00      1 (Priority)\n",
      "            788004 cpu_small stcforpa xxxx. PD       0:00      1 (Priority)\n",
      "            788005 cpu_small stcforpa xxxx. PD       0:00      1 (Priority)\n",
      "            788006 cpu_small stcforpa xxxx. PD       0:00      1 (Priority)\n",
      "            788007 cpu_small stcforpa xxxx. PD       0:00      2 (Priority)\n",
      "            788008 cpu_small stcforpa xxxx. PD       0:00      3 (Priority)\n",
      "            788009 cpu_small stcforpa xxxx. PD       0:00      3 (Priority)\n",
      "            788010 cpu_small stcforpa xxxx. PD       0:00      4 (Priority)\n"
     ]
    }
   ],
   "source": [
    "! squeue -n stcforpa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================================\n",
      "- Job ID: 788003\n",
      "- Tasks per node:\n",
      "- Number of nodes: 1\n",
      "- Total tasks: 1\n",
      "- Nodes alocated: sdumont1149\n",
      "- Directory where sbatch was called ($SLURM_SUBMIT_DIR):\n",
      "/prj/ampemi/xxxx.xxxx/stnc/Fortran\n",
      "sdumont1149\n",
      "-- modules ----------------------------\n",
      "module load openmpi/gnu/4.0.1\n",
      "-- srun -------------------------------\n",
      "$ srun --mpi=pmi2 -n 1 /scratch/ampemi/xxxx.xxxx/stnc/Fortran/stencil_mpi 4800 1 500\n",
      "Heat=1500.00 | Time=21.9799 | MPI_Size=1 | MPI_Dims=(1,1) | bx,by=(4800,4800)\n",
      "-- END --------------------------------\n",
      "========================================\n",
      "- Job ID: 788004\n",
      "- Tasks per node:\n",
      "- Number of nodes: 1\n",
      "- Total tasks: 4\n",
      "- Nodes alocated: sdumont1149\n",
      "- Directory where sbatch was called ($SLURM_SUBMIT_DIR):\n",
      "/prj/ampemi/xxxx.xxxx/stnc/Fortran\n",
      "sdumont1149\n",
      "-- modules ----------------------------\n",
      "module load openmpi/gnu/4.0.1\n",
      "-- srun -------------------------------\n",
      "$ srun --mpi=pmi2 -n 4 /scratch/ampemi/xxxx.xxxx/stnc/Fortran/stencil_mpi 4800 1 500\n",
      "Heat=1500.00 | Time=7.3660 | MPI_Size=4 | MPI_Dims=(2,2) | bx,by=(2400,2400)\n",
      "-- END --------------------------------\n",
      "========================================\n",
      "- Job ID: 788005\n",
      "- Tasks per node:\n",
      "- Number of nodes: 1\n",
      "- Total tasks: 9\n",
      "- Nodes alocated: sdumont1149\n",
      "- Directory where sbatch was called ($SLURM_SUBMIT_DIR):\n",
      "/prj/ampemi/xxxx.xxxx/stnc/Fortran\n",
      "sdumont1149\n",
      "-- modules ----------------------------\n",
      "module load openmpi/gnu/4.0.1\n",
      "-- srun -------------------------------\n",
      "$ srun --mpi=pmi2 -n 9 /scratch/ampemi/xxxx.xxxx/stnc/Fortran/stencil_mpi 4800 1 500\n",
      "Heat=1500.00 | Time=6.1560 | MPI_Size=9 | MPI_Dims=(3,3) | bx,by=(1600,1600)\n",
      "-- END --------------------------------\n",
      "========================================\n",
      "- Job ID: 788006\n",
      "- Tasks per node:\n",
      "- Number of nodes: 1\n",
      "- Total tasks: 16\n",
      "- Nodes alocated: sdumont1149\n",
      "- Directory where sbatch was called ($SLURM_SUBMIT_DIR):\n",
      "/prj/ampemi/xxxx.xxxx/stnc/Fortran\n",
      "sdumont1149\n",
      "-- modules ----------------------------\n",
      "module load openmpi/gnu/4.0.1\n",
      "-- srun -------------------------------\n",
      "$ srun --mpi=pmi2 -n 16 /scratch/ampemi/xxxx.xxxx/stnc/Fortran/stencil_mpi 4800 1 500\n",
      "Heat=1500.00 | Time=4.7203 | MPI_Size=16 | MPI_Dims=(4,4) | bx,by=(1200,1200)\n",
      "-- END --------------------------------\n",
      "========================================\n",
      "- Job ID: 788007\n",
      "- Tasks per node:\n",
      "- Number of nodes: 2\n",
      "- Total tasks: 36\n",
      "- Nodes alocated: sdumont[1149,1272]\n",
      "- Directory where sbatch was called ($SLURM_SUBMIT_DIR):\n",
      "/prj/ampemi/xxxx.xxxx/stnc/Fortran\n",
      "sdumont1149 sdumont1272\n",
      "-- modules ----------------------------\n",
      "module load openmpi/gnu/4.0.1\n",
      "-- srun -------------------------------\n",
      "$ srun --mpi=pmi2 -n 36 /scratch/ampemi/xxxx.xxxx/stnc/Fortran/stencil_mpi 4800 1 500\n",
      "Heat=1500.00 | Time=2.1258 | MPI_Size=36 | MPI_Dims=(6,6) | bx,by=(800,800)\n",
      "-- END --------------------------------\n",
      "========================================\n",
      "- Job ID: 788008\n",
      "- Tasks per node:\n",
      "- Number of nodes: 3\n",
      "- Total tasks: 49\n",
      "- Nodes alocated: sdumont[1083,1149,1272]\n",
      "- Directory where sbatch was called ($SLURM_SUBMIT_DIR):\n",
      "/prj/ampemi/xxxx.xxxx/stnc/Fortran\n",
      "sdumont1083 sdumont1149 sdumont1272\n",
      "-- modules ----------------------------\n",
      "module load openmpi/gnu/4.0.1\n",
      "-- srun -------------------------------\n",
      "$ srun --mpi=pmi2 -n 49 /scratch/ampemi/xxxx.xxxx/stnc/Fortran/stencil_mpi 4800 1 500\n",
      "Heat=1500.00 | Time=1.5254 | MPI_Size=49 | MPI_Dims=(7,7) | bx,by=(685,685)\n",
      "-- END --------------------------------\n",
      "========================================\n",
      "- Job ID: 788009\n",
      "- Tasks per node:\n",
      "- Number of nodes: 3\n",
      "- Total tasks: 64\n",
      "- Nodes alocated: sdumont[1083,1149,1272]\n",
      "- Directory where sbatch was called ($SLURM_SUBMIT_DIR):\n",
      "/prj/ampemi/xxxx.xxxx/stnc/Fortran\n",
      "sdumont1083 sdumont1149 sdumont1272\n",
      "-- modules ----------------------------\n",
      "module load openmpi/gnu/4.0.1\n",
      "-- srun -------------------------------\n",
      "$ srun --mpi=pmi2 -n 64 /scratch/ampemi/xxxx.xxxx/stnc/Fortran/stencil_mpi 4800 1 500\n",
      "Heat=1500.00 | Time=1.2199 | MPI_Size=64 | MPI_Dims=(8,8) | bx,by=(600,600)\n",
      "-- END --------------------------------\n",
      "========================================\n",
      "- Job ID: 788010\n",
      "- Tasks per node:\n",
      "- Number of nodes: 4\n",
      "- Total tasks: 81\n",
      "- Nodes alocated: sdumont[1083,1149,1391,1410]\n",
      "- Directory where sbatch was called ($SLURM_SUBMIT_DIR):\n",
      "/prj/ampemi/xxxx.xxxx/stnc/Fortran\n",
      "sdumont1083 sdumont1149 sdumont1391 sdumont1410\n",
      "-- modules ----------------------------\n",
      "module load openmpi/gnu/4.0.1\n",
      "-- srun -------------------------------\n",
      "$ srun --mpi=pmi2 -n 81 /scratch/ampemi/xxxx.xxxx/stnc/Fortran/stencil_mpi 4800 1 500\n",
      "Heat=1500.00 | Time=2.0314 | MPI_Size=81 | MPI_Dims=(9,9) | bx,by=(533,533)\n",
      "-- END --------------------------------\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "d='/scratch/ampemi/xxxx.xxxx/stnc/Fortran'\n",
    "cat $d/slurm-788003.out  #01\n",
    "cat $d/slurm-788004.out  #04\n",
    "cat $d/slurm-788005.out  #09\n",
    "cat $d/slurm-788006.out  #16\n",
    "cat $d/slurm-788007.out  #36\n",
    "cat $d/slurm-788008.out  #49\n",
    "cat $d/slurm-788009.out  #64\n",
    "cat $d/slurm-788010.out  #81"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"height:10px;border-width:0;background-color:green\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Third measurement:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submitted batch job 788013\n",
      "Submitted batch job 788014\n",
      "Submitted batch job 788015\n",
      "Submitted batch job 788016\n",
      "Submitted batch job 788017\n",
      "Submitted batch job 788018\n",
      "Submitted batch job 788019\n",
      "Submitted batch job 788020\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "# 1x1=1, 2x2=4, 3x3=9, 4x4=16, 6x6=36, 7x7=49, 8x8=64, 9x9=81\n",
    "sbatch stfopagnu485_01.srm\n",
    "sbatch stfopagnu485_04.srm\n",
    "sbatch stfopagnu485_09.srm\n",
    "sbatch stfopagnu485_16.srm\n",
    "sbatch stfopagnu485_36.srm\n",
    "sbatch stfopagnu485_49.srm\n",
    "sbatch stfopagnu485_64.srm\n",
    "sbatch stfopagnu485_81.srm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             JOBID PARTITION     NAME     USER ST       TIME  NODES NODELIST(REASON)\n",
      "            788009 cpu_small stcforpa xxxx. PD       0:00      3 (Resources)\n",
      "            788010 cpu_small stcforpa xxxx. PD       0:00      4 (Resources)\n",
      "            788007 cpu_small stcforpa xxxx. PD       0:00      2 (Resources)\n",
      "            788008 cpu_small stcforpa xxxx. PD       0:00      3 (Resources)\n",
      "            788006 cpu_small stcforpa xxxx. PD       0:00      1 (Resources)\n",
      "            788005 cpu_small stcforpa xxxx. PD       0:00      1 (Resources)\n",
      "            788004 cpu_small stcforpa xxxx. PD       0:00      1 (Resources)\n",
      "            788003 cpu_small stcforpa xxxx. PD       0:00      1 (Resources)\n",
      "            788013 cpu_small stcforpa xxxx. PD       0:00      1 (Priority)\n",
      "            788014 cpu_small stcforpa xxxx. PD       0:00      1 (Priority)\n",
      "            788015 cpu_small stcforpa xxxx. PD       0:00      1 (Priority)\n",
      "            788016 cpu_small stcforpa xxxx. PD       0:00      1 (Priority)\n",
      "            788017 cpu_small stcforpa xxxx. PD       0:00      2 (Priority)\n",
      "            788018 cpu_small stcforpa xxxx. PD       0:00      3 (Priority)\n",
      "            788019 cpu_small stcforpa xxxx. PD       0:00      3 (Priority)\n",
      "            788020 cpu_small stcforpa xxxx. PD       0:00      4 (Priority)\n"
     ]
    }
   ],
   "source": [
    "! squeue -n stcforpa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================================\n",
      "- Job ID: 788013\n",
      "- Tasks per node:\n",
      "- Number of nodes: 1\n",
      "- Total tasks: 1\n",
      "- Nodes alocated: sdumont1149\n",
      "- Directory where sbatch was called ($SLURM_SUBMIT_DIR):\n",
      "/prj/ampemi/xxxx.xxxx/stnc/Fortran\n",
      "sdumont1149\n",
      "-- modules ----------------------------\n",
      "module load openmpi/gnu/4.0.1\n",
      "-- srun -------------------------------\n",
      "$ srun --mpi=pmi2 -n 1 /scratch/ampemi/xxxx.xxxx/stnc/Fortran/stencil_mpi 4800 1 500\n",
      "Heat=1500.00 | Time=21.8653 | MPI_Size=1 | MPI_Dims=(1,1) | bx,by=(4800,4800)\n",
      "-- END --------------------------------\n",
      "========================================\n",
      "- Job ID: 788014\n",
      "- Tasks per node:\n",
      "- Number of nodes: 1\n",
      "- Total tasks: 4\n",
      "- Nodes alocated: sdumont1149\n",
      "- Directory where sbatch was called ($SLURM_SUBMIT_DIR):\n",
      "/prj/ampemi/xxxx.xxxx/stnc/Fortran\n",
      "sdumont1149\n",
      "-- modules ----------------------------\n",
      "module load openmpi/gnu/4.0.1\n",
      "-- srun -------------------------------\n",
      "$ srun --mpi=pmi2 -n 4 /scratch/ampemi/xxxx.xxxx/stnc/Fortran/stencil_mpi 4800 1 500\n",
      "Heat=1500.00 | Time=7.3280 | MPI_Size=4 | MPI_Dims=(2,2) | bx,by=(2400,2400)\n",
      "-- END --------------------------------\n",
      "========================================\n",
      "- Job ID: 788015\n",
      "- Tasks per node:\n",
      "- Number of nodes: 1\n",
      "- Total tasks: 9\n",
      "- Nodes alocated: sdumont1149\n",
      "- Directory where sbatch was called ($SLURM_SUBMIT_DIR):\n",
      "/prj/ampemi/xxxx.xxxx/stnc/Fortran\n",
      "sdumont1149\n",
      "-- modules ----------------------------\n",
      "module load openmpi/gnu/4.0.1\n",
      "-- srun -------------------------------\n",
      "$ srun --mpi=pmi2 -n 9 /scratch/ampemi/xxxx.xxxx/stnc/Fortran/stencil_mpi 4800 1 500\n",
      "Heat=1500.00 | Time=6.2291 | MPI_Size=9 | MPI_Dims=(3,3) | bx,by=(1600,1600)\n",
      "-- END --------------------------------\n",
      "========================================\n",
      "- Job ID: 788016\n",
      "- Tasks per node:\n",
      "- Number of nodes: 1\n",
      "- Total tasks: 16\n",
      "- Nodes alocated: sdumont1149\n",
      "- Directory where sbatch was called ($SLURM_SUBMIT_DIR):\n",
      "/prj/ampemi/xxxx.xxxx/stnc/Fortran\n",
      "sdumont1149\n",
      "-- modules ----------------------------\n",
      "module load openmpi/gnu/4.0.1\n",
      "-- srun -------------------------------\n",
      "$ srun --mpi=pmi2 -n 16 /scratch/ampemi/xxxx.xxxx/stnc/Fortran/stencil_mpi 4800 1 500\n",
      "Heat=1500.00 | Time=4.5761 | MPI_Size=16 | MPI_Dims=(4,4) | bx,by=(1200,1200)\n",
      "-- END --------------------------------\n",
      "========================================\n",
      "- Job ID: 788017\n",
      "- Tasks per node:\n",
      "- Number of nodes: 2\n",
      "- Total tasks: 36\n",
      "- Nodes alocated: sdumont[1083,1149]\n",
      "- Directory where sbatch was called ($SLURM_SUBMIT_DIR):\n",
      "/prj/ampemi/xxxx.xxxx/stnc/Fortran\n",
      "sdumont1083 sdumont1149\n",
      "-- modules ----------------------------\n",
      "module load openmpi/gnu/4.0.1\n",
      "-- srun -------------------------------\n",
      "$ srun --mpi=pmi2 -n 36 /scratch/ampemi/xxxx.xxxx/stnc/Fortran/stencil_mpi 4800 1 500\n",
      "Heat=1500.00 | Time=2.0752 | MPI_Size=36 | MPI_Dims=(6,6) | bx,by=(800,800)\n",
      "-- END --------------------------------\n",
      "========================================\n",
      "- Job ID: 788018\n",
      "- Tasks per node:\n",
      "- Number of nodes: 3\n",
      "- Total tasks: 49\n",
      "- Nodes alocated: sdumont[1083,1149,1272]\n",
      "- Directory where sbatch was called ($SLURM_SUBMIT_DIR):\n",
      "/prj/ampemi/xxxx.xxxx/stnc/Fortran\n",
      "sdumont1083 sdumont1149 sdumont1272\n",
      "-- modules ----------------------------\n",
      "module load openmpi/gnu/4.0.1\n",
      "-- srun -------------------------------\n",
      "$ srun --mpi=pmi2 -n 49 /scratch/ampemi/xxxx.xxxx/stnc/Fortran/stencil_mpi 4800 1 500\n",
      "Heat=1500.00 | Time=2.5394 | MPI_Size=49 | MPI_Dims=(7,7) | bx,by=(685,685)\n",
      "-- END --------------------------------\n",
      "========================================\n",
      "- Job ID: 788019\n",
      "- Tasks per node:\n",
      "- Number of nodes: 3\n",
      "- Total tasks: 64\n",
      "- Nodes alocated: sdumont[1083,1149,1272]\n",
      "- Directory where sbatch was called ($SLURM_SUBMIT_DIR):\n",
      "/prj/ampemi/xxxx.xxxx/stnc/Fortran\n",
      "sdumont1083 sdumont1149 sdumont1272\n",
      "-- modules ----------------------------\n",
      "module load openmpi/gnu/4.0.1\n",
      "-- srun -------------------------------\n",
      "$ srun --mpi=pmi2 -n 64 /scratch/ampemi/xxxx.xxxx/stnc/Fortran/stencil_mpi 4800 1 500\n",
      "Heat=1500.00 | Time=1.2071 | MPI_Size=64 | MPI_Dims=(8,8) | bx,by=(600,600)\n",
      "-- END --------------------------------\n",
      "========================================\n",
      "- Job ID: 788020\n",
      "- Tasks per node:\n",
      "- Number of nodes: 4\n",
      "- Total tasks: 81\n",
      "- Nodes alocated: sdumont[1083,1149,1391,1410]\n",
      "- Directory where sbatch was called ($SLURM_SUBMIT_DIR):\n",
      "/prj/ampemi/xxxx.xxxx/stnc/Fortran\n",
      "sdumont1083 sdumont1149 sdumont1391 sdumont1410\n",
      "-- modules ----------------------------\n",
      "module load openmpi/gnu/4.0.1\n",
      "-- srun -------------------------------\n",
      "$ srun --mpi=pmi2 -n 81 /scratch/ampemi/xxxx.xxxx/stnc/Fortran/stencil_mpi 4800 1 500\n",
      "Heat=1500.00 | Time=.9991 | MPI_Size=81 | MPI_Dims=(9,9) | bx,by=(533,533)\n",
      "-- END --------------------------------\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "d='/scratch/ampemi/xxxx.xxxx/stnc/Fortran'\n",
    "cat $d/slurm-788013.out  #01\n",
    "cat $d/slurm-788014.out  #04\n",
    "cat $d/slurm-788015.out  #09\n",
    "cat $d/slurm-788016.out  #16\n",
    "cat $d/slurm-788017.out  #36\n",
    "cat $d/slurm-788018.out  #49\n",
    "cat $d/slurm-788019.out  #64\n",
    "cat $d/slurm-788020.out  #81"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"height:10px;border-width:0;background-color:red\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GNU Fortran (GCC) 7.4.0\n",
      "Copyright (C) 2017 Free Software Foundation, Inc.\n",
      "This is free software; see the source for copying conditions.  There is NO\n",
      "warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "module load gcc/7.4\n",
    "module load openmpi/gnu/4.0.1\n",
    "mpif90 --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gcc (GCC) 4.8.5 20150623 (Red Hat 4.8.5-36)\n",
      "Copyright (C) 2015 Free Software Foundation, Inc.\n",
      "This is free software; see the source for copying conditions.  There is NO\n",
      "warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.\n",
      "\n",
      "GNU Fortran (GCC) 4.8.5 20150623 (Red Hat 4.8.5-36)\n",
      "Copyright (C) 2015 Free Software Foundation, Inc.\n",
      "\n",
      "GNU Fortran comes with NO WARRANTY, to the extent permitted by law.\n",
      "You may redistribute copies of GNU Fortran\n",
      "under the terms of the GNU General Public License.\n",
      "For more information about these matters, see the file named COPYING\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "module load openmpi/gnu/4.0.1\n",
    "gcc --version\n",
    "mpif90 --version"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
