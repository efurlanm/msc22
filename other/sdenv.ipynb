{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ca6eedea-72a7-4237-80fe-1ec84b825e67",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Module sdenv\n",
    "\n",
    "Adapted from: http://www.idris.fr/eng/jean-zay/gpu/jean-zay-gpu-torch-multi-eng.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "08b5e3b4-84e2-4760-883b-0a87f1131814",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting sdenv.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile sdenv.py\n",
    "\n",
    "# sdenv module\n",
    "\n",
    "import os, hostlist, socket\n",
    "\n",
    "job_partition = os.uname()[1]\n",
    "hostnames = [job_partition]\n",
    "\n",
    "if 'SLURM_PROCID' in os.environ:\n",
    "    # Equivalent to MPI rank\n",
    "    rank = int(os.environ['SLURM_PROCID'])\n",
    "    os.environ['RANK'] = str(rank)\n",
    "    # rank inside a node\n",
    "    local_rank = int(os.environ['SLURM_LOCALID'])\n",
    "    # node rank inside NODELIST\n",
    "    node_rank = int(os.environ['SLURM_NODEID'])\n",
    "    size = int(os.environ['SLURM_NTASKS'])\n",
    "    os.environ['WORLD_SIZE'] = str(size)\n",
    "    cpus_per_task = int(os.environ['SLURM_CPUS_PER_TASK'])\n",
    "    job_partition = os.environ['SLURM_JOB_PARTITION']\n",
    "    job_id = os.environ['SLURM_JOB_ID']\n",
    "    hostnames = hostlist.expand_hostlist(\n",
    "        os.environ['SLURM_JOB_NODELIST'])\n",
    "\n",
    "# Get IDs of GPUs\n",
    "# Queues should be adusted as needed\n",
    "gpu_ids = []\n",
    "if job_partition in ['nvidia_dev', 'nvidia', 'nvidia_small',\n",
    "                     'nvidia_scal', 'nvidia_long']:\n",
    "    gpu_ids = [0, 1]\n",
    "elif job_partition in ['sequana_gpu_dev', 'sequana_gpu_shared',\n",
    "                       'sdumont18']:\n",
    "    gpu_ids = [0, 1, 2, 3]\n",
    "\n",
    "# Define the dataset directory.\n",
    "# For simplicity, it is assumed that the same directory structure as the \n",
    "# login node also exists in the scratch/ area, i.e. assuming there is a \n",
    "# \"dataset\" directory in the login node, then there is also a \"dataset\" \n",
    "# directory in the scratch/ area.\n",
    "DSDIR = os.environ['PWD'].replace('/prj/', '/scratch/')\n",
    "os.environ['DSDIR'] = DSDIR\n",
    "\n",
    "# Define the MASTER\n",
    "MASTER_ADDR = socket.gethostbyname(hostnames[0])\n",
    "os.environ['MASTER_ADDR'] = MASTER_ADDR\n",
    "\n",
    "# To avoid port conflict on the same node.\n",
    "# 20324 is an randon port.\n",
    "MASTER_PORT = str(20324 + int(min(gpu_ids)))\n",
    "os.environ['MASTER_PORT'] = MASTER_PORT\n",
    "\n",
    "# Initializing dist.init_process_group via TCP \n",
    "MASTER_TCP = 'tcp://' + MASTER_ADDR + \":\" + MASTER_PORT\n",
    "\n",
    "# Define the\n",
    "MASTER_FILE = 'file://////' + MASTER_ADDR + DSDIR + '/sharedfile.txt'\n",
    "\n",
    "# NCCL backend network interface to use\n",
    "NCCL_SOCKET_IFNAME = 'ib0'\n",
    "os.environ['NCCL_SOCKET_IFNAME'] = NCCL_SOCKET_IFNAME\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # CHECK IF IT IS WORKING\n",
    "    print(f'rank: {rank:02} ', f'| node_rank: {node_rank:2}',\n",
    "          f'| local_rank: {local_rank:2}')\n",
    "    if not rank:\n",
    "        print(\"01. job_partition: \", job_partition)\n",
    "        print(\"02. job_id: \", job_id)\n",
    "        print(\"03. size: \", size)\n",
    "        print(\"04. cpus_per_task: \", cpus_per_task)\n",
    "        print(\"05. hostnames: \", hostnames)\n",
    "        print(\"06. gpu_ids: \", gpu_ids)\n",
    "        print(\"07. MASTER_ADDR: \", MASTER_ADDR)\n",
    "        print(\"08. MASTER_PORT: \", MASTER_PORT)\n",
    "        print(\"09. MASTER_TCP: \", MASTER_TCP)\n",
    "        print(\"10. DSDIR: \", DSDIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12b5afb3-632d-484e-b332-4bf4787edc0d",
   "metadata": {},
   "source": [
    "## Check if it is working"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "dc3e694d-aa98-44e6-8964-4f7e9cdd1d04",
   "metadata": {},
   "outputs": [],
   "source": [
    "! cp sdenv.py /scratch${PWD#/prj}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "7e298881-c294-46ea-84d8-7bbe44d05028",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ptch.srm\n"
     ]
    }
   ],
   "source": [
    "%%writefile ptch.srm\n",
    "#!/bin/bash\n",
    "#SBATCH --job-name ptch         # SLURM_JOB_NAME\n",
    "#SBATCH --partition nvidia_dev  # SLURM_JOB_PARTITION\n",
    "#SBATCH --nodes=4               # SLURM_JOB_NUM_NODES\n",
    "#SBATCH --ntasks-per-node=3     # SLURM_NTASKS_PER_NODE\n",
    "#SBATCH --cpus-per-task=2       # SLURM_CPUS_PER_TASK\n",
    "#SBATCH --time=00:01:00         # Limit execution time\n",
    "#SBATCH --exclusive             # Exclusive acccess to nodes\n",
    "\n",
    "# VARIABLES OF INTEREST IN THE SLURM ENVIRONMENT\n",
    "# <https://slurm.schedmd.com/sbatch.html>\n",
    "# SLURM_PROCID\n",
    "#     The MPI rank (or relative process ID) of the current process.\n",
    "# SLURM_LOCALID\n",
    "#     Node local task ID for the process within a job.\n",
    "# SLURM_NODEID\n",
    "#     ID of the nodes allocated. \n",
    "\n",
    "echo '========================================'\n",
    "echo '- Job ID:' $SLURM_JOB_ID\n",
    "echo '- # of nodes in the job:' $SLURM_JOB_NUM_NODES\n",
    "echo '- # of tasks per node:' $SLURM_NTASKS_PER_NODE\n",
    "echo '- # of tasks:' $SLURM_NTASKS\n",
    "echo '- # of cpus per task:' $SLURM_CPUS_PER_TASK\n",
    "echo '- Dir from which sbatch was invoked:' ${SLURM_SUBMIT_DIR##*/}\n",
    "echo -n '- Nodes allocated to the job: '\n",
    "nodeset -e $SLURM_JOB_NODELIST\n",
    "\n",
    "# go to the work directory from which sbatch was invoked\n",
    "cd $SLURM_SUBMIT_DIR\n",
    "\n",
    "# load the Python environment\n",
    "SCR=/scratch${PWD#/prj}\n",
    "BASE=/scratch${HOME#/prj}\n",
    "source $BASE/env2/etc/profile.d/conda.sh\n",
    "conda activate $BASE/env2\n",
    "cd $SCR\n",
    "\n",
    "# run\n",
    "echo -n '<1. starting python script > ' && date\n",
    "echo '-- output -----------------------------'\n",
    "\n",
    "srun python -u sdenv.py | sort\n",
    "\n",
    "echo '-- end --------------------------------'\n",
    "echo -n '<2. quit>                    ' && date"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "382fe785-6ac6-4d48-9fb6-044f97968211",
   "metadata": {},
   "source": [
    "## Submit a job, wait, and print the result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "0cca3e4a-b0f2-4bea-b234-02dd0fefa138",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submitted batch job 10474508\n",
      "========================================\n",
      "- Job ID: 10474508\n",
      "- # of nodes in the job: 4\n",
      "- # of tasks per node: 3\n",
      "- # of tasks: 12\n",
      "- # of cpus per task: 2\n",
      "- Dir from which sbatch was invoked: pt\n",
      "- Nodes allocated to the job: sdumont3076 sdumont3077 sdumont3078 sdumont3079\n",
      "<1. starting python script > Qui Mar 24 21:35:58 -03 2022\n",
      "-- output -----------------------------\n",
      "01. job_partition:  nvidia_dev\n",
      "02. job_id:  10474508\n",
      "03. size:  12\n",
      "04. cpus_per_task:  2\n",
      "05. hostnames:  ['sdumont3076', 'sdumont3077', 'sdumont3078', 'sdumont3079']\n",
      "06. gpu_ids:  [0, 1]\n",
      "07. MASTER_ADDR:  172.20.10.70\n",
      "08. MASTER_PORT:  20324\n",
      "09. MASTER_TCP:  tcp://172.20.10.70:20324\n",
      "10. DSDIR:  /scratch/<project>/<username>/pt\n",
      "rank: 00  | node_rank:  0 | local_rank:  0\n",
      "rank: 01  | node_rank:  0 | local_rank:  1\n",
      "rank: 02  | node_rank:  0 | local_rank:  2\n",
      "rank: 03  | node_rank:  1 | local_rank:  0\n",
      "rank: 04  | node_rank:  1 | local_rank:  1\n",
      "rank: 05  | node_rank:  1 | local_rank:  2\n",
      "rank: 06  | node_rank:  2 | local_rank:  0\n",
      "rank: 07  | node_rank:  2 | local_rank:  1\n",
      "rank: 08  | node_rank:  2 | local_rank:  2\n",
      "rank: 09  | node_rank:  3 | local_rank:  0\n",
      "rank: 10  | node_rank:  3 | local_rank:  1\n",
      "rank: 11  | node_rank:  3 | local_rank:  2\n",
      "-- end --------------------------------\n",
      "<2. quit>                    Qui Mar 24 21:36:00 -03 2022\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "a = !sbatch ptch.srm\n",
    "print(a[0])\n",
    "while True:\n",
    "    time.sleep(5)\n",
    "    b = ! squeue --user $USER --name=ptch\n",
    "    if len(b) < 2: break\n",
    "b = !echo /scratch${PWD#/prj}/slurm-\n",
    "%cat {b[0]+a[0].replace('Submitted batch job ','')}.out"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
